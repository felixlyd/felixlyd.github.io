{"/blog/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/":{"data":{"":"主业先考证，争取评级或跳槽。副业年入4W。"},"title":"职业规划"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/":{"data":{"":" 规则引擎 交易系统 接口设计 权益系统 数据库设计 网关设计 "},"title":"系统设计方案"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E4%BA%A4%E6%98%93%E7%B3%BB%E7%BB%9F/":{"data":{"seata#seata":"如何实现一个TCC分布式事务框架的一点思考 - 腾讯云开发者社区-腾讯云\n从零开始写一个分布式事务框架(一) | 佑祺’s Blog\nseata","spring-cloud-seata#spring-cloud-seata":"easy-cs/分布式事务，看这篇真的够了!.md at main · allentofight/easy-cs\nGitHub - seata/seata-samples: seata-samples\nseata-samples/tcc/dubbo-tcc-sample at master · seata/seata-samples\n95–分布式事务六-Seata TCC模式-Spring Cloud微服务案例（添加TCC事务）\nSeata TCC模式实战_斗者_2013的博客-CSDN博客_seate tcc\n使用Seata彻底解决Spring Cloud中的分布式事务问题！","交易系统设计#交易系统设计":"核心银行系统 之五 7X24小时不间断运行的核心系统设计\n堪称最详细的支付系统设计 - 腾讯云开发者社区-腾讯云\n如何实现下单的幂等性 - 腾讯云开发者社区-腾讯云\n创建订单实现幂等的一点思考_Sam_Deep_Thinking的博客-CSDN博客_订单幂等性\n账务交易一致性设计概述 - ITechLib"},"title":"交易系统"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E4%BA%A4%E6%98%93%E7%B3%BB%E7%BB%9F/%E4%BA%A4%E6%98%93%E6%B5%81%E6%B0%B4%E5%A4%84%E7%90%86%E8%AE%BE%E8%AE%A1/":{"data":{"":"","其他#其他":" 事务A和事务C都可以在拦截器中实现，也可以在AOP中实现，这样所有的业务逻辑代码都可以被交易流水进行控制 交易流水日志表在服务运行一段时间后会变得臃肿，因此需要清理。设计一个交易流水日志历史表，并且设计一个定时任务每天扫描，将一个月或者几个月前的交易流水日志迁移到历史表中。如果交易量很大，也可以迁移几天前的数据。另外，交易流水日志历史表并不需要主键或者索引。 ","数据表设计#数据表设计":"交易流水日志表 tran_seq_log\n字段 描述 备注 tran_seq_no 本系统交易流水号 主键 channel_seq_no 上游渠道交易流水号 channel_seq_no、channel_code、tran_code三者构造唯一索引 channel_code 渠道编码 tran_code 本系统提供的交易编码 create_time 交易流水创建时间 update_time 交易处理后更新时间 status_code 交易状态码 00-成功，01-失败，99-处理中 response_code 响应码 response_msg 响应描述 ","流程设计#流程设计":""},"title":"交易流水处理设计"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E5%9F%BA%E4%BA%8Espring-boot%E5%BC%80%E5%8F%91%E8%87%AA%E5%AE%9A%E4%B9%89jar%E5%8C%85notes/":{"data":{"1-一些坑#1. 一些坑":"","11-package包名称#1.1. \u003ccode\u003epackage\u003c/code\u003e（包）名称":"","12-自动下载依赖包#1.2. 自动下载依赖包":"","13-value注入配置项#1.3. \u003ccode\u003e@Value\u003c/code\u003e注入配置项":"","14-使用自定义jar包时扫描不到service#1.4. 使用自定义jar包时，扫描不到\u003ccode\u003e@Service\u003c/code\u003e":"前言我使用的环境或配置如下：\n语言 java 1.8 框架 spring-boot 2.3.12.RELEASE 打包工具 maven IDE idea 远端maven库 Nexus私库 1. 一些坑1.1. package（包）名称 务必使用类似于org.example.name的package名称，并且尽量与pom.xml文件中groupId一致\n1.2. 自动下载依赖包 如果会使用mvn install，它会自动打包当前pom.xml文件为.pom文件，安装到本地库中；如果没使用或者不会使用mvn install，则需要将工具包的pom.xml复制一份，命名为xxx-x.x.x.pom，并且放在本地maven仓库引用该包的对应路径下。 否则，不会自动下载工具包的依赖包。网上有很多教人将依赖包打入工具包中形成一个\"大而全\"的jar包的教程，想想这肯定是不正确的。\n1.3. @Value注入配置项 在构建工具包时，@Value注解会使用工具包的application.yml或.properties文件。但将工具包引入到实际开发应用后，@Value注解使用的application.yml文件则是实际开发应用的而非工具包的。这是由于classpath的改变。 我构建工具包时，明确需要@Value注解使用被实际开发应用的配置文件。如果你希望使用工具包自带的application.yml文件，或许有解决办法，例如增加classpath或者使用@TestProperties注解等等，**我没有尝试，无法给出准确答案。**实际上，我不推荐这样做，如果是一份写死的静态资源，我认为不需要使用@Value注解。 @Value注解则涉及null的问题，务必加上默认值null，因为这是一个工具包，不一定含有application.yml配置文件。并且，在使用相关变量时，进行空校验处理。\n1.4. 使用自定义jar包时，扫描不到@Service 由于自定义jar包基于spring-boot框架，那么在别的spring-boot应用里，可能没被指定包扫描路径。需要在启动项注解加上配置，如下所示：\n@SpringBootApplication(scanBasePackages = {\"org.example.xxx\"}) 如果实际开发应用和工具包的包名不同，那么还需要加上实际开发应用的包名称，否则，应用中的类会无法被扫描注入到spring容器中","15-反编译导致变量名混淆注释丢失#1.5. 反编译导致变量名混淆、注释丢失":"反编译导致变量名混淆和注释丢失无法避免，那么只能在maven打包时，增加添加打包源码的步骤，pom.xml配置如下：\n\u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-source-plugin\u003c/artifactId\u003e \u003cversion\u003e3.2.1\u003c/version\u003e \u003cconfiguration\u003e \u003cattach\u003etrue\u003c/attach\u003e \u003c/configuration\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cphase\u003ecompile\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003ejar\u003c/goal\u003e \u003c/goals\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e 在使用idea查看.class文件时，则可以下载到source.jar文件","2-applicationyml自动补全提示配置#2. application.yml自动补全提示配置":"在src/main/resources下新建META-INF目录，新建additional-spring-configuration-metadata.json文件，相关配置和语法见Configuration Metadata\n在pom.xml文件中新增依赖，如下所示：\n\u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-configuration-processor\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e ","3-maven-deploy到nexus库步骤#3. maven deploy到Nexus库步骤":"在pom.xml文件中增加以下配置：\n\u003cdistributionManagement\u003e \u003crepository\u003e \u003cid\u003eNexus-Releases\u003c/id\u003e \u003cname\u003eMy Nexus\u003c/name\u003e \u003curl\u003ehttp://url:port/repository/releases/\u003c/url\u003e \u003c/repository\u003e \u003c/distributionManagement\u003e repository为发行版仓库，发行版的版本后缀添加-RELEASE，或者直接是数字，deploy时会自动部署到发行版仓库。还有一类为snapshotRepository快照版仓库，需要加-SNAPSHOT后缀。\n在maven的配置文件中settings.xml下增加：\n\u003cservers\u003e \u003cserver\u003e \u003cid\u003eNexus-Releases\u003c/id\u003e \u003cusername\u003eadmin\u003c/username\u003e \u003cpassword\u003e123456\u003c/password\u003e \u003c/server\u003e \u003c/servers\u003e 注意，distributionManagement下repository或者snapshotRepository配置的id需要和server下的id一致\n之后，执行mvn clean deploy即可.","前言#前言":""},"title":"基于spring-boot开发自定义jar包Notes"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/":{"data":{"加密解密#加密、解密":"加密、解密国密算法介绍\nHutool参考文档\nPython\u0026Java互通rsa加密解密 - UCloud云社区\nJava RSA、AES加解密，Python RSA、AES加解密","系统鉴权设计#系统鉴权设计":"Sa-Token"},"title":"接口设计"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/%E6%8E%A5%E5%8F%A3%E5%AE%89%E5%85%A8%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF/":{"data":{"":" 前后端分离的应用中，接口调用的安全性如何保证？\n前后端分离的应用中，可以使用以下方法来保证接口调用的安全性：\n身份认证：使用用户名和密码或者其他身份验证机制来确保只有授权的用户才能访问接口。 授权：使用角色或权限控制，确保用户只能访问其拥有权限的资源。 加密：使用 HTTPS 加密通信来保护数据传输过程中的隐私。 签名：在请求中添加签名，防止数据被篡改。 限流：限制单位时间内允许的请求次数，防止恶意攻击。 不能通过限制ip的方式进行吗？\n限制 IP 地址是一种常用的方式来保护接口的安全性。 通过限制访问接口的 IP 地址范围，可以防止非法用户或攻击者访问接口。 但是这种方式有一定的缺点：\nIP 地址可能会变化，如果限制的 IP 地址范围不准确或不能及时更新，可能会导致合法用户无法访问。 通过代理服务器或 VPN 访问接口时， IP 地址会发生变化，导致限制失效。 黑客可以通过欺骗 IP 地址来绕过限制。 因此，限制 IP 地址可能是一种有效的保护接口安全性的方式，但并不能完全保证接口的安全性，应该与其他安全措施相结合使用。\n如何设计签名，避免签名的伪造？\n签名是一种常用的方式来防止数据被篡改，但如果签名不能有效防止伪造，就会失去保护数据完整性的意义。因此，为了避免签名的伪造，需要设计一种机制来确保签名的有效性。\n使用私钥加密：在请求数据中添加签名时，使用私钥加密，只有拥有私钥的服务器才能解密并验证签名的有效性。 添加时间戳：在请求数据中添加时间戳，防止签名被重放攻击。 签名的字段限制：限制签名的字段，只对必要的字段进行签名，防止签名被篡改。 不要在请求中暴露私钥：避免在请求中暴露私钥，防止私钥被盗用。 限制签名的次数：限制签名的使用次数，防止签名被重复使用。 使用标准算法签名：使用高强度的算法，例如: HMAC-SHA256，RSA-SHA256等进行签名 使用以上方法结合，可以有效防止签名被伪造，保证请求数据的完整性。 假设前端使用vue写，然后用私钥加密，这样代码不就暴露在网页中了吗？是否有办法避免？\n使用私钥加密签名是一种常用的方式，但在前端使用 Vue 时确实存在私钥泄露的风险。因为前端代码会被打包成 JavaScript 文件，在网页中可以被直接访问，如果私钥被泄露，就会导致签名被伪造。 针对这种情况，可以使用以下几种方式来避免私钥泄露：\n后端提供签名服务：将签名的操作移到后端服务器上进行，前端只需要将请求数据发送给后端，由后端服务器进行签名。这样私钥就不会被暴露在前端代码中了。 使用混淆工具：使用混淆工具来混淆前端代码，使得私钥在网页中看起来是一堆乱码，防止被人直接读取。 使用动态签名：使用动态签名，每次请求都生成新的签名，防止签名被重复使用 使用对称加密+非对称加密：使用非对称加密来加密私钥，使用对称加密来加密请求数据，这样私钥就不会被暴露在前端代码中了 这些方式都可以有效避免私钥泄露的风险， 继续\n并且保证签名的有效性。 需要注意的是, 除了第一种方式，其他方法都有可能会被高技能的黑客绕过，所以不能完全依赖这些方法来保证签名的安全性。最好的做法是结合多种安全措施来保证签名的安全性。"},"title":"接口安全设计思路"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/%E6%8E%A5%E5%8F%A3%E5%AE%89%E5%85%A8%E8%AE%BE%E8%AE%A1/":{"data":{"#":"","前后端编码方式选择#前后端编码方式选择":"","前端-响应解密#前端-响应解密":"","前端-请求加密#前端-请求加密":"","加密#加密":"","后端-响应加密#后端-响应加密":"","后端-请求解密#后端-请求解密":"","字符集编码解码加解密#字符集，编码解码，加解密":"","报文密文处理-sm4#报文密文处理-SM4":" 前端对请求报文加密，后端对请求报文解密； 后端对响应报文加密，前端对响应报文解密 将整个报文体加密为一个密文字符串\n分组加密模式 ECB、CBC、PCBC、CFB、OFB、CTR_fengwang0301的博客-CSDN博客_分组密码ecb模式\nECB ECB模式加密过程 ECB模式解密过程\nCBC CBC模式加密过程\nCBC模式解密过程","数据安全处理-最终方案#数据安全处理-最终方案":"需要使用国密算法，查阅网上资料，需引入Bouncy Castle依赖\n\u003cdependency\u003e \u003cgroupId\u003eorg.bouncycastle\u003c/groupId\u003e \u003cartifactId\u003ebcprov-jdk15to18\u003c/artifactId\u003e \u003cversion\u003e1.69\u003c/version\u003e \u003c/dependency\u003e 并引入hutools-crypto来使用SM2，SM3，SM4 Hutool参考文档-国密算法工具-SmUtil\n// 生成sm4的key byte[] key = SecureUtil.generateKey(\"SM4\").getEncoded(); String sm4KeyBase64 = Base64.getEncoder().encodeToString(key); String sm4KeyBase64U = new String(sm4k.getBytes(), StandardCharsets.UTF_8); // 生成sm2的key值对 KeyPair pair = SecureUtil.generateKeyPair(\"SM2\"); byte[] privateKey = pair.getPrivate().getEncoded(); String privateKeyBase64 = Base64.getEncoder().encodeToString(privateKey); String privateKeyBase64U = new String(privateKeyS.getBytes(), StandardCharsets.UTF_8); byte[] publicKey = pair.getPublic().getEncoded(); String publicKeyBase64 = Base64.getEncoder().encodeToString(publicKey); String publicKeyBase64U = new String(publicKeyS.getBytes(), StandardCharsets.UTF_8); 字符集，编码解码，加解密字符集是bytes和string互相转换，如果字符集没选好，就容易出乱码。一般用UTF_8. 编码一般有Hex(Base16)和Base64，编码解码是为了避免有一些特殊字符，影响传输。 Base64UrlSafe（可以避免url中特殊字符出问题） 加解密是为了避免信息泄漏。\n前后端编码方式选择前端 后端hutools-crypto、Bouncy Castle，既可以base16，又可以base64. hex,base64,urlencode编码方案对比 SM4算法主要包含5种基本模式：ECB、CBC、CFB、OFB，CTR（后4种都是ECB算法模块衍生而来）；与MAC结合还诞生了GCM，CCM等高级模式。 SM4分组密码算法介绍_浩雪峰的博客-CSDN博客_sm4分组密码算法\n考虑性能和安全性，没有一种模式是绝对的最优选择，不同的模式适用于不同的应用场景。 分组加密模式 ECB、CBC、PCBC、CFB、OFB、CTR_fengwang0301的博客-CSDN博客_分组密码ecb模式\n补码padding SM4中的padding是指在加密前对明文进行填充，使其长度为16字节的整数倍，因为SM4是一种分组密码，每个分组的长度为16字节。12\n不同的padding方式有不同的填充规则，常见的有PADDING_PKCS5、PADDING_PKCS7、PADDING_ISO10126、PADDING_ANSI_X923等。1\n填充的目的是为了保证密文的完整性和一致性，同时也可以防止一些攻击，例如重放攻击和填充攻击。2\nSM4用什么padding比较好，取决于你的加密模式和数据格式。12\n一般来说，如果你使用的是ECB模式或者CBC模式，那么你需要对明文进行padding，因为这两种模式要求明文的长度是16字节的倍数。12\n如果你使用的是CFB模式、OFB模式或者CTR模式，那么你不需要对明文进行padding，因为这三种模式可以将分组密码转化为流密码，可以对任意长度的明文进行加密。12\n常见的padding方式有PADDING_PKCS5、PADDING_PKCS7、PADDING_ISO10126、PADDING_ANSI_X923等，它们的区别主要在于填充的字节的值和验证的方式。12\nPADDING_PKCS5和PADDING_PKCS7的填充方式是在明文的末尾添加N个字节，每个字节的值都是N，其中N是从1到16的整数，表示需要填充的字节数。12\nPADDING_ISO10126的填充方式是在明文的末尾添加N个字节，其中最后一个字节的值是N，其余的字节的值是随机的。12\nPADDING_ANSI_X923的填充方式是在明文的末尾添加N个字节，其中最后一个字节的值是N，其余的字节的值都是0。12\n在解密的时候，需要根据填充的字节的值来判断填充的字节数，然后去掉填充的字节，恢复原始的明文。12\n如果你的数据格式是二进制的，那么你可以选择任意一种padding方式，只要加密和解密的时候保持一致即可。12\n如果你的数据格式是文本的，那么你最好选择PADDING_PKCS5或者PADDING_PKCS7，因为这两种方式可以避免填充的字节和原始的明文的字符冲突，导致解密的时候出错。12\n综上所述，SM4用什么padding比较好，没有一个确定的答案，你需要根据你的具体的需求和场景来选择合适的padding方式。12\n加密方式1：对每个json字段进行加密？\n方式2：对整个请求报文进行加密（网金）请求报文本身是一个JSON，然后加密了之后以参数形式传递。\n前端-请求加密 js SM4对称加密\n后端-请求解密 java SM4对称解密\n后端-响应加密 java SM4对称加密\n前端-响应解密 js SM4对称解密\n数据防重放方案1：预请求 预请求，获取redis中的某个随机数；然后，根据随机数去访问接口，就会清掉redis的key-value. 通过这种方式进行防止数据重放。\n没有redis，只能放在数据库中，并且需要设计两个请求，每个请求涉及DB的写操作，建议用方案2.\n方案2：timestamp + nonce 方案 10 种保证接口数据安全的方案！ 时间戳超时机制也是有漏洞的，如果是在时间差内，黑客进行的重放攻击，那就不好使了。可以使用timestamp + nonce方案。\nnonce指唯一的随机字符串，用来标识每个被签名的请求。我们可以将每次请求的nonce参数存储到一个“set 集合”中，或者可以 json 格式存储到数据库或缓存中。每次处理 HTTP 请求时，首先判断该请求的nonce参数是否在该“集合”中，如果存在则认为是非法请求。\n然而对服务器来说，永久保存nonce的代价是非常大的。可以结合timestamp来优化。因为timstamp参数对于超过3min的请求，都认为是非法请求，所以我们只需要存储3min的nonce参数的“集合”即可。\n方案 优点 缺点 数据库 完全防重放 每次需要查询DB，速率慢；需要写定时任务清理nonce；改造量大 本地缓存 速率快；缓存失效机制自动清理；改造少 假设有n台机器，重放攻击可以到达n次。 不同缓存框架比较 （很全面）SpringBoot 使用 Caffeine 本地缓存 SpringBoot: Implement caching with Caffeine. Spring Cache的使用教程：注解形式和api接口形式，以及调用内部方法注解失效的原因_不如敲代码的博客-CSDN博客 数据防篡改数据报文加签验签，是保证数据传输安全的常用手段，它可以保证数据在传输过程中不被篡改。以前我做的企业转账系统，就用了加签验签。 签名/验签时，将参数名ASCII码从小到大排序_vamViolet的博客-CSDN博客_参数名ascii码从小到大排序\n2.1 什么是加签验签呢？ 数据加签：用 Hash算法（SM3）把原始请求参数生成报文摘要，然后用私钥对这个摘要进行加密，就得到这个报文对应的数字签名sign（这个过程就是加签）。通常来说，请求方会把数字签名和报文原文一并发送给接收方。 验签：接收方拿到原始报文和数字签名（sign）后，用**同一个 Hash 算法（**比如都用 SM3）从报文中生成摘要 A。另外，用对方提供的公钥对数字签名进行解密，得到摘要 B，对比 A 和 B 是否相同，就可以得知报文有没有被篡改过。 其实加签，按我的理解，就是把请求参数，按照一定规则，利用hash算法+加密算法生成一个唯一标签sign。验签的话，就是把请求参数按照相同的规则处理，再用相同的hash算法，和对应的密钥解密处理，以对比这个签名是否一致。\n请求报文参考{ \"timestamp\":\"xxxx\", \"nonce\":\"xxxx\", \"sign\":\"xxxxxxxxxxxxxxx\", } 数据安全处理-最终方案","数据签名和验签#数据签名和验签":" 数据加签：原始报文按Key值排序（ASCALL码），处理成类似a=1\u0026b=2的字符串，再用 Hash算法（SM3）把原始请求参数生成报文摘要，就得到这个报文对应的数字签名sign（这个过程就是加签）。通常来说，请求方会把数字签名和报文原文密文一并发送给接收方。 验签：接收方拿到原始报文和数字签名（sign）后，用**同一个 Hash 算法（**比如都用 SM3）从报文中生成摘要 A。对比 A 和 请求的数字签名 是否相同，就可以得知报文有没有被篡改过。 ","数据防篡改#数据防篡改":"","数据防重放#数据防重放":"","数据防重放-1#数据防重放":"timestamp + nonce 方案 nonce指唯一的随机字符串，用来标识每个被签名的请求。我们可以将每次请求的nonce参数存储到一个“set 集合”中，或者可以 json 格式存储到数据库或缓存中。每次处理 HTTP 请求时，首先判断该请求的nonce参数是否在该“集合”中，如果存在则认为是非法请求。 然而对服务器来说，永久保存nonce的代价是非常大的。可以结合timestamp来优化。因为timstamp参数对于超过3min的请求，都认为是非法请求，所以我们只需要存储3min的nonce参数的“集合”即可。\n方案 优点 缺点 nonce放到数据库 完全防重放 每次需要查询DB，速率慢；需要写定时任务清理nonce；改造量大 nonce放到本地缓存 速率快；缓存失效机制自动清理；改造少 假设有n台机器，重放攻击可以到达n次。 ","方案1预请求#方案1：预请求":"","方案2timestamp--nonce-方案#方案2：timestamp + nonce 方案":"","补码padding#补码padding":"","请求报文参考#请求报文参考":""},"title":"接口安全设计"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/":{"data":{"字典表设计#字典表设计":"字典表设计通用数据字典表结构设计\n实现数据字典的缓存、加载、刷新和映射的集成框架 - 掘金\nRuoYi-Plus文档预览 - Gitee.com"},"title":"数据库设计"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%BB%BA%E8%AE%AE/":{"data":{"元数据设计#元数据设计":"元数据设计","必要字段设计#必要字段设计":""},"title":"数据库设计建议"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E6%9D%83%E7%9B%8A%E7%B3%BB%E7%BB%9F/":{"data":{"权益活动系统方案设计参考网址#权益、活动系统方案设计参考网址":"权益、活动系统方案设计参考网址这个是真的厉害，高并发场景下的订单和库存处理方案，讲的很详细了！_androidstarjack的博客-CSDN博客\n转载：设计一个会员权益体系应该怎样做 – 26点的博客\n深度推荐｜史上最完整用户成长体系设计分析，附6大案例 | 人人都是产品经理\n面霸：秒杀系统如何设计？ - TeqNG\n这是我读过写得最好的【秒杀系统架构】分析与实战！ - 腾讯云开发者社区-腾讯云"},"title":"权益系统"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F%E5%92%8C%E7%8A%B6%E6%80%81%E6%9C%BA/":{"data":{"":"本文代码位于GitHub - felixlyd/order-state-sample: 订单状态机示例项目 基于cola状态机\n彻底搞懂Spring状态机原理，实现订单与物流解耦\n详解状态模式以及Spring状态机 - mdnice 墨滴\n管理订单状态，该用上状态机吗？-阿里云开发者社区\n实现一个状态机引擎，教你看清DSL的本质_张建飞（Frank）的博客-CSDN博客\n高德打车通用可编排订单状态机引擎设计-阿里云开发者社区\n状态设计模式\n设计模式在电商业务下的实践——状态模式 - 掘金\n状态机引擎在vivo营销自动化中的深度实践 | 引擎篇02 - 墨天轮"},"title":"状态模式和状态机"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%A7%AF%E5%88%86%E5%95%86%E5%9F%8E/":{"data":{"":" https://cdn款优秀积分商城产品需具备的准则与功能梳理 | 人人都是产品经理 "},"title":"积分商城"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1/":{"data":{"":" 基于spring cloud gateway+sentinel构建微服务网关 基于spring cloud+zuul+sentinel开发微服务网关 qiqi-gateway "},"title":"网关设计"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1/%E5%9F%BA%E4%BA%8Espring-cloud+zuul+sentinel%E5%BC%80%E5%8F%91%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3/":{"data":{"hystrix原理#hystrix原理":"","sentinel-dashboard对流量的监控#Sentinel-Dashboard对流量的监控":"","sentinel多台机器#Sentinel多台机器":"","sentinel整合到zuul上#Sentinel整合到zuul上":"","sentinel规则持久化#Sentinel规则持久化":"","sentinel规则语法#Sentinel规则语法":"","zuul和feign的区别和联系#zuul和feign的区别和联系":"","前言#前言":"","如何修改feign的默认负载均衡策略#如何修改feign的默认负载均衡策略？":"","对hystrix进行监控#对hystrix进行监控？":"","引入hystrix#引入hystrix？":"","引入sentinel#引入Sentinel":"","引入zuul#引入zuul":"","数据安全处理xml和json转换#数据安全处理、xml和json转换":"数据安全处理内容：加解密、解签名、防重放验证\nzuul流程示意图 zuul 1.x流程 zuul转发：http请求-\u003eprefilter-\u003eroutingfilter-\u003e后端应用-\u003epostfilter-\u003ehttp响应\nzuul 2.x流程 spring cloud gateway 流程 方案1：http请求和esb请求由zuul网关转发 http请求能通过controller层手动转发，esb请求（netty请求）转发需要发起新的http请求访问后端应用。\nhttp请求通过controller层手动转发请求 方式1：\n@PostMapping(\"/{path1}\") public String test(@PathVariable String path1){ log.info(\"/rule/\" + path1); return \"/rule/\" + path1; } @PostMapping(\"/{path1}/{path2}\") public String test2(@PathVariable String path1, @PathVariable String path2){ log.info(\"/rule/\" + path1 + \"/\" + path2); return \"/rule/\" + path1 + \"/\" + path2; } @PostMapping(\"/{path1}/{path2}/{path3}\") public String test2(@PathVariable String path1, @PathVariable String path2, @PathVariable String path3){ log.info(\"/rule/\" + path1 + \"/\" + path2 + \"/\" + path3); return \"/rule/\" + path1 + \"/\" + path2 + \"/\" + path3; } spring boot 2.6.0之前对多级路径支持不友好.\n方式2：\n@PostMapping(\"/**\") public String test2(HttpServletRequest request){ String path = request.getServletPath(); List\u003cString\u003e paths = Arrays.stream(path.split(\"/\")).filter(i -\u003e !i.contains(\"rule-controller\")).collect(Collectors.toList()); String zuulPath = \"/rule/\" + String.join(\"/\", paths); log.info(zuulPath); return zuulPath; } esb请求发起新的http请求 @PostMapping(\"/**\") public String test2(HttpServletRequest request, @RequestBody Map\u003cString, String\u003e reqMap){ log.info(\"解密--\"); String path = request.getServletPath(); List\u003cString\u003e paths = Arrays.stream(path.split(\"/\")).filter(i -\u003e !i.contains(\"rule-controller\")).collect(Collectors.toList()); String zuulPath = \"http://localhost:8081/rule\" + String.join(\"/\", paths); RestTemplate restTemplate = new RestTemplate(); String responseBody = restTemplate.postForObject(zuulPath, reqMap, String.class); log.info(\"加密--\"); return responseBody; } esb请求发起http请求如果走feign，则不会过zuul的转发；如果走restTemplate，需要告诉ip:port发送给网关自己（可以走localhost:port，也可以走域名（F5）去负载均衡），然后到zuul，再由zuul转发。\nPRE-网关转发请求前 前端请求报文需要数据安全设计（解密、防重放、签名），以及esb请求报文协议转换需要xml转换为json\n不自定义zuul的prefilter。对于http请求，先进行数据安全处理、报文协议转换，然后通过controller转发到zuul，再由zuul做转发；对于esb请求，先进行数据安全处理、报文协议转换，然后发起新的http请求到zuul，再由zuul做转发。 自定义zuul的prefilter。对于http请求，在zuul的prefilter中实现数据安全处理、报文协议转换；对于esb请求，发起新的http请求到zuul，再由zuul的prefilter进行数据安全处理、报文协议转换。 zuul 1.x不支持netty转发\nPOST-网关转发请求后 前端请求的响应报文需要加密，以及esb请求的响应报文需要json转xml\n不自定义zuul的postfilter。对于http请求通过controller转发到zuul，不太可能，因为zuul转发会通过postfilter直接返回调用方。必须在postfilter中处理响应报文，或者通过研究源码写切面从postfilter捞取响应报文；对于esb请求，发起了新的http请求，可以实现。但是，发起http请求如果走feign，则不会过zuul的转发；如果走restTemplate，需要告诉ip:port发送给网关自己（可以走localhost:port，也可以走域名（F5）去负载均衡），然后到zuul，再由zuul转发。 自定义zuul的postfilter。对于http请求，在zuul的postfilter中实现数据安全处理、报文协议转换；对于esb请求，发起新的http请求到zuul，再由zuul的postfilter进行数据安全处理、报文协议转换。 综上所述，如果是http请求，直接在zuul网关中的prefilter和postfilter中处理请求响应报文最简便。如果是esb请求，发起新的http请求给网关自己，也直接在zuul网关中prefilter和postfilter处理请求响应报文。\n如果均走prefilter和postfilter，就需要区分二者。因为http请求只需要数据安全处理和限流而不需要报文协议转换，esb请求只需要报文协议转换不涉及数据安全处理，可能涉及限流。\n方案2：http请求走zuul网关转发，esb请求沿用feign客户端转发 http请求：分别在zuul网关中的prefilter和postfilter中实现对应数据安全处理。 esb请求：沿用之前的流程 因为http请求和esb请求处理请求响应报文规则不同，因此http请求走zuul，改prefilter和postfilter；esb请求在feign客户端前后另加一套逻辑。","整合feignribbon#整合feign、ribbon？":"","方案讨论#方案讨论":"前言本文代码于GitHub - felixlyd/zuul-sentinel-sample: 该demo：1. zuul、ribbon、hystrix实现网关路由转发、限流熔断；2. 引入feign实现微服务调用；3. 考虑接口安全设计\n由于项目历史遗留问题，需要使用较老的spring cloud组件集成，并且需要契合老应用做局部改造 因此，有以下考虑和要求：\nzuul、ribbon、hystrix实现网关路由转发、限流熔断; 引入feign实现微服务调用; 引入Sentinel实现限流和熔断; 考虑接口安全设计; 能够契合老spring cloud应用做局部改造; 能够对限流进行监控; 考虑两台网关的限流; 整合feign、ribbon？Ribbon：Spring Cloud负载均衡与服务调用组件（非常详细） 引入feign后，feign默认集成了ribbon做负载均衡. feign的默认负载均衡策略为轮询\n如何修改feign的默认负载均衡策略？ OpenFeign修改负载均衡策略_wx62e0b69890c77的技术博客_51CTO博客 Spring Cloud Feign 负载均衡策略配置_guoqiusheng的博客-CSDN博客 以上为两种方式，尚未实践\nzuul和feign的区别和联系 zuul作为整个应用的流量入口，接收所有的请求，如app、网页等，并且将不同的请求转发至不同的处理微服务模块，其作用可视为nginx；feign则是将当前微服务的部分服务接口暴露出来，并且主要用于各个微服务之间的服务调用。 zuul默认集成hystrix和ribbon，并基于http通讯的，用于代理服务；feign默认集成ribbon，可以通过配置集成hystrix，是在服务互相调用时使用，仿rpc通讯。 引入hystrix？zuul默认集成hystrix，hystrix只能做微服务应用层级的限流，sentinel可以做到接口级的限流\nhystrix原理 Spring Cloud - Hystrix 原理解析 - 掘金\nhystrix的限流，是基于设置最大的线程池/信号量，来进行限流，因此，是有限的限流\n对hystrix进行监控？ 需引入hystrix-dashboard和turbine\n引入zuulspring cloud zuul 原理简介及使用 Zuul 网关转发的五种方式 - qiuxuhui - 博客园\n引入Sentinel版本说明 · alibaba/spring-cloud-alibaba Wiki 网关限流 · alibaba/spring-cloud-alibaba Wiki introduction | Sentinel Sentinel/sentinel-demo/sentinel-demo-zuul-gateway at master · alibaba/Sentinel 一篇文章彻底学会使用Spring Cloud Alibaba Sentinel GitHub - sentinel-group/sentinel-awesome: A curated list of awesome things (resource, sample, extensions) for Sentinel 阿里巴巴开源限流降级神器Sentinel大规模生产级应用实践\nSentinel整合到zuul上 sentinel对接口级进行流控 sentinel 1.6.0 引入了 Sentinel API Gateway Adapter Common 模块，此模块中包含网关限流的规则和**自定义 API **的实体和管理逻辑：\nGatewayFlowRule：网关限流规则，针对 API Gateway 的场景定制的限流规则，可以针对不同 route 或自定义的 API 分组进行限流，支持针对请求中的参数、Header、来源 IP 等进行定制化的限流。 ApiDefinition：用户自定义的 API 定义分组，可以看做是一些 URL 匹配的组合。比如我们可以定义一个 API 叫 my_api，请求 path 模式为 /foo/** 和 /baz/** 的都归到 my_api 这个 API 分组下面。限流的时候可以针对这个自定义的 API 分组维度进行限流。 Q：sentinel有分组api限流的功能，如果我的两个接口都放在同一个api下面，设置10s内最多访问5次。那么我实际的访问中，是两个接口一共每10s内最多访问5次，还是两个接口分别10s内最多访问5次？\nA：如果你的两个接口都放在同一个 API 下面，设置 10s 内最多访问 5 次，那么你实际的访问中，两个接口一共每 10s 内最多访问 5 次。这意味着如果一个接口被访问了 5 次，另一个接口在 10s 内不能再被访问。\n应用被sentinel识别为网关 方式1：启动参数\njava -Dcsp.sentinel.app.type=1 -jar zuul-gateway.jar 方式2：配置文件 在资源目录下新建sentinel.properties文件，使用以下配置\ncsp.sentinel.app.type=1 方式3：代码 在main主函数中加入以下代码\nSystem.setProperty(\"csp.sentinel.app.type\", \"1\") Sentinel-Dashboard对流量的监控 sentinel-dashboard示例图 相比hystrix的dashboard sentinel监控数据持久化？ dashboard实时监控仅能查看5分钟内的metric数据，持久化到数据库中？需要自行实现相关接口代码。\nSentinel规则持久化 代码 文件 关系数据库 默认情况下：sentinel-dashboard能够读取代码中的规则配置到内存中，并且可以热修改\nSentinel规则语法 流控规则 流量控制 · alibaba/spring-cloud-alibaba Wiki\n一条限流规则主要由下面几个因素组成，我们可以组合这些元素来实现不同的限流效果：\nresource：资源名，即限流规则的作用对象 count: 限流阈值 grade: 限流阈值类型（QPS 或并发线程数） limitApp: 流控针对的调用来源，若为 default 则不区分调用来源 strategy: 调用关系限流策略 controlBehavior: 流量控制效果（直接拒绝、Warm Up、匀速排队） 熔断降级 熔断降级 · alibaba/spring-cloud-alibaba Wiki\n熔断降级规则（DegradeRule）包含下面几个重要的属性：\nField 说明 默认值 resource 资源名，即规则的作用对象 grade 熔断策略，支持慢调用比例/异常比例/异常数策略 慢调用比例 count 慢调用比例模式下为慢调用临界 RT（超出该值计为慢调用）；异常比例/异常数模式下为对应的阈值 timeWindow 熔断时长，单位为 s minRequestAmount 熔断触发的最小请求数，请求数小于该值时即使异常比率超出阈值也不会熔断（1.7.0 引入） 5 statIntervalMs 统计时长（单位为 ms），如 60*1000 代表分钟级（1.8.0 引入） 1000 ms slowRatioThreshold 慢调用比例阈值，仅慢调用比例模式有效（1.8.0 引入） 系统规则 system-adaptive-protection | Sentinel\nSentinel/SystemGuardDemo.java at master · alibaba/Sentinel\nSentinel 做系统自适应保护的目的：\n保证系统不被拖垮 在系统稳定的前提下，保持系统的吞吐量 系统保护规则是应用整体维度的，而不是资源维度的，并且仅对入口流量生效。入口流量指的是进入应用的流量。 系统自适应限流：Load（仅对 Linux/Unix-like 机器生效）：当系统 load1 超过阈值，且系统当前的并发线程数超过系统容量时才会触发系统保护。系统容量由系统的 maxQps * minRt 计算得出。设定参考值一般是 CPU cores * 2.5 .\n热点参数 parameter-flow-control | Sentinel\n何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制。比如：\n商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制 用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制 热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。\nSentinel 利用 LRU 策略统计最近最常访问的热点参数，结合令牌桶算法来进行参数级别的流控。\n在网关限流中，GatewayFlowRule 下的paramItem 会配置该规则是热点规则还是普通流控规则\n黑白名单？ 网关限流中没有 黑白名单控制 · alibaba/spring-cloud-alibaba Wiki\n网关api限流规则 其中网关限流规则 GatewayFlowRule 的字段解释如下：\nresource：资源名称，可以是网关中的 route 名称或者用户自定义的 API 分组名称。 resourceMode：规则是针对 API Gateway 的 route（RESOURCE_MODE_ROUTE_ID）还是用户在 Sentinel 中定义的 API 分组（RESOURCE_MODE_CUSTOM_API_NAME），默认是 route。 grade：限流指标维度，同限流规则的 grade 字段。 count：限流阈值 intervalSec：统计时间窗口，单位是秒，默认是 1 秒。 controlBehavior：流量整形的控制效果，同限流规则的 controlBehavior 字段，目前支持快速失败和匀速排队两种模式，默认是快速失败。 burst：应对突发请求时额外允许的请求数目。 maxQueueingTimeoutMs：匀速排队模式下的最长排队时间，单位是毫秒，仅在匀速排队模式下生效。 paramItem：**参数限流配置。若不提供，则代表不针对参数进行限流，该网关规则将会被转换成普通流控规则；否则会转换成热点规则。**其中的字段： parseStrategy：从请求中提取参数的策略，目前支持提取来源 IP（PARAM_PARSE_STRATEGY_CLIENT_IP）、Host（PARAM_PARSE_STRATEGY_HOST）、任意 Header（PARAM_PARSE_STRATEGY_HEADER）和任意 URL 参数（PARAM_PARSE_STRATEGY_URL_PARAM）四种模式。 fieldName：若提取策略选择 Header 模式或 URL 参数模式，则需要指定对应的 header 名称或 URL 参数名称。 pattern：参数值的匹配模式，只有匹配该模式的请求属性值会纳入统计和流控；若为空则统计该请求属性的所有值。（1.6.2 版本开始支持） matchStrategy：参数值的匹配策略，目前支持精确匹配（PARAM_MATCH_STRATEGY_EXACT）、子串匹配（PARAM_MATCH_STRATEGY_CONTAINS）和正则匹配（PARAM_MATCH_STRATEGY_REGEX）。（1.6.2 版本开始支持） 用户可以通过 GatewayRuleManager.loadRules(rules) 手动加载网关规则，或通过GatewayRuleManager.register2Property(property) 注册动态规则源动态推送（推荐方式）。\n集群流控？ 网关限流中没有 为什么要使用集群流控呢？假设我们希望给某个用户限制调用某个 API 的总 QPS 为 50，但机器数可能很多（比如有 100 台）。这时候我们很自然地就想到，找一个 server 来专门来统计总的调用量，其它的实例都与这台 server 通信来判断是否可以调用。这就是最基础的集群流控的方式。 另外集群流控还可以解决流量不均匀导致总体限流效果不佳的问题。假设集群中有 10 台机器，我们给每台机器设置单机限流阈值为 10 QPS，理想情况下整个集群的限流阈值就为 100 QPS。**不过实际情况下流量到每台机器可能会不均匀，会导致总量没有到的情况下某些机器就开始限流。**因此仅靠单机维度去限制的话会无法精确地限制总体流量。而集群流控可以精确地控制整个集群的调用总量，结合单机限流兜底，可以更好地发挥流量控制的效果。\nSentinel多台机器 后端 后端机器限流是单机限流，如果有2台机器，希望QPS限制到100，那么单机设为50.\n网关 例如某个单机app的QPS限制100，启动两台网关，两个网关加起来限制该app的QPS为100.\n方案讨论","网关限流#网关+限流":" zuul+sentinel 所有请求都会经过监听 sentinel 手动加api接口进行监听，就不会影响之前的代码 adapter往一个adapter2上转发（双网关思路），adapter2为一个新的用于做数据安全、限流、分发的网关 "},"title":"基于spring cloud+zuul+sentinel开发微服务网关"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1/%E5%9F%BA%E4%BA%8Espring-cloud-gateway+sentinel%E6%9E%84%E5%BB%BA%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3/":{"data":{"sentinel指定目录失效#sentinel指定目录失效":"","代码#代码":"","全局异常处理#全局异常处理":"","前言#前言":"","加密解密#加密解密":"jmeter加密解密（加密篇）","参考#参考":"","参考-1#参考":"","参考-2#参考":"","坑指南#坑指南":"","性能测试#性能测试":"前言本文代码于GitHub - felixlyd/gateway-sentinel-sample: spring cloud gateway+sentinel+数据安全处理\nspring cloud alibaba对应boot、cloud版本\nSpring Cloud Gateway微服务网关教程\nSpring Cloud Gateway-官网介绍\nspring cloud gateway实现API发布、路由转发 引入sentinel实现限流和熔断；引入sentinel-dashboard实现限流监控 考虑接口安全设计：密文、防重放、签名和验签 全局异常处理 日志配置 自动部署 actuator健康检查 打印日志？打哪些？请求和响应内容打不打？ 占用哪些端口？sentinel-dashboard和sentinel-client sentinel-dashboard部署在哪台机器？ 坑指南无法添加自定义API分组 现象： 经过反复验证测试，spring cloud gateway+sentinel搭建微服务网关时，硬编码配置限流规则时，无法配置自定义API分组Set\u003cApiDefinition\u003e 加载到GatewayApiDefinitionManager中。如果有相关硬编码，则启动时无法被sentinel dashboard识别，但限流规则仍然生效。而zuul+sentinel搭建微服务网关，硬编码配置限流规则可以配置自定义API分组。\n原因： 尚不明确，可能是sentinel的bug。\n测试过程： 硬编码限流规则，添加自定义API分组，启动sentinel dashboard，分别启动zuul和gateway，发现gateway不被识别，zuul被识别；2. 硬编码限流规则，去掉自定义API分组，启动sentinel dashboard，分别启动zuul和gateway，两者均被识别。3. 继续测试其他限流规则硬编码的影响，例如普通限流规则、热点限流规则、熔断降级规则、系统规则，均不会造成sentinel dashboard不识别网关应用的现象。 解决办法： **虽然sentinel+gateway无法通过代码硬编码配置自定义API分组，但gateway本身可以在路由配置中，将API分组为不同的路由，因此可以替代sentinel的自定义API分组。**后续如果进行规则持久化到DB的优化，有可能解决这一问题。\nsentinel指定目录失效 修改spring.cloud.sentinel.log.dir，不起作用，还是会到默认的~/logs/csp路径来。 经过测试，2.6.11不生效，2.6.3版本生效\n2021.0.4.0* Spring Cloud 2021.0.4 2.6.11 2021.0.1.0 Spring Cloud 2021.0.1 2.6.3 请求流重复获取参考 自定义过滤器 Spring Cloud Gateway—自定义过滤器 - 掘金\nSpring Cloud Gateway 4 自定义Filter - 路迢迢 - 博客园\n请求流重复获取的一些实现，但可能有问题？\nGateway网关自定义拦截器的不可重复读取数据_databufferutils.join_飘零未归人的博客-CSDN博客 SpringCloud Gateway自定义filter获取body中的数据为空_mameng1998的博客-CSDN博客\nSpringCloud Gateway读取Request Body方式 - 掘金\nSpring Cloud Gateway（读取、修改 Request Body） - 掘金\n讲了几种请求流重复获取方案的缺陷：Spring Cloud Gateway 之获取请求体（Request Body）的几种方式 - 码农的进击 - 博客园\n请求流获取和修改的官网API： Spring Cloud Gateway 结合官网API的实现： ModifyRequestBodyGatewayFilterFactory获取并修改请求体_GavinYCF的博客-CSDN博客\ngateway 网关过滤器之修改body内容\nSpring cloud gateway中的request是基于Web Flux，和普通的请求方式不同，不能直接按照普通的ServletRequest去处理请求数据，而是一个reactive.ServletRequest。\n代码 如果使用官网API的ModifyRequestBodyGatewayFilterFactory去修改请求数据，那么整个路由配置无法放在配置文件application.yml中，因为官网API需要一个RewriteFunction，在配置文件中只能写类名，类名是String，无法转换为RewriteFunction。官网API也声明了该过滤器只能通过Java代码实现。\n代码实现 package com.bocd.mkt.gatewaysentinelsample.config; import com.bocd.mkt.gatewaysentinelsample.filter.SecurityRequestFilter; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.cloud.gateway.route.RouteLocator; import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.http.MediaType; @Configuration public class GatewayRoutesConfig { @Autowired private SecurityRequestFilter securityRequestFilter; @Bean public RouteLocator routeLocator(RouteLocatorBuilder builder){ return builder.routes().route(\"test\", r -\u003e r .path(\"/api-a/**\") .filters(f -\u003e f .modifyRequestBody(String.class,String.class, MediaType.APPLICATION_JSON_VALUE, securityRequestFilter)) .uri(\"lb://RULE-LITEFLOW\") ).build(); } } 配置信息 解决方式1 继承ModifyRequestBodyGatewayFilterFactory，重写其配置，将其配置信息改为字符串类型，然后在实现方法中根据RewriteFunction类名去反射调用spring容器中的RewriteFunction。\n方式1可能有潜在问题，不然为啥gateway官方不这样做？\n解决方式2 按照ModifyRequestBodyGatewayFilterFactory的实现方式，简化其配置，在原方法中，为了能兼容大多数情况，该类需要传入一个输入类名、输出类名、请求的content-type和一个重写方法RewriteFunction，实际上，我们不需要这么多参数和步骤。\n解决方式3 按照官网API建议，将路由配置写在代码中。\n全局异常处理参考 全局异常处理 Spring Cloud Gateway-自定义异常处理 - duanxz - 博客园\n【SpringCloud】Spring Cloud Gateway 网关全局异常处理_springcloudgateway全局异常处理_islin_7的博客-CSDN博客\n优雅的自定义Spring Cloud Gateway全局异常处理\nsentinel的 api-gateway-flow-control | Sentinel\nGateway结合Sentinel1.8限流熔断管理以及自定义异常\nSpringCloud: gateway整合sentinel 自定义异常处理类_amadeus_liu2的博客-CSDN博客\nsentinel接入网关 自定义异常\n端点应用监控参考 Spring Boot 2.x系列【20】应用监控篇之Actuator入门案例及端点配置详解_management.endpoints_云烟成雨TD的博客-CSDN博客\n性能测试","打印日志#打印日志":"Spring Cloud Gateway, logging request/response\nSpringCloudGateway - Log incoming request url and corresponding route URI","无法添加自定义api分组#无法添加自定义API分组":"","端点应用监控#端点应用监控":"","请求流重复获取#请求流重复获取":"","责任链#责任链":"spring 设计模式中的责任链怎么实现？ - 知乎"},"title":"基于spring cloud gateway+sentinel构建微服务网关"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1/qiqi-gateway/":{"data":{"前言#前言":"","柒柒监控#柒柒监控":"前言 网关：spring-cloud-gateway 安全性：防重放、数据签名、验签 授权：sa-token 限流：sentinel 监控：spring-boot-admin 柒柒监控","监控安全sa-token或spring-security#监控安全：sa-token或spring-security":"公共配置类 SecureLoginProperties类\npackage com.github.felixlyd.config.properties; import lombok.Getter; import lombok.Setter; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Configuration; /** * class SecureLoginProperties: do something * * @author : liuyaodong * @date 2023/5/17 */ @ConfigurationProperties(\"qiqi-gateway.secure.login\") @Configuration @Getter @Setter public class SecureLoginProperties { private String loginUrl; private String logoutUrl; private String indexUrl; private String method; private String username; private String password; private String loginId; private String usernameField; private String passwordField; } SecureProperties类\npackage com.github.felixlyd.config.properties; import lombok.Getter; import lombok.Setter; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Configuration; import java.util.List; /** * class SecureProperties: do something * * @author : liuyaodong * @date 2023/5/17 */ @ConfigurationProperties(\"qiqi-gateway.secure\") @Configuration @Getter @Setter public class SecureProperties { /** * 拦截路由 */ private List\u003cString\u003e includeList; /** * 放行路由 */ private List\u003cString\u003e excludeList; private String originUrlSession; } sa-token`` sa-token通过参考sa-token全局过滤器实现Sa-Token 全局过滤器\npom文件 \u003c!-- 方案1：引入sa token--\u003e \u003cdependency\u003e \u003cgroupId\u003ecn.dev33\u003c/groupId\u003e \u003cartifactId\u003esa-token-spring-boot-starter\u003c/artifactId\u003e \u003c/dependency\u003e filter类和config配置 LoginFilter类\npackage com.github.felixlyd.filter; import cn.dev33.satoken.exception.BackResultException; import cn.dev33.satoken.exception.NotLoginException; import cn.dev33.satoken.filter.SaFilterErrorStrategy; import cn.dev33.satoken.router.SaRouter; import cn.dev33.satoken.session.SaSession; import cn.dev33.satoken.session.SaSessionCustomUtil; import cn.dev33.satoken.stp.StpUtil; import cn.dev33.satoken.util.SaResult; import cn.dev33.satoken.util.SaTokenConsts; import cn.hutool.core.util.StrUtil; import com.github.felixlyd.config.properties.SecureLoginProperties; import com.github.felixlyd.config.properties.SecureProperties; import lombok.extern.slf4j.Slf4j; import org.springframework.core.annotation.Order; import org.springframework.http.MediaType; import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * class LoginFilter: do something * * @author : liuyaodong * @date 2023/5/16 */ @Slf4j @Order(SaTokenConsts.ASSEMBLY_ORDER) public class LoginFilter implements Filter { private final SecureProperties secureProperties; private final SecureLoginProperties secureLoginProperties; /** * 异常处理函数：每次[认证函数]发生异常时执行此函数 */ public SaFilterErrorStrategy error = e -\u003e SaResult.error(e.getMessage()); public LoginFilter(SecureProperties secureProperties, SecureLoginProperties secureLoginProperties) { this.secureProperties = secureProperties; this.secureLoginProperties = secureLoginProperties; } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { try { HttpServletRequest request = (HttpServletRequest) servletRequest; HttpServletResponse response = (HttpServletResponse) servletResponse; // 拦截登录逻辑 if (StrUtil.equals(request.getMethod(), secureLoginProperties.getMethod()) \u0026\u0026 StrUtil.equals(request.getRequestURI(), secureLoginProperties.getLoginUrl())) { String username = request.getParameter(secureLoginProperties.getUsernameField()); String password = request.getParameter(secureLoginProperties.getPasswordField()); if (StrUtil.equals(username, secureLoginProperties.getUsername()) \u0026\u0026 StrUtil.equals(password, secureLoginProperties.getPassword())) { StpUtil.login(secureLoginProperties.getLoginId()); SaSession saSession = SaSessionCustomUtil.getSessionById(secureProperties.getOriginUrlSession()); String originUrl = String.valueOf(saSession.get(secureProperties.getOriginUrlSession())); response.sendRedirect(StrUtil.equals(originUrl, secureLoginProperties.getLoginUrl())? secureLoginProperties.getIndexUrl():originUrl); return; } else { throw new NotLoginException(\"用户名密码不正确！\", StpUtil.getLoginType(), \"-2\"); } } // 拦截注销逻辑 if(StrUtil.equals(request.getRequestURI(), secureLoginProperties.getLogoutUrl())){ StpUtil.logout(secureLoginProperties.getLoginId()); response.sendRedirect(secureLoginProperties.getLoginUrl()); return; } // 全局拦截，校验是否登录 SaRouter.match(this.secureProperties.getIncludeList()) .notMatch(this.secureProperties.getExcludeList()) .check(r -\u003e { StpUtil.checkLogin(); }); filterChain.doFilter(servletRequest, servletResponse); } catch (NotLoginException e) { // 未登录逻辑 log.info(e.getMessage()); HttpServletRequest httpServletRequest = (HttpServletRequest) servletRequest; HttpServletResponse httpServletResponse = (HttpServletResponse) servletResponse; SaSession saSession = SaSessionCustomUtil.getSessionById(secureProperties.getOriginUrlSession()); saSession.set(secureProperties.getOriginUrlSession(), httpServletRequest.getRequestURI()); httpServletResponse.sendRedirect(secureLoginProperties.getLoginUrl()); } catch (Throwable e) { // 1. 获取异常处理策略结果 String result = (e instanceof BackResultException) ? e.getMessage() : String.valueOf(error.run(e)); // 2. 写入输出流 if (servletResponse.getContentType() == null) { servletResponse.setContentType(MediaType.APPLICATION_JSON_VALUE); } servletResponse.getWriter().print(result); filterChain.doFilter(servletRequest, servletResponse); } } } SaTokenSecureConfig类\npackage com.github.felixlyd.config; import com.github.felixlyd.config.properties.SecureLoginProperties; import com.github.felixlyd.config.properties.SecureProperties; import com.github.felixlyd.filter.LoginFilter; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * class SaTokenSecureConfig: do something * * @author : liuyaodong * @date 2023/5/17 */ @Configuration public class SaTokenSecureConfig { /** * 方案1：sa-token * 引入spring security后，该代码需要注释 * @param secureProperties saToken配置 * @param secureLoginProperties saToken登录配置 * @return 登录过滤器 */ @Bean public LoginFilter loginFilter(SecureProperties secureProperties, SecureLoginProperties secureLoginProperties){ return new LoginFilter(secureProperties, secureLoginProperties); } } 配置文件 # spring配置 spring: boot: admin: ui: external-views: - label: \"注销\" url: /logout order: 2000 spring-security spring-security参考spring-boot-admin官方文档实现Spring Boot Admin Reference Guide\npom文件 \u003c!-- 方案2：引入spring security--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-security\u003c/artifactId\u003e \u003c/dependency\u003e config配置 SecuritySecureConfig类\npackage com.github.felixlyd.config; import cn.hutool.core.lang.UUID; import com.github.felixlyd.config.properties.SecureLoginProperties; import com.github.felixlyd.config.properties.SecureProperties; import de.codecentric.boot.admin.server.config.AdminServerProperties; import org.springframework.context.annotation.Configuration; import org.springframework.http.HttpMethod; import org.springframework.security.config.Customizer; import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import org.springframework.security.web.authentication.SavedRequestAwareAuthenticationSuccessHandler; import org.springframework.security.web.csrf.CookieCsrfTokenRepository; import org.springframework.security.web.util.matcher.AntPathRequestMatcher; /** * class SecuritySecureConfig: do something * * @author : liuyaodong * @date 2023/5/17 */ @Configuration(proxyBeanMethods = false) @EnableWebSecurity public class SecuritySecureConfig extends WebSecurityConfigurerAdapter { private final AdminServerProperties adminServer; private final SecureProperties secureProperties; private final SecureLoginProperties secureLoginProperties; public SecuritySecureConfig(AdminServerProperties adminServer, SecureProperties secureProperties, SecureLoginProperties secureLoginProperties) { this.adminServer = adminServer; this.secureProperties = secureProperties; this.secureLoginProperties = secureLoginProperties; } @Override protected void configure(HttpSecurity http) throws Exception { SavedRequestAwareAuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); successHandler.setTargetUrlParameter(\"redirectTo\"); successHandler.setDefaultTargetUrl(secureLoginProperties.getIndexUrl()); http.authorizeRequests( (authorizeRequests) -\u003e authorizeRequests.antMatchers(secureProperties.getExcludeList().get(0)).permitAll() .antMatchers(secureProperties.getExcludeList().get(1)).permitAll() .antMatchers(secureProperties.getExcludeList().get(2)).permitAll() .antMatchers(secureProperties.getExcludeList().get(3)).permitAll() .antMatchers(secureProperties.getExcludeList().get(4)).permitAll().anyRequest().authenticated() ).formLogin( (formLogin) -\u003e formLogin.loginPage(secureLoginProperties.getLoginUrl()).successHandler(successHandler).and() ).logout((logout) -\u003e logout.logoutUrl(secureLoginProperties.getLogoutUrl())).httpBasic(Customizer.withDefaults()) .csrf((csrf) -\u003e csrf.csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()) .ignoringRequestMatchers( new AntPathRequestMatcher(this.adminServer.path(\"/instances\"), HttpMethod.POST.toString()), new AntPathRequestMatcher(this.adminServer.path(\"/instances/*\"), HttpMethod.DELETE.toString()), new AntPathRequestMatcher(this.adminServer.path(\"/actuator/**\")) )) .rememberMe((rememberMe) -\u003e rememberMe.key(UUID.randomUUID().toString()).tokenValiditySeconds(1209600)); } // Required to provide UserDetailsService for \"remember functionality\" @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception { auth.inMemoryAuthentication().withUser(secureLoginProperties.getUsername()) .password(\"{noop}\" + secureLoginProperties.getPassword()).roles(\"USER\"); } } ","监控实现spring-boot-admin#监控实现：spring-boot-admin":"GitHub - codecentric/spring-boot-admin: Admin UI for administration of spring boot applications"},"title":"qiqi-gateway"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/":{"data":{"":"7.交叉决策表_w3cschool\n打造高效的规则引擎_规则引擎技术架构_兢兢业业的子牙的博客-CSDN博客\n规则引擎与工作流的区别在于？-CSDN社区\n规则引擎与工作流引擎区别是什么，使用规则引擎该注意什么 - 问答 - twt企业IT交流平台\nJava最强规则引擎-ice是如何炼成的？_规则引擎使用_waitmoon_InfoQ写作社区\n规则引擎[Drools] + 流程引擎\nityouknow / drools-examples\n规则引擎"},"title":"规则引擎"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF/":{"data":{"api-设计#API 设计":"前言本文代码于GitHub - felixlyd/rule-liteflow: 基于liteflow实现的规则demo\n思路 打造规则引擎时，规则引擎的接口报文需要很灵活，需要很高的拓展性，请告诉我该如何设计\n在设计规则引擎接口报文时，一种可能的方法是使用一种结构化的数据格式，如 JSON 或 XML。这样，你可以在不改变接口定义的情况下添加或删除报文中的字段，从而增加拓展性。此外，还可以考虑使用一种灵活的数据类型，例如字典或映射，以允许开发人员在不改变接口定义的情况下添加额外的字段。 结构化的数据例如JSON在规则引擎收到后，需要转换为java对象，如何设计这个java对象以实现较高的拓展性\n在将 JSON 数据转换为 Java 对象时，一种可能的方法是使用一个抽象类来定义 Java 对象的基本结构，并使用继承关系来为不同类型的 JSON 数据定义具体的 Java 对象。这样，你可以在不改变抽象类的情况下增加新类型的 JSON 数据，从而增加拓展性。此外，还可以考虑为抽象类添加一个字段来存储额外的字段，从而允许开发人员在不改变现有 Java 对象的情况下添加额外的字段。 假设有额外的字段内容，json格式的请求数据也转换为了如你所说的java对象，那么后续servcie层依旧需用到这个额外字段，service层又应该如何设计呢？\n如果你想在 service 层使用来自请求数据的额外字段，一种可能的方法是定义一个泛型类来作为 service 的基类，并使用继承关系来为不同类型的请求数据定义具体的 service 类。这样，你可以在不改变基类的情况下增加新类型的请求数据，从而增加拓展性。此外，还可以考虑在基类中添加一个字段来存储额外的字段，并在具体的 service 类中提供访问这个字段的方法。这样，service 类就可以通过这个方法来访问额外的字段。 规则引擎中有一个重要的内容是上下文设计，上下文设计中一般包括请求数据，业务数据和最终的响应数据，我应该如何设计上下文对象，让它有较高的可用性和拓展性\n在设计上下文对象时，一种可能的方法是定义一个抽象类来表示上下文对象的基本结构，并使用继承关系来为不同的业务场景定义具体的上下文对象。这样，你可以在不改变抽象类的情况下增加新的业务场景，从而增加拓展性。此外，你还可以为抽象类定义一些通用的方法，例如访问请求数据、业务数据和响应数据的方法，从而提高上下文对象的可用性。 假设上下文对象是一个抽象类，A业务场景和B业务场景的请求数据、业务数据和响应数据有相同的字段有也不同的字段，我应该如何定义它们的上下文对象？请举例说明\n如果 A 业务场景和 B 业务场景的请求数据、业务数据和响应数据有相同的字段，你可以在抽象类中定义一些通用的字段，并在 A 业务场景和 B 业务场景的上下文对象中继承这些通用字段。如果 A 业务场景和 B 业务场景的请求数据、业务数据和响应数据有不同的字段，你可以在 A 业务场景和 B 业务场景的上下文对象中分别定义这些不同的字段。 例如，如果 A 业务场景和 B 业务场景都需要访问用户 ID 和交易金额，你可以在抽象类中定义两个通用字段：\npublic abstract class AbstractContext { private String userId; private Double amount; // getters and setters } 然后，你可以在 A 业务场景的上下文对象中继承这两个通用字段，并定义一些额外的字段：\npublic class AContext extends AbstractContext { private String product; private String location; // getters and setters } 同理，你可以在 B 业务场景的上下文对象中继承这两个通用字段，并定义一些额外的字段：\npublic class BContext extends AbstractContext { private String category; private String date; // getters and setters } 这样，你就可以为 A 业务场景和 B 业务场景定 那么相当于我每次新增一个业务场景，都需要新增或修改上下文对象。我希望的是设计一个通用的上下文对象，能够覆盖大多数业务场景，应该如何设计？\n如果你希望设计一个通用的上下文对象来覆盖大多数业务场景，你可以考虑定义一个抽象类，并使用结构化的数据类型（如字典或映射）来存储所有的请求数据、业务数据和响应数据。这样，开发人员就可以在不改变上下文对象的情况下，随时为不同的业务场景添加额外的字段。 例如，你可以定义一个抽象类，并在该类中定义三个映射对象：\npublic abstract class AbstractContext { private Map\u003cString, Object\u003e request; private Map\u003cString, Object\u003e data; private Map\u003cString, Object\u003e response; // getters and setters } 然后，你可以在具体的业务场景中继承这个抽象类，并使用这三个映射对象来存储请求数据、业务数据和响应数据。例如：\npublic class AContext extends AbstractContext { public AContext() { request.put(\"user_id\", \"123456\"); request.put(\"amount\", 100.0); data.put(\"product\", \"book\"); data.put(\"location\", \"New York\"); response.put(\"status\", \"success\"); response.put(\"message\", \"Transaction completed\"); } } 这样，开发人员就可以在不改变上下文对象的情况下，为 A 业务场景添加任意数量的请求数据、业务数据和响应数据。 此外，你还可以为抽象类定义一些通用的方法，例如在 例如在\n例如，你可以为抽象类定义一些通用的方法，例如在三个映射对象中添加、删除或更新字段的方法，或者获取指定字段的值的方法。例如：\npublic abstract class AbstractContext { private Map\u003cString, Object\u003e request; private Map\u003cString, Object\u003e data; private Map\u003cString, Object\u003e response; public void addRequestField(String key, Object value) { request.put(key, value); } public void removeRequestField(String key) { request.remove(key); } public void updateRequestField(String key, Object value) { request.put(key, value); } public Object getRequestField(String key) { return request.get(key); } // other methods for data and response } 这样，开发人 这样，开发人\n这样，开发人员就可以在 A 业务场景的上下文对象中调用这些方法来访问请求数据、业务数据和响应数据。例如：\nAContext context = new AContext(); context.addRequestField(\"product_id\", \"abc123\"); context.updateRequestField(\"amount\", 120.0); Object productId = context.getRequestField(\"product_id\"); 这样，你就可以为大多数业务场景提供一个通用的上下文对象，并提高它的可用性和拓展性。 好，上下文对象的设计我明白了。现在我想问另一个问题，在规则引擎中，可能需要外部数据，例如去查某个客户有多少钱需要访问外部接口。我一个业务场景中，可能有三个规则都需要知道客户有多少钱，但我不应该访问三次外部接口，这样效率太低了。我应该怎么设计，才能够仅需访问一次外部接口呢？\n在规则引擎中，如果多个规则需要访问同一个外部接口，你可以考虑将外部数据放入上下文对象中，并在规则执行过程中访问上下文对象来获取数据。这样，外部接口只需要被访问一次，以获取需要的数据，然后将数据放入上下文对象中，供多个规则使用。 例如，你可以将上下文对象定义为如下所示：\npublic class CustomerContext extends AbstractContext { private String customerId; private Double balance; public CustomerContext(String customerId) { this.customerId = customerId; // Call external API to get customer balance this.balance = getCustomerBalance(customerId); } public String getCustomerId() { return customerId; } public Double getBalance() { return balance; } private Double getCustomerBalance(String 如下所示\n如下所示：\npublic class CustomerContext extends AbstractContext { private String customerId; private Double balance; public CustomerContext(String customerId) { this.customerId = customerId; // Call external API to get customer balance this.balance = getCustomerBalance(customerId); } public String getCustomerId() { return customerId; } public Double getBalance() { return balance; } private Double getCustomerBalance(String customerId) { // Call external API to get customer balance return 100.0; } } 然后，你可以在规则执行过程中访问上下文对象来获取客户的信息：\nCustomerContext context = new CustomerContext(\"123456\"); String customerId = context.getCustomerId(); Double balance = context.getBalance(); 这样，外部接口就只需要在创建上下文对象时 这样？\n是的，你可以通过这种方式来仅访问一次外部接口，以获取所需的数据。在创建上下文对象时，你可以通过调用外部接口来获取需要的数据，并将数据存储到上下文对象中。然后，在规则执行过程中，你可以通过访问上下文对象来获取这些数据，而无需再次调用外部接口。 这样，你就可以在规则引擎中仅访问一次外部接口，提高程序的效率。 按你所说的去调用外部接口，需要提前知道规则脚本中需要哪些外部接口的数据。而实际上，规则脚本并没有办法提前写好，而是在规则引擎系统上线后，业务人员增加规则脚本的时候，系统才知道引用了外部接口的数据。我可以提前准备好调用外部接口的代码，但如何设计才能够让系统感知到规则脚本中的那些需要调用外部接口的数据，并且仅访问一次外部接口\n在这种情况下，你可以在上下文对象中添加一个方法，用于动态获取外部接口的数据。例如：\npublic class CustomerContext extends AbstractContext { private String customerId; private Map\u003cString, Object\u003e data; public CustomerContext(String customerId) { this.customerId = customerId; this.data = new HashMap\u003c\u003e(); } public String getCustomerId() { return customerId; } public Object getExternalData(String key) { if (!data.containsKey(key)) { // Call external API to get data and put it in the map data.put(key, getExternalDataFromApi(key)); } return data.get(key); } private Object getExternalDataFromApi(String key) { // Call external API and return the data return 100.0; } } 这样，在规则执行过程中，你可以调用 getExternalData 方法来获取外部接口的数据，如果数据在内存中已经存在，则直接返回；否则，调用外部接口获取数据，并将数据存储在内存中，供下次使用。 这样，你就可以在规则脚本中动态获取需要的外部数据，同时保证数据仅被访问一次，提高程序的效率。 这样写代码就会变成，我每次实现一个外部接口的调用，都需要将外部接口的响应数据的字段作为key写到这个getExternalData方法中。如此一来，外部接口发生变化后，我也需要修改我的代码。相当于我的代码随外部系统的变化而变化，这是很差的程序设计。希望你能提出更好的建议\n是的，如果每次实现一个外部接口时都需要在代码中手动添加对应的字段作为key，这样的确不是很优秀的设计方式。 一种更好的方式是使用反射机制，让程序在运行时自动获取外部接口返回的数据字段。例如，你可以将上下文对象定义为如下所示：\npublic class CustomerContext extends AbstractContext { private String customerId; private Map\u003cString, Object\u003e data; public CustomerContext(String customerId) { this.customerId = customerId; this.data = new HashMap\u003c\u003e(); } public String getCustomerId() { return customerId; } public Object getExternalData(String key) { if (!data.containsKey(key)) { // Call external API to get data and put it in the map data.put(key, getExternalDataFromApi(key)); } return data.get(key); } private Object getExternalDataFromApi(String key) { // Call external API and return the data ExternalDataResponse response = callExternalApi(key); // Use reflection to get the field value from the response object try { Field field = response.getClass().getDeclaredField(key); field.setAccessible(true); return field.get(response); } catch (NoSuchFieldException | IllegalAccessException e) { // Handle exception return null; } } } 这样，在规则执行过程中， 这样，在规则执行过程中？\n对，在规则执行过程中，你可以通过调用 getExternalData 方法来获取外部接口返回的数据。例如：\nCustomerContext context = new CustomerContext(\"123456\"); Object balance = context.getExternalData(\"balance\"); 在这种情况下，程序会自动调用外部接口并获取响应的数据，然后使用反射机制从响应数据对象中获取 balance 字段的值，并将其返回给调用方。 这样，你就不需要在代码中手动添加外部接口返回的字段作为key，程序会自动根据实际情况来获取数据，从而避免了因外部 从而避免了？\n从而避免了因外部接口变化而导致的代码变更。 例如，如果外部接口的返回数据发生了变化，例如增加了一个新的字段，那么程序会自动识别新增的字段，并且可以通过调用 getExternalData 方法来获取新增字段的值。 这样，你的代码就不需要因为外部接口的变化而发生改变，更加灵活和稳定。 实际场景中，规则脚本中可能既需要内部数据又需要外部数据。内部数据通常指请求数据或者是根据请求数据查询数据库获得的数据，外部数据则是上面我们讨论的调用外部接口获取的数据。因此，设计上下文时，不能仅用getExternalData方法来获取数据。或许需要将请求数据、数据库获取的数据、外部数据分开？请告诉我应该如何设计\n在设计规则脚本中获取数据时，需要考虑到数据的来源和使用方式。如果需要获取请求数据或数据库中的数据，可以将这些数据作为规则脚本的输入参数，在规则脚本中直接使用。如果需要获取外部数据，可以在规则脚本中调用特定的方法（例如getExternalData）来获取外部数据，并将获取到的数据作为方法的返回值使用。这样做可以保证数据的来源和使用方式的一致性，并且可以更好地管理数据。 业务人员在写规则脚本时，并不知道这些数据到底是内部数据还是外部数据。\n那么，为了方便业务人员写规则脚本，可以在规则脚本的文档中详细说明数据的来源和使用方式，包括哪些数据是内部数据，哪些是外部数据，如何获取这些数据等内容。这样，业务人员在写规则脚本时就可以根据文档中的信息来使用这些数据。此外，为了进一步简化业务人员的操作，可以为规则脚本提供一些辅助工具，例如数据管理工具或调试工具，来帮助业务人员管理和使用数据。 我想通过良好的设计来使得业务人员无需关心这些数据是内部数据还是外部数据\n如果想让业务人员在写规则脚本时无需关心数据的来源和使用方式，那么可以通过提供一个统一的、易于使用的数据访问接口来实现。这个接口可以封装内部数据和外部数据的获取方式，并统一提供给业务人员使用。业务人员可以通过这个接口来获取所需的数据，而无需关心数据的来源和使用方式。这样，业务人员就可以专注于业务逻辑的实现，而无需关心数据方面的细节。 封装这样的接口，我认为需要增加一张数据库的表，来存储数据字段名称和数据来源类型\n是的，为了封装一个统一的数据访问接口，可以考虑在数据库中增加一张表来存储数据字段名称和数据来源类型。这张表可以用来管理所有可用的数据字段，并且提供一种方便的方式来查询数据字段的来源类型。这样，在封装统一的数据访问接口时，可以根据数据字段的来源类型来获取对应的数据，而无需关心具体的实现细节。\n实现术语定义 名词概念 概念描述 liteflow组件-component liteflow框架组件，可以由java实现，继承liteflow组件类或者添加了liteflow组件注解，可交由spring管理；也可以由groovy、JavaScript等脚本语言实现 liteflow规则链-chain liteflow规则流程，由一或多个特定逻辑顺序编排的liteflow组件实现 系统概述 系统体系结构 规则引擎架构设计示意图\n功能模块 按照微服务思想，划分不同功能模块，并单独部署\n规则引擎功能模块设计示意图\n程序设计详细描述 规则判定设计说明 在controller层级提供一个入口，根据交易码来调用service层。每个service层调用一个或者多个liteflow规则链（chain）。每个chain由多个按特定逻辑顺序编排的liteflow组件组成。liteflow组件可以由java类实现，也可以基于脚本语言实现（脚本语言可选QLExpress、javaScript、Groovy，这里选择Groovy），脚本以字符串的形式存储在数据库表中。在chain中，有一个上下文java对象贯穿每一个liteflow组件。\n规则判定具体实现示意图\n请求报文在controller层组装为请求业务对象，传入service层。在service层中，请求业务对象数据组装为上下文对象，传入chain。按照chain的特定逻辑顺序执行liteflow组件中的代码时，从上下文对象获取对应的输入数据，并将输出数据放入上下文对象中。chain执行完毕后，service层从上下文对象中获取响应数据并组装响应业务对象，再由controller层组装为响应报文。\n规则判定数据流转示意图\n请求对象设计 请求对象设计为JSON的结构化数据。 反序列化时，会用泛型类接收请求报文的body，而header和其他元信息（时间、流水）会用具体的字段属性接收。 接收请求报文后，会对请求报文的字段属性进行校验。设计必需属性校验，例如流水号、渠道码等属性的校验。必需属性校验的相关配置可以存入到数据库中，在程序启动时以缓存的形式加载到程序中。\n响应对象设计 响应对象设计为两类，一类只有相对简略的最终的结果，一类包含详细的每个规则的输出结果，具体返回哪一种数据，由请求报文中的结果属性字段判定。 响应对象为按一定模板设计的JSON结构化数据。模板里肯定包括流水号、渠道码等必须属性。\n上下文对象设计 上下文对象由请求数据、上下文数据、响应数据三部分数据组成。这些数据均设计为JSON的结构化数据，根据key值来调用getter和setter。\n外部数据获取设计说明 liteflow组件在使用外部数据时，如果是在同一个chain之中，则认为是使用同一外部数据，即同一个chain只会外呼外部接口一次来获取同一个外部数据。判定依据为数据源的名称，即便是调用同一外部接口，不同的数据源名称则会发起一次新的调用。\n**实现方式1：**在执行chain中的组件之前，扫描组件所需的输入数据，如果有外部数据，则先进行外呼，将外呼响应数据存储到内存中。（推荐使用） **实现方式2：**执行chain，遇到外部数据时，如果内存中有，直接使用；如果没有，先进行外呼。\n对于规则而言，规则本身并不知道所需数据是外部数据、内部数据、数据库数据或者是配置文件数据。因此，在配置规则前，需要先配置数据信息，这些配置信息存入数据库中，在程序启动时以缓存的形式加载到程序中。\n数据库数据获取设计说明 数据库数据、配置文件数据获取的设计均参照外部数据获取的设计。\n运用消息中心 规则引擎的响应数据除了返回给渠道，也会推送给消息队列。消息队列中的数据会持久化于数据库中，用于监控、交易追踪、数据核查等。消息生产者-规则引擎与消息消费者解耦，可以减少规则引擎的响应时间。\n规则链设计 在liteflow中，规则链中可以嵌套规则链，每个规则链由按特定顺序编排的若干liteflow组件组成。liteflow组件可以用java类或规则脚本实现。规则链则按liteflow语法配置化实现。规则脚本和规则链持久化在数据库中，在程序启动时以缓存的形式加载到程序中。 在设计数据表和后管接口时，要考虑规则链修改、逻辑删除的权限，例如，某个规则链正在使用，是否能修改或者逻辑删除。\n数据库设计 前缀规范：\ntmp_ 临时表 v_ 版本表 his_ 历史表 r_ 关系表 b_xxx_ xxx业务表 d_ 字典表（地区表、机构表等基本不被业务逻辑修改的表） 规则脚本表 liteflow_script 规则脚本表存储了规则脚本的ID、内容、类型，以及时间、人员等元数据 type分为6种：\nscript：普通脚本节点，脚本里无需返回。 switch_script：选择脚本节点，脚本里需要返回选择的节点Id。 if_script：条件脚本节点，脚本里需要返回true/false。 for_script：数量循环节点，脚本里需要返回数值，表示循环次数。 while_script：条件循环节点，脚本里需要返回true/false，表示什么条件才继续循环。 break_script：退出循环节点，脚本里需要返回true/false，表示什么时候退出循环。 status分为3种：\nenabled：启用 disabled：禁用 deleted：逻辑删除 状态的改变不会增加新版本\n规则脚本版本表 v_liteflow_script 每次新增和修改规则脚本的描述、内容、类型等字段，则会新增版本记录\n规则链表 liteflow_chain 规则链表存储了规则链的ID、内容、层级，以及时间、人员等元数据 chain_level考虑为两级：\nlv1：第1级 lv2：第2级 status分为3种：\nenabled：启用 disabled：禁用 deleted：逻辑删除 规则链版本表 v_liteflow_chain 每次新增和修改规则链的描述、内容、层级等字段，则会新增版本记录\n规则链-规则链关系表 r_chain_chain 记录了规则链和规则链的关系，保证了当规则链被使用时，无法逻辑删除和禁用。\n脚本-规则链关系表 r_script_chain 记录了脚本和规则链的关系，保证了当脚本被使用时，无法逻辑删除和禁用。\n规则脚本数据字段表 script_field 记录了规则脚本中使用到的数据字段信息，以及数据字段的来源信息以及在来源中对应的字段名。\n数据源配置属性表 data_source_properties 记录了数据源的配置信息，外部数据、数据库数据等的特殊配置信息例如外部数据中涉及的响应数据等，以及数据库数据中涉及的表名、表字段名等，用json字符串存储。 source_type有以下几类：\n00：请求数据，来源于请求报文的数据 01：外部数据，来源于请求外部接口获得的响应报文中的数据 02：数据库数据，来源于本数据库中数据表的字段数据 03：配置文件数据，来源于配置文件中的字段数据 数据源JSON设置表 d_source_json_settings 数据源存储的json字符串必须按照一定格式配置，因此这里增加一张字典表来记录数据源json配置信息 数据源json设置限制条件不能过于苛刻，source_json_settings示例： 数据库数据配置：\n{ \"table_name\":\"string(100)\", \"field_names\":\"list(100)\", \"search_condition_fields\":\"list(20)\" } 外部数据配置：\n{ \"service_code\":\"string(100)\", \"response_fields\":\"list(100)\", \"request_fields\":\"list(100)\" } 配置文件数据配置：\n{ \"yaml_name\":\"string(1000)\", \"field_names\":\"list(100)\" } 这里有一个潜在问题，嵌套字段应该如何设计json设置？\nAPI 设计 ","前言#前言":"","实现#实现":"","思路#思路":"","数据库设计#数据库设计":"","术语定义#术语定义":"","程序设计详细描述#程序设计详细描述":"","系统概述#系统概述":"","讨论点#讨论点":"数据表设计中是否需要考虑审核业务？ 需要，后续完善\n是否引入内存数据库redis？ 为了提升响应速度，许多配置信息会在程序启动时以缓存的形式加载到程序中。当配置信息过多时，例如说超过了程序运行的机器内存的一半，可能需要引入redis。 需要"},"title":"规则引擎设计思路"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/%E9%A1%B9%E7%9B%AE%E5%92%8C%E9%9C%80%E6%B1%82%E5%BF%AB%E9%80%9F%E5%BC%80%E5%8F%91%E6%80%9D%E8%B7%AF/":{"data":{"":"工作一坤年后，开始思考项目和需求快速开发应该怎么做。","吐槽#吐槽":"项目可以理解为从头拿IDEA构建一个工程，不过一般会使用用以前的一个架子打底来做。当然，某些初创公司会重搭。需求可以理解为已上线运行的项目基础之上的增量迭代开发。增量开发也就是功能优化、漏洞修复，改动的是代码、配置、数据库，恼火的是表实体层dao层那一堆，崩溃的是文档维护修改\n这一段话总结下，就是一个架子，很重要，一个代码生成器，很重要，一个文档自动更新，非常重要！\n先说架子。调研了下，架子，网上开源的很多，收费的只是文档和视频教程，这也没办法，都要吃饭。不过有一说一，直接拿来用很痛苦，太重了，拆分很是受罪，如果不缴费学习，自己去探索剥离，纯纯坐牢。我决定不缴这个钱，从所谓开源框架中找文档还算多的，以最小的、能随便拆分的方式，自己写或者标明作者，来组建架子的各个模块。当然，我暂时没有写详细注释和文档的打算，也没有录视频教程抢开源大佬生意的打算，只是磨炼一下自己的技术。\n开源架子往往都有些共通点，需求不同，issue实现多了，轻量呢慢慢变重，拆分不便。不用呢，重复造轮子，用呢，abcd几个开源组件一组合，然后修bug，妥妥活受罪。之前组合spring gateway和sentinel，直接因为文档不清晰加版本问题，验出来几个坑（基于spring cloud gateway+sentinel构建微服务网关）\n所以，还是能少用则少用。拿很知名的hutools来说，拆成了多个模块，这种想法就很棒，用谁引谁，不然包冲突能让人头皮发麻，虽然有maven helper了。\n再说代码生成器，脱离设计和框架的代码生成器就不说了，没啥太大意义，mybatis-plus直接写个脚本，妥妥完事。如果想自定义模板，网上开源的也多，一大堆，推荐一手PDmaner，纯纯免费，公众号也不需要关注，改改模板，非常够用。或者设计到实现，搞个apifox，从数据模型到接口文档，再转变成代码，完全OK。不过，apifox还是有不少BUG，例如数据模型的长度在转sql的时候直接一个宣告死亡，只能255，也不知道哪个版本修。唉，但是，又不得不说但是了，真的符合工作岗位上要求使用的开发框架吗？\n当然，现在的系统实现已经互相借鉴互相学习，大家都找到最优解了，但真的是这样吗？举个例子，早年写Javadoc，后来写swagger注解，现在又在追求无侵入了，看了下smart-doc，确实很厉害，但引了一堆依赖，又得去看看依赖里有哪些jar包…\n当然，其实生产上挂了就挂了，无所谓了，漏洞就漏洞了，也没办法，老板就给那点工资，凭啥呢。确实是。\n再说说文档及时更新，showdoc+runapi、smart-doc+torna集成、或者就拿apifox吧，真的apifox再把bug修修简直无敌。可惜就是得联网登录，私网部署又得找领导谈，我也谈不动啊。我就一秋儿，却要关心这么多事，我也很费解。这些东西头疼的只能说头疼，单机也就算了，微服务呢？杀死游戏。其实还有knife4j，他们是真的很厉害，我也很敬佩，swagger丑爆的ui直接干烂，springdoc迁移了还在用swagger那丑不拉几的ui，简直牛皮。这一方面不得不说我们国人很牛。knife4j能干微服务，答案已经出来了。","思路#思路":"吐完槽，讲讲思路。 首先是架子，这个生产上跑起来了，就先不慌了。先整个思维导图，后面慢慢补充迭代起走。思路很简单，按spring-boot的思想，做成模块的装配，要引就引，不引拉到。而且模块粒度拆的足够细，要用谁用谁。然后再像hutools做个hutools-all，给懒人专用。\n其次是代码生成器，这个比较紧急，而且方案要好好想下。模板这东西，肯定要能够导入导出，而且得稍微改改，就能套到另一个架子中。表肯定是能够和数据库连的，mysql和oracle能支持就行了，毕竟oracle都收购mysql了，国产数据库语法大概也不会脱离这两个了。实在不行，找个GPT翻译数据库导出的sql吧，再自己找个mysql连上。这两者其实PDmaner已经实现了，所以我一直在夸。但PDmaner呢，模板不好管理，实在不行也就它了。\n再说文档，这样一来，哪怕单用PDmaner，数据字典咱们已经有了。就差接口文档了，其实swagger那玩意勉强是够的，换个knife4j前身swagger-bootstrap-ui把烂UI一换，其实够用。可是我们很馋knife4j那个离线导出html，那玩意是真的好用，谁用谁知道。而且它还能导出word，绝不绝，真的是打了国人的七寸痛点。apifox的接口文档html其实很好看，但它没法导出来，想想也知道，这玩意能导出来，谁还买它的私网部署？所以，我要用上一个springdoc加上它官方推荐的一个几百个star的小项目（真怕这个作者直接关项目）GitHub - dnault/therapi-runtime-javadoc: Read Javadoc comments at run time.来做这件事。","方案#方案":""},"title":"项目和需求快速开发思路"},"/lyd-notes/1.%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%B9%E6%A1%88/redis%E6%90%AD%E5%BB%BA%E5%8F%8C%E4%B8%AD%E5%BF%83%E9%9B%86%E7%BE%A4/":{"data":{"1kafkaredis-sentinel#1.kafka+redis-sentinel":"首先，在两个地点分别搭建一个 Kafka 集群，每个集群至少包含一个生产者、一个消费者和一个 broker。\n在每个地点都搭建一个 Redis Sentinel 集群，每个集群至少包含一个主服务器、一个从服务器和一个 Sentinel。\n然后，在 Kafka 消费者中实现数据复制的逻辑，在两个地点的 Kafka 消费者中都运行这段逻辑。\n接着，在 Redis Sentinel 中配置数据复制的逻辑，即将两个地点的 Redis Sentinel 集群配置为双中心集群。\n最后，在 Redis 主服务器中添加一个客户端，客户端能够通过 Redis Sentinel 访问 Redis 主服务器，并将数据写入 Kafka 生产者。\n当客户端写入数据时，Kafka 生产者会将数据发送到两个地点的 Kafka 消费者，由消费者进行数据复制。","2kafkatwemproxy#2.kafka+Twemproxy":" 安装 kafka，并在两个不同地点启动两个 kafka 集群。 安装 redis，并在每个地点启动一个 redis 集群。 安装 Twemproxy，并启动 Twemproxy 服务。 将 kafka 集群与 redis 集群连接起来，通过消息队列实现数据的异步复制。 在 Twemproxy 中配置 redis 地址，并通过负载均衡策略对客户端的请求进行路由。 ","haproxy#HaProxy":"","keepalived#Keepalived":"","nginx#Nginx":"","redis-cluster#Redis Cluster":"","redis-sentinel#Redis-sentinel":"","twemproxy#Twemproxy":"","代理服务器#代理服务器":"","伪实现#伪实现":"Redis迁移工具之Redis-shake - X-Wolf - 博客园\n揭开RedisShake的秘密-阿里云开发者社区\n搞懂异地多活，看这篇就够了 - 掘金\nGitHub - ctripcorp/x-pipe: X-Pipe是由携程框架部门研发的Redis多数据中心复制管理系统。基于Redis的Master-Slave复制协议，实现低延时、高可用的Redis多数据中心、跨公网数据复制，并且提供一键机房切换，复制监控、异常报警等功能。开源版本和携程内部生产环境版本一致。\n再谈Redis双活实现方案\n代理服务器使用 HAProxy、nginx、keepalived、redis-sentinel、Twemproxy 等代理服务器实现 Redis 双中心集群有以下优点：\n可以灵活地配置负载均衡策略，以满足不同的业务需求。 通过分布式的方式实现高可用，可以保证 Redis 的数据完整性和可用性。 代理服务器的容错能力很强，可以应对 Redis 集群中的故障和不同级别的网络故障。 可以使用监控工具监控 Redis 集群的运行情况，及时发现问题并进行处理。 但是，使用这些代理服务器实现 Redis 双中心集群也有一些缺点：\n需要维护额外的代理服务器，增加了系统的复杂度。 如果代理服务器出现故障，可能会导致整个 Redis 集群不可用。 需要对代理服务器进行调优，以确保性能和可用性。 由于 Redis 本身不支持分布式事务，代理服务器可能需要实现一些特殊的逻辑来解决这个问题。 HaProxy HAProxy 是一个高性能的 TCP/HTTP 负载均衡器，它提供了多种负载均衡策略和容错机制。它的优点在于支持多协议、性能优良、稳定可靠，能够提供高可用性保障。但它并不能直接支持 Redis 协议，需要配合其他工具，例如 redis-sentinel 或 keepalived 来实现 Redis 集群的双中心高可用。\n另外，HAProxy 需要一定的配置才能够对 Redis 进行负载均衡，需要用户对其有一定的了解。同时，它在故障转移方面的支持也比较弱，如果要实现更复杂的容错机制，需要配合其他工具来实现。\nNginx nginx 是一个高性能的 HTTP 和反向代理服务器，它支持多种负载均衡策略和提供了一定的容错机制。它的优点在于支持多协议、性能优良、稳定可靠，能够提供一定程度的高可用性保障。\n但是 nginx 并不能直接支持 Redis 协议，需要配合其他工具，例如 redis-sentinel 或 keepalived 来实现 Redis 集群的双中心高可用。同时，nginx 的配置略显复杂，需要对其有一定的了解才能够使用。在故障转移方面，nginx 支持的也比较弱，如果要实现更复杂的容错机制，需要配合其他工具来实现。\nKeepalived keepalived 是一个高可用的 Linux 工具，它支持 VRRP 协议，能够实现两个或多个网关设备之间的虚拟 IP 地址的高可用性。通过在 Redis 双中心集群中部署 keepalived 可以实现两个 Redis 中心之间的虚拟 IP 地址，并实现故障转移。\n使用 keepalived 的优点在于配置简单，可以实现自动故障转移，且能够提供一定程度的高可用性保障。缺点在于不能直接支持 Redis 协议，需要和其他工具配合使用，且支持的故障转移策略较少。\nRedis-sentinel Redis Sentinel 是 Redis 提供的一个高可用方案，它能够监控 Redis 主从复制状态，并在主服务器故障时实现自动故障转移。使用 Redis Sentinel 可以实现 Redis 双中心集群的高可用，并能够提供多种故障转移策略的支持。\n使用 Redis Sentinel 的优点在于支持多种故障转移策略，且支持 Redis 协议，可以直接配合 Redis 使用。缺点在于配置较为复杂，需要在每个 Redis 节点上都配置 Sentinel，且在转移期间可能存在数据不一致的情况。\nTwemproxy Twemproxy 是 Twitter 开源的一款 Redis 代理服务器，它具有轻量级、高性能和支持 Redis 协议的特点。使用 Twemproxy 可以实现 Redis 双中心集群的高可用，并且支持基于轮询、哈希、随机等多种负载均衡策略。\n使用 Twemproxy 的优点在于配置简单，只需要在代理服务器上配置 Twemproxy 即可。缺点在于不支持 Redis Sentinel 故障转移策略，且在转移期间也可能存在数据不一致的情况。\nRedis Cluster 使用 Redis Cluster 实现 Redis 双中心集群的优点是它可以自动分配数据到不同的节点上，并提供内置的故障转移机制，保证了数据的高可用性。但是，Redis Cluster 有一些缺点，包括不支持所有 Redis 的数据类型和操作，比如不支持事务操作和排序操作，还需要手动进行数据的重新平衡，因此需要进行一定的折衷。\n除了使用 Redis Cluster，我们还可以使用基于消息队列的方案来实现 Redis 双中心集群。例如，我们可以在主中心和备中心之间使用消息队列来进行数据同步，从而保证两个中心的数据一致性。这种方案的优点是可以支持所有 Redis 的数据类型和操作，并且在失效时可以通过消息队列来进行数据的恢复，不需要手动平衡数据。但是，这种方案的缺点是实现较为复杂，需要消息队列的支持，同时还可能会增加延迟。\n引入消息队列使用消息队列实现 redis 双中心集群的方式是将两个 redis 集群分别部署在不同的地域，然后使用消息队列在两个集群之间同步数据。这种方式的优点是可以实现异地多活，避免单点故障；缺点是需要引入消息队列组件，对系统的复杂度和成本都有一定的增加。\n在搭建 Redis 双中心集群时，消息队列和代理服务器可以结合使用，这样可以实现更灵活、可扩展的方案。比如，在使用 HAProxy、nginx 或 Twemproxy 等代理服务器时，可以通过消息队列实现主从数据同步、状态监控和快速切换等功能。这样一来，即使在大量请求的情况下，也可以保证 Redis 双中心集群的高可用性。\n代理服务器可以在客户端和 Redis 集群之间提供负载均衡和故障切换等功能。而消息队列可以在两个 Redis 集群之间提供异步通信，实现数据的同步更新和一致性。\n只用消息队列进行搭建Redis双中心集群 在只使用消息队列的 redis 双中心集群方案中，主从服务器之间的数据复制采用异步方式实现。也就是说，主服务器在接收到新的写请求时，会先将请求写入本地缓存，然后将请求发送到消息队列中。从服务器通过消息队列订阅主服务器发送的请求，并根据请求内容更新自己的缓存。由于数据复制采用了异步方式，主从服务器之间的复制延迟会大大降低，这样在主服务器宕机时，从服务器能够快速接管主服务器的角色，从而提高了系统的可用性。\n不过，只使用消息队列的 redis 双中心集群方案也有一些缺点。由于主从服务器之间的数据复制是异步的，容易出现数据不一致的情况。此外，在消息队列中传递的请求可能会丢失，从而导致数据的一致性和可靠性降低。这是因为消息队列并不能保证每条消息都能被正确地发送和接收，因此有可能会导致主从集群之间的数据不一致。此外，如果消息队列中的消息堆积过多，也可能会影响主从集群之间数据的同步。因此，如果要实现 Redis 双中心集群，使用消息队列实现异步复制可能不是一个非常好的选择。\n伪实现","只用消息队列进行搭建redis双中心集群#只用消息队列进行搭建Redis双中心集群":"","引入消息队列#引入消息队列":""},"title":"Redis搭建双中心集群"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/":{"data":{"":" Docker Nginx Redis Git系使用 IDE使用 Netty "},"title":"必知必会工具"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/docker/":{"data":{"docker#Docker":"Dockerdocker 命令报异常permission denied的解决方案_docker_脚本之家\ndocker部署gitlab - 我好高冷 - 博客园\n（九） Docker 容器数据卷(挂载) - 时光, - 博客园\nRTNETLINK answers: Operation not permitted - 苏轶然 - 博客园","docker-compose#Docker-compose":"docker-compose 中 volumes、environment、privileged、network_mode的使用_九亿少女的梦@的博客-CSDN博客_docker-compose privileged: true\ndocker-compose启动consul - brady-wang - 博客园\nDocker Compose - 命令说明 - 《Docker — 从入门到实践（v1.3.0）》 - 书栈网 · BookStack","dockerfile#DOCKERFILE":"docker学习笔记18：Dockerfile 指令 VOLUME 介绍 - 51kata - 博客园"},"title":"Docker"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/docker/docker%E9%87%8D%E8%A6%81tips/":{"data":{"":"","1-镜像加速器#1. 镜像加速器":"","2-慎用-docker-commit#2. 慎用 \u003ccode\u003edocker commit\u003c/code\u003e":"","3-镜像构建上下文context#3. 镜像构建上下文（Context）":"","4-其它-docker-build-的用法#4. 其它 \u003ccode\u003edocker build\u003c/code\u003e 的用法":"","5-进入容器#5. 进入容器":"","6-数据卷#6. 数据卷":"","7-挂载主机目录#7. 挂载主机目录":"","8-容器互联#8. 容器互联":"","附录1常见问题总结httpswwwbookstackcnreaddocker_practice-130appendix-faq-readmemd#附录1：\u003ca href=\"https://www.bookstack.cn/read/docker_practice-1.3.0/appendix-faq-README.md\"\u003e常见问题总结\u003c/a\u003e":"","附录2docker命令查询httpswwwbookstackcnreaddocker_practice-130appendix-command-dockermd#附录2：\u003ca href=\"https://www.bookstack.cn/read/docker_practice-1.3.0/appendix-command-docker.md\"\u003eDocker命令查询\u003c/a\u003e":"1. 镜像加速器 国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了国内加速器服务，例如：\n阿里云加速器(点击管理控制台 -\u003e 登录账号(淘宝账号) -\u003e 右侧镜像中心 -\u003e 镜像加速器 -\u003e 复制地址) 网易云加速器 [https://hub-mirror.c.163.com](https://www.163yun.com/help/documents/56918246390157312) 百度云加速器 [https://mirror.baidubce.com](https://cloud.baidu.com/doc/CCE/s/Yjxppt74z#%E4%BD%BF%E7%94%A8dockerhub%E5%8A%A0%E9%80%9F%E5%99%A8) 对于使用 Windows 10 的用户，在任务栏托盘 Docker 图标内右键菜单选择 Settings，打开配置窗口后在左侧导航菜单选择 Docker Engine，在右侧像下边一样编辑 json 文件，之后点击 Apply \u0026 Restart 保存后 Docker 就会重启并应用配置的镜像地址了。\n{ \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 2. 慎用 docker commit 使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为 黑箱镜像。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。这会让镜像更加臃肿。\n3. 镜像构建上下文（Context） 如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定 上下文路径。那么什么是上下文呢？\n当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。\n如果在 Dockerfile 中这么写：\nCOPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。\n因此，**COPY**** 这类指令中的源文件的路径都是_相对路径_。**\n那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。\n4. 其它 docker build 的用法 直接用 Git repo 进行构建 或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建：\n# $env:DOCKER_BUILDKIT=0# export DOCKER_BUILDKIT=0$ docker build -t hello-world https://github.com/docker-library/hello-world.git#master:amd64/hello-worldStep 1/3 : FROM scratch ---\u003eStep 2/3 : COPY hello / ---\u003e ac779757d46eStep 3/3 : CMD [\"/hello\"] ---\u003e Running in d2a513a760edRemoving intermediate container d2a513a760ed ---\u003e 038ad4142d2bSuccessfully built 038ad4142d2b 这行命令指定了构建所需的 Git repo，并且指定分支为 master，构建目录为 /amd64/hello-world/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。\n用给定的 tar 压缩包构建 $ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。\n从标准输入中读取 Dockerfile 进行构建 docker build - \u003c Dockerfile 或\ncat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。\n从标准输入中读取上下文压缩包进行构建 $ docker build - \u003c context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。\n5. 进入容器 某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，从docker exec 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因\n6. 数据卷 数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性：\n数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 $ docker volume create my-vol 查看所有的 数据卷\n复制代码$ docker volume lsDRIVER VOLUME NAMElocal my-vol 参考\n7. 挂载主机目录 挂载一个主机目录作为数据卷 使用 --mount 标记可以指定挂载一个本地主机的目录到容器中去。\n$ docker run -d -P \\ --name web \\ # -v /src/webapp:/usr/share/nginx/html \\ --mount type=bind,source=/src/webapp,target=/usr/share/nginx/html \\ nginx:alpine 上面的命令加载主机的 /src/webapp 目录到容器的 /usr/share/nginx/html目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，以前使用 **-v** 参数时如果本地目录不存在 Docker 会自动为你创建一个文件夹，现在使用 **--mount** 参数时如果本地目录不存在，Docker 会报错。\nDocker 挂载主机目录的默认权限是 读写，用户也可以通过增加 readonly 指定为 只读。\n8. 容器互联 下面先创建一个新的 Docker 网络。\n$ docker network create -d bridge my-net -d 参数指定 Docker 网络类型，有 bridge overlay。其中 overlay 网络类型用于 Swarm mode，在本小节中你可以忽略它。\n附录1：常见问题总结 附录2：Docker命令查询 "},"title":"Docker重要Tips"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/docker/docker-compose%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/":{"data":{"":"","#":" 和Dockerfile为了编译镜像不同，Compose管理的是容器的编排，但是Compose本身也可以去build镜像\n模板文件是使用 Compose 的核心，涉及到的指令关键字也比较多。但大家不用担心，这里面大部分指令跟 docker run 相关参数的含义都是类似的。\n默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。\nversion: \"3\" services: webapp: image: examples/web ports: - \"80:80\" volumes: - \"/data\" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。\n如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中重复设置。\n下面分别介绍各个指令的用法。\nbuild 指定 **Dockerfile** 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。\nversion: '3' services: webapp: build: ./dir 你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径。\n使用 dockerfile 指令指定 Dockerfile 文件名。\n使用 arg 指令指定构建镜像时的变量。\nversion: '3' services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 使用 cache_from 指定构建镜像的缓存\nbuild: context: . cache_from: - alpine:latest - corp/web_app:3.14 cap_add, cap_drop 指定容器的内核能力（capacity）分配。\n例如，让容器拥有所有能力可以指定为：\ncap_add: - ALL 去掉 NET_ADMIN 能力可以指定为：\ncap_drop: - NET_ADMIN command 覆盖容器启动后默认执行的命令。\ncommand: echo \"hello world\" configs 仅用于 Swarm mode，详细内容请查看 [Swarm mode](https://www.bookstack.cn/read/docker_practice-1.3.0/$swarm_mode) 一节。\ncgroup_parent 指定父 cgroup 组，意味着将继承该组的资源限制。\n例如，创建了一个 cgroup 组名称为 cgroups_1。\ncgroup_parent: cgroups_1 container_name 指定容器名称。默认将会使用 项目名称_服务名称_序号 这样的格式。\ncontainer_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。\ndeploy 仅用于 Swarm mode，详细内容请查看 [Swarm mode](https://www.bookstack.cn/read/docker_practice-1.3.0/$swarm_mode) 一节\ndevices 指定设备映射关系。\ndevices: - \"/dev/ttyUSB1:/dev/ttyUSB0\" depends_on 解决容器的依赖、启动先后的问题。以下例子中会先启动 redis db 再启动 web\nversion: '3' services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。\ndns 自定义 DNS 服务器。可以是一个值，也可以是一个列表。\ndns: 8.8.8.8 dns: - 8.8.8.8 - 114.114.114.114 dns_search 配置 DNS 搜索域。可以是一个值，也可以是一个列表。\ndns_search: example.com dns_search: - domain1.example.com - domain2.example.com tmpfs 挂载一个 tmpfs 文件系统到容器。\ntmpfs: /run tmpfs: - /run - /tmp env_file 从文件中获取环境变量，可以为单独的文件路径或列表。\n如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file 中变量的路径会基于模板文件路径。\n如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。\nenv_file: .env env_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。\n# common.env: Set development environment PROG_ENV=development environment 设置环境变量。你可以使用数组或字典两种格式。\n只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。\nenvironment: RACK_ENV: development SESSION_SECRET: environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达 布尔 含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括\ny|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF expose 暴露端口，但不映射到宿主机，只被连接的服务访问。\n仅可以指定内部端口为参数\nexpose: - \"3000\" - \"8000\" external_links 注意：不建议使用该指令。\n链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。\nexternal_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql extra_hosts 类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。\nextra_hosts: - \"googledns:8.8.8.8\" - \"dockerhub:52.1.157.61\" 会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。\n8.8.8.8 googledns 52.1.157.61 dockerhub healthcheck 通过命令检查容器是否健康运行。\nhealthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"] interval: 1m30s timeout: 10s retries: 3 image 指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉取这个镜像。\nimage: ubuntu image: orchardup/postgresql image: a4bc65fd labels 为容器添加 Docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。\nlabels: com.startupteam.description: \"webapp for a startup team\" com.startupteam.department: \"devops department\" com.startupteam.release: \"rc3 for v1.0\" links 注意：不推荐使用该指令。\nlogging 配置日志选项。\nlogging: driver: syslog options: syslog-address: \"tcp://192.168.0.42:123\" 目前支持三种日志驱动类型。\ndriver: \"json-file\" driver: \"syslog\" driver: \"none\" options 配置日志驱动的相关参数。\noptions: max-size: \"200k\" max-file: \"10\" network_mode 设置网络模式。使用和 docker run 的 --network 参数一样的值。\nnetwork_mode: \"bridge\" network_mode: \"host\" network_mode: \"none\" network_mode: \"service:[service name]\" network_mode: \"container:[container name/id]\" networks 配置容器连接的网络。\nversion: \"3\" services: some-service: networks: - some-network - other-network networks: some-network: other-network: pid 跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。\npid: \"host\" ports 暴露端口信息。\n使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\nports: - \"3000\" - \"8000:8000\" - \"49100:22\" - \"127.0.0.1:8001:8001\" 注意：当使用 _HOST:CONTAINER_ 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 _YAML_ 会自动解析 _xx:yy_ 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。\nsecrets 存储敏感数据，例如 mysql 服务密码。\nversion: \"3.1\" services: mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password secrets: - db_root_password - my_other_secret secrets: my_secret: file: ./my_secret.txt my_other_secret: external: true security_opt 指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。\nsecurity_opt: - label:user:USER - label:role:ROLE stop_signal 设置另一个信号来停止容器。在默认情况下使用的是 SIGTERM 停止容器。\nstop_signal: SIGUSR1 sysctls 配置容器内核参数。\nsysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0 sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits 指定容器的 ulimits 限制值。\n例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。\nulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes 数据卷所挂载路径设置。可以设置为宿主机路径(HOST:CONTAINER)或者数据卷名称(VOLUME:CONTAINER)，并且可以设置访问模式 （HOST:CONTAINER:ro）。\n该指令中路径支持相对路径。\nvolumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 如果路径为数据卷名称，必须在文件中配置数据卷。\nversion: \"3\" services: my_src: image: mysql:8.0 volumes: - mysql_data:/var/lib/mysql volumes: mysql_data: 其它指令 此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir 等指令，基本跟 docker run 中对应参数的功能一致。\n指定服务容器启动后执行的入口文件。\nentrypoint: /code/entrypoint.sh 指定容器中运行应用的用户名。\nuser: nginx 指定容器中工作目录。\nworking_dir: /code 指定容器中搜索域名、主机名、mac 地址等。\ndomainname: your_website.com hostname: test mac_address: 08-00-27-00-0C-0A 允许容器中运行一些特权命令。\nprivileged: true 指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。\nrestart: always 以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。\nread_only: true 打开标准输入，可以接受外部输入。\nstdin_open: true 模拟一个伪终端。\ntty: true 读取变量 Compose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。\n例如，下面的 Compose 文件将从运行它的环境中读取变量 ${MONGO_VERSION} 的值，并写入执行的指令中。\nversion: \"3\" services: db: image: \"mongo:${MONGO_VERSION}\" 如果执行 MONGO_VERSION=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 MONGO_VERSION=2.8 docker-compose up 则会启动一个 mongo:2.8 镜像的容器。\n若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。\n在当前目录新建 .env 文件并写入以下内容。\n# 支持 # 号注释MONGO_VERSION=3.6 执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。\n参考资料 官方文档 awesome-compose "},"title":"Docker-compose文件详解"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/docker/dockerfile-%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/":{"data":{"":"说在前面：dockerfile的根本目的是去构建一个image，在最后运行docker build命令，并不是对containers的管理","1-copy和add-复制文件#1. COPY和ADD  复制文件":"在使用该指令的时候还可以加上 --chown=\u003cuser\u003e:\u003cgroup\u003e 选项来改变文件的所属用户及所属组。\nCOPY --chown=55:mygroup files* /mydir/ COPY --chown=bin files* /mydir/ COPY --chown=1 files* /mydir/ COPY --chown=10:11 files* /mydir/ ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。\nADD 指令，比如 \u003c源路径\u003e 可以是一个 URL，就会下载，件权限自动设置为 600。如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。\n如果 \u003c源路径\u003e 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u003c目标路径\u003e 去。\n在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 **ADD** 的场合，就是所提及的需要自动解压缩的场合。","10-onbuild-为他人做嫁衣裳#10. ONBUILD 为他人做嫁衣裳":"格式：ONBUILD \u003c其它指令\u003e。\nONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。\nDockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。\n假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile：\nFROM node:slim RUN mkdir /app WORKDIR /app COPY ./package.json /app RUN [ \"npm\", \"install\" ] COPY . /app/ CMD [ \"npm\", \"start\" ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。\n如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。\n那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为：\nFROM node:slim RUN mkdir /app WORKDIR /app CMD [ \"npm\", \"start\" ] 这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为：\nFROM my-node COPY ./package.json /app RUN [ \"npm\", \"install\" ] COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。\n那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。\nONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile:\nFROM node:slim RUN mkdir /app WORKDIR /app ONBUILD COPY ./package.json /app ONBUILD RUN [ \"npm\", \"install\" ] ONBUILD COPY . /app/ CMD [ \"npm\", \"start\" ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地：\nFROM my-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。","11-label-指令#11. LABEL 指令":"LABEL 指令用来给镜像以键值对的形式添加一些元数据（metadata）。\n参考：Dockerfile 指令详解 - LABEL 为镜像添加元数据 - 《Docker — 从入门到实践（v1.3.0）》 - 书栈网 · BookStack\n参考","12-shell-指令#12. SHELL 指令":"格式：SHELL [\"executable\", \"parameters\"]\nSHELL 指令可以指定 RUN ENTRYPOINT CMD 指令的 shell，Linux 中默认为 [\"/bin/sh\", \"-c\"]","13-多阶段构建#13. 多阶段构建":"Docker v17.05 开始支持多阶段构建 (multistage builds)。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile\n参考：使用镜像 - Dockerfile 多阶段构建 - 《Docker — 从入门到实践（v1.3.0）》 - 书栈网 · BookStack","14-构建多种系统架构支持的-docker-镜像--docker-manifest-命令详解#14. 构建多种系统架构支持的 Docker 镜像 — docker manifest 命令详解":"我们发现在 Linux x86_64 和 Linux arm64v8 架构的计算机中分别使用 golang:alpine 镜像运行容器 $ docker run golang:alpine go version 时，容器能够正常的运行。\n这是什么原因呢？\n原因就是 golang:alpine 官方镜像有一个 [manifest](https://docs.docker.com/registry/spec/manifest-v2-2/) 列表 ([manifest list](https://docs.docker.com/registry/spec/manifest-v2-2/))。\n当用户获取一个镜像时，Docker 引擎会首先查找该镜像是否有 manifest 列表，如果有的话 Docker 引擎会按照 Docker 运行环境（系统及架构）查找出对应镜像（例如 golang:alpine）。如果没有的话会直接获取镜像（例如上例中我们构建的 username/test）。\n我们可以使用 $ docker manifest inspect golang:alpine 查看这个 manifest 列表的结构。\n参考：使用镜像 - 构建多种系统架构支持的 Docker 镜像 - 《Docker — 从入门到实践（v1.3.0）》 - 书栈网 · BookStack","2-cmd-和run-启动命令#2. CMD 和RUN 启动命令":"Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。\n仅仅使用一个 **RUN** 指令，并使用 **\u0026\u0026** 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。\n还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。\nCMD 指令的格式和 RUN 相似，也是两种格式：\nshell 格式：CMD \u003c命令\u003e exec 格式：CMD [\"可执行文件\", \"参数1\", \"参数2\"...] 参数列表格式：CMD [\"参数1\", \"参数2\"...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。**CMD**** 指令就是用于指定默认的容器主进程的启动命令的。**\n在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 \"，而不要使用单引号。\nCMD和ENTRYPOINT只能写一个，多个的话，以最后那个为准。\n提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。\nDocker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。\n一些初学者将 CMD 写为：\nCMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。\n对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。\n而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ \"sh\", \"-c\", \"service nginx start\"]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。\n正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如：\nCMD [\"nginx\", \"-g\", \"daemon off;\"] ","3-entrypoint-入口点#3. ENTRYPOINT 入口点":"那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" 有什么好处么？让我们来看几个场景。\n场景一：让镜像变成像命令一样使用 假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现：\nFROM ubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* CMD [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行：\n$ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？\n$ docker run myip -i docker: Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"exec: \\\\\\\"-i\\\\\\\": executable file not found in $PATH\\\"\\n\". 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s http://myip.ipip.net 后面。而 -i 根本不是命令，所以自然找不到。\n那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令：\n$ docker run myip curl -s http://myip.ipip.net -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像：\nFROM ubuntu:18.04RUN apt-get update \\ \u0026\u0026 apt-get install -y curl \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* ENTRYPOINT [ \"curl\", \"-s\", \"http://myip.ipip.net\" ] 这次我们再来尝试直接使用 docker run myip -i：\n$ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 $ docker run myip -i HTTP/1.1 200 OK Server: nginx/1.8.0 Date: Tue, 22 Nov 2016 05:12:40 GMT Content-Type: text/html; charset=UTF-8 Vary: Accept-Encoding X-Powered-By: PHP/5.6.24-1~dotdeb+7.1 X-Cache: MISS from cache-2 X-Cache-Lookup: MISS from cache-2:80 X-Cache: MISS from proxy-2_6 Transfer-Encoding: chunked Via: 1.1 cache-2:80, 1.1 proxy-2_6:8006 Connection: keep-alive 当前 IP：61.148.226.66 来自：北京市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。\n场景二：应用运行前的准备工作 启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。\n比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。\n此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。\n这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 \u003cCMD\u003e）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的：\nFROM alpine:3.4 ... RUN addgroup -S redis \u0026\u0026 adduser -S -G redis redis ... ENTRYPOINT [\"docker-entrypoint.sh\"] EXPOSE 6379 CMD [ \"redis-server\" ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。\n#!/bin/sh ... # allow the container to be started with `--user` if [ \"$1\" = 'redis-server' -a \"$(id -u)\" = '0' ]; then find . \\! -user redis -exec chown redis '{}' + exec gosu redis \"$0\" \"$@\" fi exec \"$@\" 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如：\n$ docker run -it redis id uid=0(root) gid=0(root) groups=0(root) ","4-env和arg-环境变量#4. ENV和ARG 环境变量":"ENV格式有两种：\nENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... ARG格式：ARG \u003c参数名\u003e[=\u003c默认值\u003e]\nARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。\nARG 指令有生效范围，如果在 FROM 指令之前指定，那么只能用于 FROM 指令中。\n感觉ENV比ARG实用","5-volume-定义匿名卷#5. VOLUME 定义匿名卷":"格式为：\nVOLUME [\"\u003c路径1\u003e\", \"\u003c路径2\u003e\"...] VOLUME \u003c路径\u003e 容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中。在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。\n当然，运行容器时可以覆盖这个挂载设置。比如：\n$ docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。","6-expose-声明端口#6. EXPOSE 声明端口":"格式为 EXPOSE \u003c端口1\u003e [\u003c端口2\u003e...]\n**EXPOSE**** 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射**","7-workdir-指定工作目录#7. WORKDIR 指定工作目录":"格式为 WORKDIR \u003c工作目录路径\u003e。\n使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。\n之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：\nRUN cd /app RUN echo \"hello\" \u003e world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。\n因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。\nWORKDIR /app RUN echo \"hello\" \u003e world.txt 如果你的 WORKDIR 指令使用的相对路径，那么所切换的路径与之前的 WORKDIR 有关：\nWORKDIR /a WORKDIR b WORKDIR c RUN pwd RUN pwd 的工作目录为 /a/b/c。","8-user-指定当前用户#8. USER 指定当前用户":"格式：USER \u003c用户名\u003e[:\u003c用户组\u003e]\nUSER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。\n注意，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。\n如果没有用户，需要先RUN groupadd -r group \u0026\u0026 useradd -r -g user group","9-healthcheck-健康检查#9. HEALTHCHECK 健康检查":"自 1.12 之后，Docker 提供了 HEALTHCHECK 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。\n当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。\n和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。\n感觉不重要，放个链接在这，HEALTHCHECK 健康检查"},"title":"Dockerfile 指令详解"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/git%E7%B3%BB%E4%BD%BF%E7%94%A8/":{"data":{"git#Git":"git代码从A库某分支迁到B库某分支（亲测有效）_fu_jian_feng的博客-CSDN博客_git 仓库a引用仓库b部分代码\nGit报错解决：OpenSSL SSL_read: Connection was reset, errno 10054 错误解决 - 沾青先生 - 博客园 Learn Git with Bitbucket Cloud | Atlassian Git Tutorial","git-branch#Git-branch":"一个可以提高开发效率的命令：cherry-pick\nStop cherry-picking, start merging, Part 1: The merge conflict\ngit删除远程分支和本地分支 - 罗松超 - 博客园\ngit命令－切换分支_木_凌的博客-CSDN博客_git 更换分支\nGit自学成才——新建远程分支和删除\n3.4 使用分支 · geeeeeeeeek/git-recipes Wiki","github#Github":"Github如何高效利用GitHub\nGitHub Wiki 页面的添加和设置 - 掘金\n如何利用GitHub进行代码审查\n2020年，一招教你打造个性化的Github主页！_依然范特稀西的博客-CSDN博客\nGitHub - anuraghazra/github-readme-stats: Dynamically generated stats for your github readmes GitHub Docs\nWhat You Can Do With Gists on Github? - Digital Inspiration\n如何看待 Github Gist 这个服务，怎样更好的利用？ - 知乎","github-action#Github-action":"Github Actions自动化部署_liannian9_zlh的博客-CSDN博客\n使用github action发布python包到Pypi - hiyang - 博客园\nGithub Action将项目自动部署到ph-pages步骤-百度经验\n编写自己的 GitHub Action，体验自动化部署\n从 GitLab CI/CD 迁移到 GitHub Actions","gitlab#Gitlab":"手把手教你 GitLab 的安装及使用\ngitlab持续集成，自动部署 - fengxiaofeng - 博客园\ngitlab自动部署搭建历程\nDocker + GitLab CI/CD 实现自动化部署_薛洋洋洋的博客-CSDN博客\nGitLab+Docker快速搭建CI/CD自动化部署"},"title":"Git系使用"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/ide%E4%BD%BF%E7%94%A8/":{"data":{"idea使用#idea使用":"idea使用社区版Intellij IDEA中配置Tomcat服务器_拾年一剑的博客-CSDN博客_intelj社区版可以配置tomcat么\nidea 启动项目错误解决：错误信息Command line is too long._我是阿木木的博客-CSDN博客_too long idea\nIntelliJ 自动生成测试类, 自定义测试类","idea技巧#idea技巧":"IDEA怎么生成UML类图_hy_coming的博客-CSDN博客_idea生成uml"},"title":"IDE使用"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/netty/":{"data":{"":"超详细Netty入门，看这篇就够了！-阿里云开发者社区\nNetty概述 - Netty教程\nNetty 学习手册"},"title":"Netty"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/nginx/":{"data":{"nginx常见名词释义#Nginx常见名词释义":"","nginx高并发转发time_wait#Nginx高并发转发TIME_WAIT":"Nginx高并发转发TIME_WAIT记一次nginx与linux内核性能调优的过程 - 思凡念真 - 博客园\nNginx 实现 10w+ 并发之 Linux 内核优化-阿里云开发者社区","反向代理#反向代理":"Nginx——nginx 反向代理 （建议收藏）\nnginx–❤️图解及代码实现正向代理、反向代理及负载均衡（非常实用，建议收藏❤️）-云社区-华为云","多个域名复用一个ip#多个域名复用一个IP":"通过Nginx反向代理实现多个域名站点复用一个公网IP","静态代理#静态代理":"把所有静态资源的访问改为访问nginx，而不是访问tomcat，这种方式叫静态代理。因为nginx更擅长于静态资源的处理，性能更好，效率更高。 所以在实际应用中，我们将静态资源比如图片、css、html、js等交给nginx处理，而不是由tomcat处理。"},"title":"Nginx"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/redis/":{"data":{"kafka#Kafka":"超详细“零”基础kafka入门篇 - alonghub - 博客园\n四万字32图，Kafka知识体系保姆级教程宝典_五分钟学大数据的技术博客_51CTO博客","redis#Redis":"RedisRedis和数据库 数据同步问题的解决_Redis_脚本之家\nWhich MicroServices Message Broker To Choose?\n1. Redis 文档 - 1.1. 键空间通知（keyspace notification） - 《Redis 中文文档》 - 书栈网 · BookStack 9. Redis 持久化 - 《Redis 学习教程》 - 书栈网 · BookStack\n10个很多人不知道的Redis使用技巧 - Java架构师追风 - 博客园\nRedis缓存使用技巧和设计方案 - 墨天轮\nRedis 命令参考 — Redis 命令参考\n连redis 连不上怎么办！ - 花开如梦 - 博客园\n解决Unable to connect to Redis server: 192.168.110.1/192.168.110.1:6379_zeal9s的博客-CSDN博客_unable to connect to redis server:\nRedis Desktop连接不上Centos7上部署的redis数据库解决方法_qq_35673617的博客-CSDN博客 Liunx中如何让Redis-server后台运行-百度经验"},"title":"Redis"},"/lyd-notes/2.%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E5%B7%A5%E5%85%B7/redis/redis-tips/":{"data":{"":"","1字符串#1.字符串":"incr ­­ 对key的值加1操作,incr一个不存在key,则设置key为1\n\u003e set age 20 \u003e incr age (integer) 21 incrby ­­ 同incr类似,加指定值 ,key不存在时候初始设置value为 0的key,再相加\n\u003e incrby age 5 (integer) 26 \u003e incrby age1111 5 (integer) 5 \u003e get age1111\"5\" decr ­­ 对key的值做的是减1操作,decr一个不存在key,则设置key为-­1\ndecrby ­­ 同decr,减指定值\nstrlen 返回字符串的长度\nsetex ­­ 设置key对应字符串value，并且设置key在给定的seconds时间之后超时过期。\nappend 追加字符串\nsubstr 截取字符串\nsetrange key offset value 改写字符串","2hash#2.HASH":"基础操作之前加h\nhexists ­­ 测试指定field是否存在。\n\u003e hexists myhash field1 (integer) 1 \u003e hexists myhash field9 (integer) 0 hlen ­­ 返回指定hash的field数量。\n\u003e hlen myhash (integer) 3 hstrlen — 返回 hash指定field的value的字符串长度\n\u003e hstrlen myhash field1 (integer) 5 hkeys ­­ 返回hash的所有field。\nhvals ­­ 返回hash的所有value。\nhgetall ­­ 获取某个hash中全部的filed及value。","3list#3.List":"基础操作之前加l(从左边),r(从右边)","4事务transaction#4.事务（transaction）":"MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。\n事务可以一次执行多个命令， 并且带有以下两个重要的保证：\n事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令：\n如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。\n如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。\n使用 redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。\n从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。\n用法 MULTI 命令用于开启一个事务，它总是返回 OK 。\nMULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。\n另一方面， 通过调用 DISCARD ， 客户端可以清空事务队列， 并放弃执行事务。\n以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值：\n\u003e MULTI OK \u003e INCR foo QUEUED \u003e INCR bar QUEUED \u003e EXEC 1) (integer) 1 2) (integer) 1 EXEC 命令的回复是一个数组， 数组中的每个元素都是执行事务中的命令所产生的回复。 其中， 回复元素的先后顺序和命令发送的先后顺序一致。\n当客户端处于事务状态时， 所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply）， 这些被入队的命令将在 EXEC 命令被调用时执行。\n事务中的错误 使用事务时可能会遇上以下两种错误：\n事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。 对于发生在 EXEC 执行之前的错误，检查命令入队所得的返回值，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。\n在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。\n至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。\nEXEC 返回两条批量回复（bulk reply）： 第一条是 OK ，而第二条是 -ERR 。 至于怎样用合适的方法来表示事务中的错误， 则是由客户端自己决定的。\n最重要的是记住这样一条， 即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。\n为什么 Redis 不支持回滚（roll back） 如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。\n以下是这种做法的优点：\nRedis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR ， 回滚是没有办法处理这些情况的。 鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。\n放弃事务 当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出：\nredis\u003e SET foo 1OK redis\u003e MULTI OK redis\u003e INCR foo QUEUED redis\u003e DISCARD OK redis\u003e GET foo\"1\" 使用 check-and-set 操作实现乐观锁 WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。\n被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已经失败。\n举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR 不存在）。\n首先我们可能会这样做：\nval = GET mykey val = val + 1 SET mykey $val 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。\n举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。\n有了 WATCH ， 我们就可以轻松地解决这类问题了：\nWATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。\n这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。\n了解 WATCH WATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行。\nNote\n如果你使用 WATCH 监视了一个带过期时间的键， 那么即使这个键过期了， 事务仍然可以正常执行， 关于这方面的详细情况，请看这个帖子： http://code.google.com/p/redis/issues/detail?id=270\nWATCH 命令可以被调用多次。 对键的监视从 WATCH 执行之后开始生效， 直到调用 EXEC 为止。\n用户还可以在单个 WATCH 命令中监视任意多个键， 就像这样：\nredis\u003e WATCH key1 key2 key3 OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。\n另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。\n使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。\n使用 WATCH 实现 ZPOP WATCH 可以用于创建 Redis 没有内置的原子操作。\n举个例子， 以下代码实现了原创的 ZPOP 命令， 它可以原子地弹出有序集合中分值（score）最小的元素：\nWATCH zset element = ZRANGE zset 0 0 MULTI ZREM zset element EXEC 程序只要重复执行这段代码， 直到 EXEC 的返回值不是空多条回复（null multi-bulk reply）即可。\nRedis 脚本和事务 从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。\n因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。\n不过我们并不打算在短时间内就移除事务功能， 因为事务提供了一种即使不使用脚本， 也可以避免竞争条件的方法， 而且事务本身的实现并不复杂。\n不过在不远的将来， 可能所有用户都会只使用脚本来实现事务也说不定。 如果真的发生这种情况的话， 那么我们将废弃并最终移除事务功能。"},"title":"Redis Tips"},"/lyd-notes/3.%E9%AB%98%E8%B4%A8%E9%87%8F%E4%BB%A3%E7%A0%81/":{"data":{"io#IO":"聊聊 IO 多路复用","设计模式#设计模式":"设计模式MyBatis 和 Spring 中的 15 种设计模式，真香，拿去用吧！-技术圈\nnotes/设计模式（45种）.md at master · guanguans/notes"},"title":"高质量代码"},"/lyd-notes/4.%E6%8E%A5%E5%8F%A3%E5%92%8C%E7%BD%91%E7%BB%9C/":{"data":{"httpclient-http连接配置#httpClient http连接配置":"HttpClient4.5的连接管理_马路上开飞机的博客-CSDN博客_httpclient4.5 连接管理\nHttpClient连接池指标分析","jwt#JWT":"JWT什么是 JWT – JSON WEB TOKEN","resttemplate#RestTemplate":"【WEB系列】如何支持下划线驼峰互转的传参与返回\n精讲RestTemplate第4篇-POST请求方法使用详解 - 腾讯云开发者社区-腾讯云\nRestTemplate Post Request with JSON\nfastjson反序列化嵌套类为对应实体类 ResultObject的反序列化及泛型优化\nspring boot resttemplate使用fastjson 转换器_我的裤衩呢的博客-CSDN博客_resttemplate 使用fastjson\n关于RestTemplate反序列化嵌套对象的问题_小楼夜听雨QAQ的博客-CSDN博客_resttemplate反序列化\nspringboot系列十二、springboot集成RestTemplate及常见用法 - 小人物的奋斗 - 博客园\nJackson泛型反序列化的5种方式 - zincredible - 博客园","rpc#RPC":"HTTP和RPC的优缺点-阿里云开发者社区\nRPC 和 REST 的优缺点、区别、如何选择"},"title":"接口和网络"},"/lyd-notes/5.%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86/spring-batch-notes/":{"data":{"1-chunk-oriented-processing#1. Chunk-oriented Processing":" 参考1：Chunk-oriented Processing官方相关文档 参考2：demo项目 1. 只需要读和写： 2. 读、处理、写： ","2-taskletstep#2. TaskletStep":"Chunk-oriented processing is not the only way to process in a Step. What if a Step must consist of a simple stored procedure call? You could implement the call as an ItemReader and return null after the procedure finishes. However, doing so is a bit unnatural, since there would need to be a no-op ItemWriter. Spring Batch provides the TaskletStep for this scenario.\nTasklet is a simple interface that has one method, execute, which is called repeatedly by the TaskletStep until it either returns RepeatStatus.FINISHED or throws an exception to signal a failure. Each call to a Tasklet is wrapped in a transaction. Tasklet implementors might call a stored procedure, a script, or a simple SQL update statement.\nTo create a TaskletStep, the bean passed to the tasklet method of the builder should implement the Tasklet interface. No call to chunk should be called when building a TaskletStep. The following example shows a simple tasklet:\n@Bean public Step step1() { return this.stepBuilderFactory.get(\"step1\") .tasklet(myTasklet()) .build(); } TaskletStep官方相关文档","3-使用场景#3. 使用场景":"Tasklet 适用于该 Step 操作不需要读操作，或不需要写操作，或两者都不需要。\n而Chunk则适用于典型的Read、Process、Write 操作。\n官网的介绍中提到，如果你的步骤包含一个简单的存储过程调用（只存，不需要写），那就可以用Tasklet。当然，你也可以用Chunk，但这时，你的ItemWritter就是一个空实现，看着不自然。\n而官网的Tasklet例子只对文件进行删除，不需要读、写文件的内容，因此使用Tasklet也就非常合适了。","jobbuilderfactory#JobBuilderFactory":"JobBuilderFactory是Spring Batch提供的一个工厂类,用来创建JobBuilder的实例。JobBuilder是一个帮助类，用于配置和构建Job实例。JobBuilderFactory通常用于Spring Batch应用程序的配置中，用来创建和配置Job实例。\nJobBuilderFactory的主要目的是提供一种简洁方便的方式来创建和配置Job实例。它提供了一些方法来配置Job的各个方面，如名称、要使用的**JobRepository**和要使用的**JobExecutionListener**。\n使用JobBuilderFactory需要在Spring Batch应用程序配置文件中创建它的一个实例。之后，可以使用JobBuilderFactory来创建和配置Job实例， 可以通过调用它的方法来实现。\n下面是一个在Spring Batch应用程序配置文件中使用JobBuilderFactory的示例：\n@Autowired private JobBuilderFactory jobBuilderFactory; @Bean public Job job1() { return jobBuilderFactory.get(\"job1\") .start(step1()) .build(); } 在这个例子中，在配置类中创建了一个JobBuilderFactory的实例并进行自动装配，然后job1()方法使用JobBuilderFactory创建和配置了名为\"job1\"的Job实例，并且这个Job实例只有一个step \"step1\"。\n总之, JobBuilderFactory 是 Spring Batch 提供的工厂类，用来创建和配置 Job 实例，提供了一种简洁方便的方式来创建和配置 Job 实例, 通过调用它的方法来实现。","jobexplorer#JobExplorer":"JobExplorer是Spring Batch中的一个重要接口，主要作用是查询和检索作业的执行状态和历史信息。它主要用于支持监控和管理作业的运行状态。\nJobExplorer提供了很多方法，可以查询作业执行的状态，例如：\ngetJobNames()：获取所有作业名称。 getJobInstance(JobInstance jobInstance)：根据JobInstance查询作业实例。 getJobExecutions(JobInstance jobInstance)：根据JobInstance查询作业执行。 getStepExecutions(JobExecution jobExecution)：根据JobExecution查询步骤执行。 JobExplorer 一般是和 JobRepository 配合使用， **JobRepository**负责维护作业状态， **JobExplorer** 则用于查询和检索这些状态。\n在实际项目中，您可以使用JobExplorer查询作业的运行状态，并在需要时对其进行重启或停止。","joblauncher#JobLauncher":"","jobrepository#JobRepository":"","mapjobrepositoryfactorybean和jobrepositoryfactorybean#MapJobRepositoryFactoryBean和JobRepositoryFactoryBean":"MapJobRepositoryFactoryBean和JobRepositoryFactoryBean是Spring Batch提供的两个工厂类，用来创建JobRepository实例。两者的主要区别在于它们使用的存储机制不同。\nMapJobRepositoryFactoryBean用来创建一个使用内存映射存储任务执行信息的JobRepository实例。这意味着任务执行信息不会在应用程序的不同运行之间持久化，只能在当前应用程序实例中使用。这通常用于开发或测试环境，在这些环境中不需要数据持久化。\n而JobRepositoryFactoryBean用来创建一个使用数据库存储任务执行信息的JobRepository实例。这意味着任务执行信息可以在应用程序的不同运行之间持久化，可以在以后检索分析或重新运行。这通常用于生产环境，需要数据持久化。\n总的来说, 如果不需要数据持久化，可以使用MapJobRepositoryFactoryBean，否则使用JobRepositoryFactoryBean更好。","platformtransactionmanager#PlatformTransactionManager":"PlatformTransactionManager是Spring Batch中一个关键的组件，主要用于提供批处理作业的事务管理。它负责管理批处理作业的事务边界，如在作业开始前启动新事务，在作业完成后提交或回滚事务。\n其中主要作用是保证批处理作业处理的数据的一致性和完整性。通过在作业开始前启动一个事务，并在作业完成后提交或回滚事务， PlatformTransactionManager能确保在作业期间对数据进行的所有修改都能被完全提交或完全回滚，即使在发生错误或失败的情况下也是如此。\n另一个用途是通过减少数据库提交次数来提高批处理作业的性能。通过在单个事务中包装多个数据库操作， PlatformTransactionManager可以大大减少数据库提交次数，提高批处理作业的性能。 PlatformTransactionManager通常与其他Spring Batch组件，如JobRepository和ItemWriter结合使用，为批处理作业提供完整的事务管理解决方案。\n总之, PlatformTransactionManager负责管理批处理作业的事务边界，用来保证批处理作业处理的数据的一致性和完整性，并且通过减少数据库提交次数来提高批处理作业的性能。","running-a-job#Running a job":"","step#Step":"JobRepositoryJobRepository是Spring Batch中一个关键的组件，负责管理任务执行信息的持久性。它提供了一个中央位置来存储批处理作业的执行信息，如作业执行的开始时间、结束时间和状态。\nJobRepository的主要目的是提供一种将任务执行信息持久化的方法，以便在后面可以用于分析或重新运行。存储在JobRepository中的任务执行信息可用于确定作业的状态、识别在作业执行期间发生的错误以及重新运行失败的作业。\nJobRepository可以使用不同的存储机制实现,比如数据库和内存映射等。Spring Batch提供了内置的JobRepository实现，可将任务执行信息存储在数据库中，可以通过设置数据源和使用Spring Batch提供的SQL脚本在数据库中创建必要的表来配置。\n使用JobRepository需要在Spring Batch应用程序配置文件中配置它，并定义一个PlatformTransactionManager来管理事务。之后，可以在批处理作业中使用JobRepository来更新作业的状态和检索有关过去作业执行的信息。\n总之，JobRepository是Spring Batch中负责管理任务执行信息持久性的组件，它用来存储批处理作业的执行信息，如开始时间、结束时间和状态。这些信息可以用来分析任务执行情况，并帮助我们决定是否需要重新运行任务,可以使用不同的存储机制来实现，如数据库和内存映射等, 为了使用JobRepository，需要在Spring Batch应用程序配置文件中配置数据源，并使用Spring Batch提供的SQL脚本在数据库中创建必要的表, 之后可以在批处理作业中使用JobRepository来更新作业的状态和检索有关过去作业执行的信息。需要注意的是，在使用JobRepository时需要配置PlatformTransactionManager来管理事务, 保证数据的持久性和一致性。\nWhen using java configuration, a JobRepository is provided for you. A JDBC based one is provided out of the box if a DataSource is provided, the Map based one if not. However you can customize the configuration of the JobRepository via an implementation of the BatchConfigurer interface.\nWhen using @EnableBatchProcessing, a JobRegistry is provided out of the box for you. This section addresses configuring your own.\nThe most basic implementation of the JobLauncher interface is the SimpleJobLauncher. Its only required dependency is a JobRepository\nJobLauncherJobLauncher是Spring Batch中用来启动作业的组件。它提供了一种简单的方式来启动作业，并且可以传递参数给作业，比如JobParameters。\nJobLauncher的主要目的是提供一种简单的方式来启动作业，并且可以传递参数给作业。它提供了run()方法来启动作业，这个方法接收两个参数，一个是要启动的作业的名称，另一个是用来传递给作业的参数。\n使用JobLauncher需要在Spring Batch应用程序配置文件中创建它的一个实例。之后，可以使用JobLauncher来启动作业。\n下面是一个在Spring Batch应用程序配置文件中使用JobLauncher的示例：\n@Autowired private JobLauncher jobLauncher; @Autowired private Job job; public void runJob() throws Exception { JobParameters jobParameters = new JobParametersBuilder() .addString(\"input.file\", \"input.txt\") .toJobParameters(); jobLauncher.run(job, jobParameters); } 在这个例子中，在配置类中创建了一个JobLauncher的实例并进行自动装配，然后runJob()方法使用JobLauncher来启动作业, 传递了一个JobParameters参数给作业，这个参数可以在作业中使用。\n总之, JobLauncher是Spring Batch中用来启动作业的组件, 提供了一种简单的方式来启动作业，并且可以传递参数给作业，使用方法是在应用程序配置文件中创建实例并使用run()方法来启动作业。\nRunning a job At a minimum, launching a batch job requires two things: the Job to be launched and a JobLauncher. Both can be contained within the same context or different contexts. For example, if launching a job from the command line, a new JVM will be instantiated for each Job, and thus every job will have its own JobLauncher. However, if running from within a web container within the scope of an HttpRequest, there will usually be one JobLauncher, configured for asynchronous job launching, that multiple requests will invoke to launch their jobs.\n如果是将spring-batch结合到web container，则需要一个异步jobAsynchronous Job\nHistorically, offline processing such as batch jobs have been launched from the command-line, as described above. However, there are many cases where launching from an HttpRequest is a better option. Many such use cases include reporting, ad-hoc job running, and web application support. Because a batch job by definition is long running, the most important concern is ensuring to launch the job asynchronously:\nStep","stepbuilderfactory#StepBuilderFactory":"StepBuilderFactory是Spring Batch提供的一个工厂类, 用来创建StepBuilder的实例。StepBuilder是一个帮助类，用于配置和构建Step实例。StepBuilderFactory通常用于Spring Batch应用程序的配置中，用来创建和配置Step实例。\nStepBuilderFactory的主要目的是提供一种简洁方便的方式来创建和配置Step实例。它提供了一些方法来配置Step的各个方面，如名称，要执行的tasklet，以及要使用的ItemReader, ItemProcessor,和ItemWriter。\n使用StepBuilderFactory需要在Spring Batch应用程序配置文件中创建它的一个实例。之后，可以使用StepBuilderFactory来创建和配置Step实例, 通过调用它的方法来实现。\n下面是一个在Spring Batch应用程序配置文件中使用StepBuilderFactory的示例：\n@Autowired private StepBuilderFactory stepBuilderFactory; @Bean public Step step1() { return stepBuilderFactory.get(\"step1\") .\u003cString, String\u003e chunk(10) .reader(itemReader()) .processor(itemProcessor()) .writer(itemWriter()) .build(); } 在这个例子中，在配置类中创建了一个StepBuilderFactory的实例并进行自动装配，然后step1()方法使用StepBuilderFactory创建和配置了名为\"step1\"的Step实例,并且这个Step实例的chunk的大小为10, reader, processor, writer 是 itemReader(), itemProcessor(), itemWriter() 总之，StepBuilderFactory是Spring Batch提供的工厂类，用来创建和配置Step实例，提供了一种简洁方便的方式来创建和配置Step实例，通过调用它的方法来实现。"},"title":"Spring batch Notes"},"/lyd-notes/5.%E6%89%B9%E9%87%8F%E5%A4%84%E7%90%86/xxl-job%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/":{"data":{"":"","#":" 依赖于Spring-boot时，通过xxl-job的注解@XxlJob来注册JobHandler xxl-job-core是一个依赖库，xxl-job-admin是一个可视化的web网页，xxl-job-executor-samples是执行器 xxl-job的框架使用即在该项目中改写或增加\n“调度中心”平台自身不承担业务逻辑，负责调度；任务抽象分散成JobHandler，交由执行器JobExecutor统一管理，负责任务 xxl-job使用的自研调度模块，早于Quartz 参考：XXL-JOB\n一、调度模块剖析 1 quartz的不足 Quartz作为开源作业调度中的佼佼者，是作业调度的首选。但是集群环境中Quartz采用API的方式对任务进行管理，从而可以避免上述问题，但是同样存在以下问题：\n问题一：调用API的的方式操作任务，不人性化； 问题二：需要持久化业务QuartzJobBean到底层数据表中，系统侵入性相当严重。 问题三：调度逻辑和QuartzJobBean耦合在同一个项目中，这将导致一个问题，在调度任务数量逐渐增多，同时调度任务逻辑逐渐加重的情况下，此时调度系统的性能将大大受限于业务； 问题四：quartz底层以“抢占式”获取DB锁并由抢占成功节点负责运行任务，会导致节点负载悬殊非常大；而XXL-JOB通过执行器实现“协同分配式”运行任务，充分发挥集群优势，负载各节点均衡。 XXL-JOB弥补了quartz的上述不足之处。\n2 自研调度模块 XXL-JOB最终选择自研调度组件（早期调度组件基于Quartz）；一方面是为了精简系统降低冗余依赖，另一方面是为了提供系统的可控度与稳定性；\nXXL-JOB中“调度模块”和“任务模块”完全解耦，调度模块进行任务调度时，将会解析不同的任务参数发起远程调用，调用各自的远程执行器服务。这种调用模型类似RPC调用，调度中心提供调用代理的功能，而执行器提供远程服务的功能。\n3 调度中心HA（集群） 基于数据库的集群方案，数据库选用Mysql；集群分布式并发环境中进行定时任务调度时，会在各个节点会上报任务，存到数据库中，执行时会从数据库中取出触发器来执行，如果触发器的名称和执行时间相同，则只有一个节点去执行此任务。\n4 调度线程池 调度采用线程池方式实现，避免单线程因阻塞而引起任务调度延迟。\n5 并行调度 XXL-JOB调度模块默认采用并行机制，在多线程调度的情况下，调度模块被阻塞的几率很低，大大提高了调度系统的承载量。\nXXL-JOB的不同任务之间并行调度、并行执行。\nXXL-JOB的单个任务，针对多个执行器是并行运行的，针对单个执行器是串行执行的。同时支持任务终止。\n6 过期处理策略 任务调度错过触发时间时的处理策略：\n可能原因：服务重启；调度线程被阻塞，线程被耗尽；上次调度持续阻塞，下次调度被错过； 处理策略： 过期超5s：本次忽略，当前时间开始计算下次触发时间 过期5s内：立即触发一次，当前时间开始计算下次触发时间 7 日志回调服务 调度模块的“调度中心”作为Web服务部署时，一方面承担调度中心功能，另一方面也为执行器提供API服务。\n8 调度日志 调度中心每次进行任务调度，都会记录一条任务日志，任务日志主要包括以下三部分内容：\n任务信息：包括“执行器地址”、“JobHandler”和“执行参数”等属性，点击任务ID按钮可查看，根据这些参数，可以精确的定位任务执行的具体机器和任务代码； 调度信息：包括“调度时间”、“调度结果”和“调度日志”等，根据这些参数，可以了解“调度中心”发起调度请求时具体情况。 执行信息：包括“执行时间”、“执行结果”和“执行日志”等，根据这些参数，可以了解在“执行器”端任务执行的具体情况； 调度日志，针对单次调度，属性说明如下：\n执行器地址：任务执行的机器地址； JobHandler：Bean模式表示任务执行的JobHandler名称； 任务参数：任务执行的入参； 调度时间：调度中心，发起调度的时间； 调度结果：调度中心，发起调度的结果，SUCCESS或FAIL； 调度备注：调度中心，发起调度的备注信息，如地址心跳检测日志等； 执行时间：执行器，任务执行结束后回调的时间； 执行结果：执行器，任务执行的结果，SUCCESS或FAIL； 执行备注：执行器，任务执行的备注信息，如异常日志等； 执行日志：任务执行过程中，业务代码中打印的完整执行日志，见“4.8 查看执行日志”； 10 任务依赖 原理：XXL-JOB中每个任务都对应有一个任务ID，同时，每个任务支持设置属性“子任务ID”，因此，通过“任务ID”可以匹配任务依赖关系。\n当父任务执行结束并且执行成功时，将会根据“子任务ID”匹配子任务依赖，如果匹配到子任务，将会主动触发一次子任务的执行。\n在任务日志界面，点击任务的“执行备注”的“查看”按钮，可以看到匹配子任务以及触发子任务执行的日志信息，如无信息则表示未触发子任务执行，可参考下图。 11 执行器 执行器实际上是一个内嵌的Server，默认端口9999（配置项：xxl.job.executor.port）。\n在项目启动时，执行器会通过“@JobHandler”识别Spring容器中“Bean模式任务”，以注解的value属性为key管理起来。\n“执行器”接收到“调度中心”的调度请求时，如果任务类型为“Bean模式”，将会匹配Spring容器中的“Bean模式任务”，然后调用其execute方法，执行任务逻辑。如果任务类型为“GLUE模式”，将会加载GLue代码，实例化Java对象，注入依赖的Spring服务（注意：Glue代码中注入的Spring服务，必须存在与该“执行器”项目的Spring容器中），然后调用execute方法，执行任务逻辑。\n二、任务注册, 任务自动发现 自v1.5版本之后, 任务取消了”任务执行机器”属性, 改为通过任务注册和自动发现的方式, 动态获取远程执行器地址并执行。\n1. AppName: 每个执行器机器集群的唯一标示, 任务注册以 \"执行器\" 为最小粒度进行注册; 每个任务通过其绑定的执行器可感知对应的执行器机器列表; 2. 注册表: 见\"xxl_job_registry\"表, \"执行器\" 在进行任务注册时将会周期性维护一条注册记录，即机器地址和AppName的绑定关系; \"调度中心\" 从而可以动态感知每个AppName在线的机器列表; 3. 执行器注册: 任务注册Beat周期默认30s; 执行器以一倍Beat进行执行器注册, 调度中心以一倍Beat进行动态任务发现; 注册信息的失效时间为三倍Beat; 4. 执行器注册摘除：执行器销毁时，将会主动上报调度中心并摘除对应的执行器机器信息，提高心跳注册的实时性； 为保证系统”轻量级”并且降低学习部署成本，没有采用Zookeeper作为注册中心，采用DB方式进行任务注册发现；\n三、执行器灰度上线 调度中心与业务解耦，只需部署一次后常年不需要维护。但是，执行器中托管运行着业务作业，作业上线和变更需要重启执行器，尤其是Bean模式任务。\n执行器重启可能会中断运行中的任务。但是，XXL-JOB得益于自建执行器与自建注册中心，可以通过灰度上线的方式，避免因重启导致的任务中断的问题。 步骤如下：\n1、执行器改为手动注册，下线一半机器列表（A组），线上运行另一半机器列表（B组）； 2、等待A组机器任务运行结束并编译上线；执行器注册地址替换为A组； 3、等待B组机器任务运行结束并编译上线；执行器注册地址替换为A组+B组； 操作结束； 四、任务超时控制 支持设置任务超时时间，任务运行超时的情况下，将会主动中断任务；\n需要注意的是，任务超时中断时与任务终止机制（可查看“4.9 终止运行中的任务”）类似，也是通过 “interrupt” 中断任务，因此业务代码需要将 “InterruptedException” 外抛，否则功能不可用。\n五、命令行任务 原生提供通用命令行任务Handler（Bean任务，”CommandJobHandler”）；业务方只需要提供命令行即可；\n如任务参数 “pwd” 将会执行命令并输出数据；"},"title":"XXL-JOB框架使用"},"/lyd-notes/6.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/":{"data":{"arthas#Arthas":"Arthas\nArthas Tutorials","单元测试#单元测试":"单元测试java单元测试规范/java单元测试开发规范.md · 18d99c9c3386f8de8352a947a489550145f9e3e4 · document-repository / specification\nHow to test Classes with @ConfigurationProperties and @Autowired\nHow to test spring batch job within @Transactional SpringBootTest test case?","性能测试#性能测试":"性能测试：动态报文长度处理方法\n两分钟读懂什么是TPS和QPS_张维鹏的博客-CSDN博客_tps和qps\njmeter加密解密（加密篇）","软件测试#软件测试":"功能测试：三轮测试的定义_yanzilee的博客-CSDN博客"},"title":"软件测试"},"/lyd-notes/6.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/%E7%94%A8powermock%E8%BF%9B%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/":{"data":{"mock单元测试#Mock单元测试":"[Solved] IllegalStateException: Could not initialize plugin MockMaker\nSpringRunner vs SpringBootTest\nMockito asks to add @PrepareForTest for the class even after adding @PrepareForTest\nPowermockito一些遇到的坑 - iWuYc - 博客园\nMockito 3.6.0 中文文档\nMockito - mockito-core 4.9.0 javadoc","一为什么使用powermock#一、为什么使用PowerMock？":"1. Mock的理念及使用场景 概念及使用场景 Mock在单元测试过程中一般指模拟对象的行为，一般用于以下场景： 1）依赖的对象无法获得，例如相关代码未实现或未交付。 2）依赖的对象构造用例时极为复杂，例如目的是测试A，A依赖于B，构造B的输入十分麻烦。 3）依赖的对象为第三方接口，可能遇到网络通信异常、逻辑分支处理过多、协调配合测试麻烦等问题。 4）依赖的对象为数据库，需要数据隔离（也可以使用H2内存数据库测试，或者springbootTest的rollback功能）\nmock的方法 手写mock：编写mockDao继承原有的Dao类，并覆写方法代替实际的数据库操作（见代码demo-01）。手写mock的前提是Dao类通过Service类的构造函数传参加载的类属性，而非利用注解@Autowired注入，也不能是Service类中某个方法实例化的局部变量，因此手写mock的代码侵入性高、局限性大 mock框架使用：jmock/easymock/mockito，而这些框架不能mock带有static、final、private关键字的方法。（注：mocktio于3.4.0版本后支持静态方法的mock） 2. PowerMock的强大之处 能在编译时修改字节码（.class文件）来完成测试。\n能mock局部变量的类和方法 能mock类中的静态方法 能mock带有final关键字的类和方法 注：以下是发挥powermock强大之处的两个关键注解\n@RunWith(PowerMockRunner.class) @PrepareForTest( { YourClassWithTestMethod.class }) ","三-使用powermockrunner进行单元测试#三、 使用PowerMockRunner进行单元测试":"回顾PowerMock的强大之处一节，经过第二大节的内容，想必对@RunWith(PowerMockRunner.class)和@PrepareForTest两个注解有了更深的认识。\nQ：没有这两个注解会怎么样？写错了会怎么样？ A：将无法使用PowerMock的强大能力。并且经过笔者测试，仅引入这两个注解，完全使用Mockito的语法，也能够引入对static、final、private的mock，但这里不做过多延伸，感兴趣的人可以自行测试。","二如何使用powermock#二、如何使用PowerMock？":"1. PowerMock简介 前置知识\nmock理念 单元测试理念及junit框架 PowerMock并非从头编写的mock测试框架，而是基于其他测试框架API编写的。目前兼容\nmock框架：powermock-easymock-api/powermock-mockito-api 测试框架：``powermock-junit-module/powermock-testng-module` 本文着重于**powermock-mockito-api**和**powermock-junit-module**\n2. 快速开始 （见代码demo-02）\npublic class UserServiceImplTest { /** * demo-02：基于powermock的单元测试 */ @Test public void testQueryUserCount() { // mock类 UserDao userDao = PowerMockito.mock(UserDaoImpl.class); // 模拟方法 PowerMockito.doReturn(10).when(userDao).getUserCount(); UserService userService = new UserServiceImpl(); // 注入到UserService Whitebox.setInternalState(userService, \"userDao\", userDao); // 调用模拟方法 int result = userService.queryUserCount(); // 验证 Assert.assertEquals(10, result); } } 3. 对类进行Mock的语法 1）对类或接口进行Mock 通过构造函数传参加载 （见代码demo-03）\n// mock类 UserDao userDao = PowerMockito.mock(UserDaoImpl.class); // 通过构造函数传参 UserService userService = new UserServiceWithConstructorImpl(userDao); 注入 （见代码demo-02）\n// mock类 UserDao userDao = PowerMockito.mock(UserDaoImpl.class); UserService userService = new UserServiceImpl(); // 注入到UserService Whitebox.setInternalState(userService, \"userDao\", userDao); 补充Mockito提供的注解（见代码demo-19）：\n// 需被mock的类 @Mock private UserDaoImpl userDaoImpl; // 需要注入mock类的类 @InjectMocks private UserService userService = new UserServiceImpl(); // 将@Mock注解的类注入到@InjectMocks注解的类 MockitoAnnotations.initMocks(this); 相比于Powermock，Mockito的注解写出的代码更好看\nQ：既想使用Mockito的注解，又想使用Powermock的mock静态、final，怎么办？\nA：Powermock本身兼容Mockito的API，可以使用其语法。为了能mock静态、final，一定要用到@RunWith(PowerMockRunner)和@PrepareForTest\n局部变量new关键字 （见代码demo-03）\n// mock类 UserDaoImpl userDaoImpl = PowerMockito.mock(UserDaoImpl.class); // 当new一个类时，引入该类 PowerMockito.whenNew(UserDaoImpl.class).withNoArguments().thenReturn(userDaoImpl); 需引入\n// **必须** @RunWith(PowerMockRunner.class) // 准备需修改字节码的class：引入局部变量类的类 @PrepareForTest(UserServiceWithLocalVarImpl.class) 2）对final关键字的类进行Mock （见代码demo-05） 语法同1）对类或接口进行Mock，并且需要引入\n// **必须** @RunWith(PowerMockRunner.class) // 准备需修改字节码的class：引入final类的类和final类 @PrepareForTest({UserServiceWithFinalImpl.class, UserDaoWithFinalImpl.class}) 3）对\"静态类\"进行Mock （见代码demo-06）\nPowerMockito.mockStatic(CommonService.class); // 该类中调用了CommonService提供的静态方法 UserService userService = new UserServiceImpl(); 并且需要引入\n// **必须** @RunWith(PowerMockRunner.class) // 准备需修改字节码的class：静态类本身 @PrepareForTest(CommonService.class) Q：一个类中既有其他类提供的静态方法又有该类提供的普通方法时该如何Mock？\nA：既需要mock普通方法又需要mock静态方法（见代码demo-07）\nQ：如果“静态类”中有需要初始化的私有属性怎么办？\nA：使用@SuppressStaticInitializationFor(静态类类名位置)的注解（见代码demo-20）\n4）通过spy来“模拟”真实类 mock是什么？spy是什么？ 首先，这里的mock指具体的PowerMockito.mock(),spy是指具体的PowerMockito.spy()。前者会生成mock类，其中的方法都不会是真实的方法，返回值都是null或空collection；后者是监视一个真实对象，如果没有模拟方法，则会调用真实的方法。 spy类的语法 （见代码demo-08、demo-09）\n// spy一个类 UserDao userDao = PowerMockito.spy(new UserDaoImpl()); UserService userService = new UserServiceImpl(); // 注入（也可以是局部变量whenNew和构造函数传参） Whitebox.setInternalState(userService, \"userDao\", userDao); Q：对一个接口进行spy会怎么样？ A：从spy类的语法可以看出，spy是模拟一个真实的对象。因此对一个接口进行spy必须先实现该接口中的所有方法。\n4. 模拟方法操作的语法 1）do…when…风格(推荐) 有返回值 PowerMockito.doReturn(returnObject).when(mockedClass).toMockMethod(argsIfExists); 无返回值 PowerMockito.doNothing().when(mockedClass).toMockMethod(argsIfExists); static、final、private等关键字下语法 1 static（见代码demo-06）、private（见代码demo-17）\n// 注意区别 PowerMockito.doReturn(returnObject).when(MockedStaticClass.class,\"toMockMethodName\",\"argsIfExists\"); 2 final（见代码demo-05） 与普通风格一致\n其他 1 抛出异常：（见代码demo-10） PowerMockito.doThrow() 2 执行特定应答： PowerMockito.doAnswer() 3 执行真实方法：（不推荐，请使用spy来做） PowerMockito.doCallRealMethod()\n2）when…then…风格 有返回值 PowerMockito.when(mockedClass.toMockMethod(argsIfExists)).thenReturn(returnObject); 无返回值 无，不支持\nstatic、final、private等关键字下语法 1 static、private\nPowerMockito.when(MockedStaticClass.class,\"toMockMethodName\",\"argsIfExists\").thenReturn(returnObject); 2 final 与普通风格一致\n其他 1 抛出异常：（见代码demo-10） PowerMockito.when().thenThrow() 2 执行特定应答： PowerMockito.when().thenAnswer() 3 执行真实方法：（不推荐，请使用spy来做） PowerMockito.when().thenCallRealMethod()\nQ：这两种语法风格有什么区别吗？ A：一般而言，除了when...then...缺少doNothing()，没有其他区别。但是，使用spy时，when...then...会调用具体的逻辑。（见代码demo-11）\n5. 对结果进行验证的语法 1）有返回值 直接用org.junit.Assert进行断言，例如：\nAssert.assertEquals(excepted, actual); Assert.assertNotEquals(unexcepted, actual); Assert.assertNull(object); Assert.assertNotNull(object); Assert.assertTrue(condition); Assert.assertFalse(condition); Assert.fail(msg); 2）无返回值 普通 （见代码demo-12）\n// Mockito.times(1)代表调用1次，不传此参数默认为1次 // Mockito.never()可以表示从未调用 Mockito.verify(mockedClass, Mockito.times(1)).mockedMehthod(Mockito.any(Arg.class)); 静态 （见代码demo-13）\n// 调用模拟方法 userService.saveUserWithStaticMethod(userDO); // 告诉PowerMockito.verifyStatic需要验证几次 PowerMockito.verifyStatic(CommonService.class, Mockito.times(1)); // 告诉PowerMockito.verifyStatic需要验证的调用逻辑 userService.saveUserWithStaticMethod(Mockito.any(UserDO.class)); 私有 （见代码demo-17）\n// 验证私有方法 PowerMockito.verifyPrivate(userService,Mockito.times(1)).invoke(\"isOk\"); 3）调用顺序验证 （见代码demo-18）\nInOrder inOrder = Mockito.inOrder(userDao); inOrder.verify(userDao).getUserCount(); inOrder.verify(userDao).insertUser(userDO); inOrder.verify(userDao).queryUserPhoneNumber(userDO); 4）异常验证 在进行单元测试时，如果预期结果是抛出异常，则可以使用 JUnit 的 @Test 注解的 expected 属性来声明期望的异常类型。 下面是一个使用 @Test 注解的 expected 属性进行单元测试的简单例子：\nimport org.junit.Test; public class MyTest { @Test(expected = ArithmeticException.class) public void testDivisionByZero() { int a = 1 / 0; } } 在这个例子中，我们使用 @Test 注解的 expected 属性声明了期望的异常类型为 ArithmeticException。如果在运行 testDivisionByZero() 方法时，抛出了 ArithmeticException 异常，则该测试用例就会通过；如果没有抛出 ArithmeticException 异常，则该测试用例就会失败。 另外，还可以使用 try-catch 语句来测试是否抛出了期望的异常。下面是一个使用 try-catch 语句进行单元测试的简单例子：\nimport org.junit.Test; public class MyTest { @Test public void testDivisionByZero() { try { int a = 1 / 0; fail(\"Expected ArithmeticException\"); } catch (ArithmeticException e) { // 异常处理 } } } 在这个例子中，我们使用 try-catch 语句来测试是否抛出了 ArithmeticException 异常。如果没有抛出异常，则调用 fail() 方法，使测试用例失败。如果抛出了 ArithmeticException 异常，则该测试用例就会通过。","五附录#五、附录":"1.pom.xml引入依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.powermock\u003c/groupId\u003e \u003cartifactId\u003epowermock-module-junit4\u003c/artifactId\u003e \u003cversion\u003e2.0.9\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.powermock\u003c/groupId\u003e \u003cartifactId\u003epowermock-api-mockito2\u003c/artifactId\u003e \u003cversion\u003e2.0.9\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ejunit\u003c/groupId\u003e \u003cartifactId\u003ejunit\u003c/artifactId\u003e \u003cversion\u003e4.13\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e ","前言#前言":"前言本文代码于GitHub - felixlyd/powermock-sample: 基于powermock进行单元测试demo","四其他api介绍#四、其他API介绍":"1）PowerMockito.WhenNew()：参数构造器 无参构造 PowerMockito.whenNew(ToMockClass.class).withNoArguments()\n有参构造 PowerMockito.whenNew(ToMockClass.class).withArguments()\n2）参数匹配 精确匹配 Mockito.eq()\n模糊匹配（不推荐） Mockito.startsWith() Mockito.endsWith()\n任意匹配 Mockito.any(Class.class) Mockito.anyInt() ......\n自定义匹配 Mockito.argThat(i-\u003ei.equals(1)) （见代码demo-14）\n3）自定义应答：实现Mocktio.Answer接口 根据参数自定义应答（见代码demo-15） PowerMockito.doAnswer(new Answer\u003cString\u003e() { @Override public String answer(InvocationOnMock invocationOnMock) throws Throwable { UserDO arg = invocationOnMock.getArgument(0); if(arg.getName().equals(userNameA)){ return phoneNumberA; } else if (arg.getName().equals(userNameB)) { return phoneNumberB; }else { throw new RuntimeException(\"未知参数\"); } } }) 根据参数获取返回值 PowerMockito.doAnswer(new Answer\u003cString\u003e() { @Override public String answer(InvocationOnMock invocationOnMock) throws Throwable { int arg = invocationOnMock.getArgument(0); return arg+1; } }) 保存中间变量并取出（见代码demo-16） public class MyAnswer implements Answer\u003cString\u003e { private final UserDO userDO = new UserDO(); @Override public String answer(InvocationOnMock invocationOnMock) throws Throwable { UserDO arg = invocationOnMock.getArgument(0); userDO.setName(arg.getName()); if(arg.getName().equals(\"zhangSan\")){ return \"123456\"; } else { throw new RuntimeException(\"未知参数\"); } } public UserDO getUserDO() { return userDO; } } 4）SpringRunner冲突：引入PowerMockRule（不推荐） 避免同时使用两者，提高了单元测试编写的复杂性和难度"},"title":"用PowerMock进行单元测试"},"/lyd-notes/6.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/jmeter%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%9A%90%E6%99%A6%E7%9A%84%E5%9D%91/":{"data":{"jmeter本身的坑#Jmeter本身的坑":"Jmeter本身的坑 4.0版本会把斜杠/自动转成反斜杠\\导致在linux下无法执行；无法对大体积的jtl进行绘图 5.0版本有时画图会报错 5.3版本不兼容5.0版本 kill掉jmeter进程后，还会有ApacheJmeter.jar的进程，要一并kill掉 ","加密报文的测试-预置处理器#加密报文的测试-预置处理器":"jmeter加密解密（加密篇）","忽略http-status响应码#忽略Http Status响应码":"How to Use JMeter Assertions in Three Easy Steps | Blazemeter by Perforce","性能测试动态报文长度处理方法#性能测试：动态报文长度处理方法":"性能测试：动态报文长度处理方法\nliuqiang.zip","性能测试常见命令#性能测试常见命令":"# 跑命令 jmeter -n -t myTest.jmx -l myReport.jtl # 跑命令带画图 jmeter -n -t myTest.jmx -l myReport.jtl -e -o myHtmlDir # 跑命令带参数 # 提前写好${__P(thread,)}等 jmeter -n -t myTest.jmx -Jthread=10 -l myReport.jtl # 根据jtl出图 jmeter -g myReport.jtl -o myHtmlDir -- 重建索引 alter index myIndex reuild; -- 表分析 analyze table myTable compute statistics; ","性能测试报告#性能测试报告":" 性能测试有 基准测试 1u-60s 单负载测试 10u-300s 容量测试， 反复尝试不同的并发执行60s，找出性能最优点 一般而言，读和写的接口分开来测\n故障测试，对于负载均衡的测试，关掉其中一台机器，不影响正常功能 稳定性测试，读+写接口的混合测试12小时，读的tps和写的tps均模拟生产要求，并发数为容量测试的最佳性能点 报告上 注意95line、99line等指标 故障测试主要是通过TPS的图来体现 稳定性测试主要通过平均响应时间的图来体现 最后附件可以附上原始的html报告截图 ","性能测试要注意的点#性能测试要注意的点":" 稳定性测试要做日志的定时清理 大量数据的插入和删除后要做表分析，重建索引等操作 涉及到对接第三方外呼的测试时，要做挡板 性能测试结束后要收尾（取消日志定时和取消挡板） "},"title":"Jmeter性能测试和隐晦的坑"},"/lyd-notes/6.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/jmx%E5%92%8Carthas%E8%BF%9B%E8%A1%8C%E7%9B%91%E6%8E%A7/":{"data":{"arthas#arthas":"arthas阿里Java诊断工具 arthas - 排查线上环境内存使用过大、线程cpu使用率高问题_arthas排查内存高问题_小毕超的博客-CSDN博客\n快速入门 | Arthas\nJVM调优工具锦囊：JDK自带工具与Arthas线上分析工具对比","jmx#jmx":"","profiler#profiler":"坑1 使用arthas监控docker中java应用时，使用profiler命令绘制监控图，报错AsyncProfiler error： No access to perf events. Try --fdtransfer or --all-user option or 'sysctl kernel.perf_event_paranoid=1' 查原因是BUG\n基于alpine的docker镜像使用arthas火焰图遇到的问题解决\nAsyncProfiler error: No access to perf events. Try –fdtransfer or –all-user option or ’sysctl kern_QMCoder的博客-CSDN博客\ncommunity.docker.docker_container module – manage Docker containers — Ansible Documentation\n解决办法： docker-compose.yml\ncap_add: - SYS_ADMIN ansible-playbook.yml\ncapabilities: - SYS_ADMIN 坑2 profiler如果输出直接可视化的html文件，一次只能监控一个事件，要么cpu要么alloc，如果要同时监控多项，cpu和alloc，那么需要生成jfr文件，云桌面中的jmc.exe报错提示需升级jmc版本。\n坑3 jfr命令无法使用，怀疑 Docker | Arthas"},"title":"jmx和arthas进行监控"},"/lyd-notes/6.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/springbatch%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95notes/":{"data":{"springbatchtest-注解#@SpringBatchTest 注解":"","springbatch整合mybatis-plus单元测试并回滚数据#Springbatch整合Mybatis-plus单元测试并回滚数据":"@SpringBatchTest 注解这个注解会注册JobLauncherTestUtils和JobRepositoryTestUtils到上下文context中 因此，有两种方式 方式1：SpringBatchTest\n@SpringBatchTest @RunWith(SpringRunner.class) @ContextConfiguration(classes:{otherClass.class}) public class SkipSampleFunctionalTests { ... } 方式2：SpringJUnit4ClassRunner\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes:{JobLauncherTestUtils.class, JobRepositoryTestUtils.class, otherClass.class}) public class SkipSampleFunctionalTests { ... } Springbatch整合Mybatis-plus单元测试并回滚数据","springbootspringbatchmybatis-plus整合进行单元测试#springboot+springbatch+mybatis-plus整合进行单元测试":"先抛出问题，同时使用@EnableBatchProcessing和@MybatisPlusTest会有事务冲突。 解决办法，很简单，但从两个很方便的注解直接入手查资料，着实很废时间，这里不卖关子，直接给出答案。\n@Bean public MapJobRepositoryFactoryBean mapJobRepositoryFactoryBean(@Qualifier(\"transactionManager\") PlatformTransactionManager transactionManager) throws Exception { MapJobRepositoryFactoryBean factoryBean = new MapJobRepositoryFactoryBean(); factoryBean.setTransactionManager(transactionManager); factoryBean.setValidateTransactionState(false); // 在这里 factoryBean.afterPropertiesSet(); return factoryBean; } 这一行，就行了。setValidateTransactionState(false)的作用是禁用在使用MapJobRepositoryFactoryBean时对事务状态的验证。这意味着在使用该工厂创建的 JobRepository 时，不会检查当前事务状态是否为活动状态。之后，是否使用@EnableBatchProcessing和@MybatisPlusTest注解，都无所谓了。这样也可以发现，可以拿到很多个版本的单元测试代码。","springboot整合mybatis-plus单元测试#springboot整合mybatis-plus单元测试":"如果是在springboot对mybatis-plus进行单元测试，那么需要在测试类上进行以下设置：\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = {someClasses.class}) @AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.AUTO_CONFIGURED) @MybatisPlusTest @MapperScan(basePackages = {\"com.xxx.xxx.dao\"}) public class xxTest{ @Test public void testXXX(){ } @TestConfiguration public static class MyConfiguration{ @Bean(\"dataSource\") @ConfigurationProperties(prefix = \"data-source\") public DataSource dataSource(){ return new DruidDataSource(); } @Bean public SqlSessionFactory sqlSessionFactory(@Qualifier(\"dataSource\") DataSource dataSource) throws Exception{ MybatisSqlSessionFactoryBean bean = new MybatisSqlSessionFactoryBean(); bean.setDataSource(dataSource); ResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); String mapperLocation = \"classpath:mapper/**/*Mapper*.xml\"; bean.setMapperLocations(resolver.getResources(mapperLocation)); return bean.getObject(); } } } 其中，@MybatisPlusTest注解中配置了很多注解，基本上可以一键完成springboot+mybatis的单元测试，并且可以使用数据库数据，并完成数据回滚。\n如果去掉@MybatisPlusTest，需要自行添加@Transactional注解来控制事务，使数据回滚。而且，@ConfigurationProperties注解会失效：当使用.properties配置文件时，需要在配置类上添加@TestPropertySource(location={\"classpath:xxx.properties\"})的注解和@EnableConfigurationProperties的注解；当使用.yml配置文件时，需要在测试类上添加@ContextConfiguration(initializers = {ConfigFileApplicationContextInitializer.class})的内容。\n在springboot 2.6版本中，ConfigFileApplicationContextInitializer更新为ConfigDataApplicationContextInitializer\n总而言之，@MybatisPlusTest进行了很多操作，便于整合到springboot的单元测试之中。\n@MybatisPlusTest中有@AutoConfigureTestDatabase注解，但该注解默认值为AutoConfigureTestDatabase.Replace.ANY，在我的测试中，AUTO_CONFIGURED才起作用。","springboot整合springbatch单元测试#springboot整合springbatch单元测试":"如果是在springboot对springbatch进行单元测试，那么需要在测试类上进行以下设置：\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes:{JobLauncherTestUtils.class, JobRepositoryTestUtils.class, otherClass.class}) @EnableBatchProcessing @AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.AUTO_CONFIGURED) public class SkipSampleFunctionalTests { @Test public void testXXX(){ } @TestConfiguration public static class MyConfiguration{ @Bean(\"dataSource\") @ConfigurationProperties(prefix = \"data-source\") public DataSource dataSource(){ return new DruidDataSource(); } } } 其中，@EnableBatchProcessing注解完成了很多事，包括事务管理transactionManager、jobRepository、jobBuilderFactory、stepBuilderFactory、jobLauncher、jobExplorer等等。它是依靠注入了SimpleBatchConfiguration完成的。\n如果去掉@EnableBatchProcessing注解，那么需要手动配置。如下所示：\n@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes:{JobLauncherTestUtils.class, JobRepositoryTestUtils.class, otherClass.class}) @AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.AUTO_CONFIGURED) public class SkipSampleFunctionalTests { @Test public void testXXX(){ } @TestConfiguration public static class MyConfiguration{ @Bean(\"dataSource\") @ConfigurationProperties(prefix = \"data-source\") public DataSource dataSource(){ return new DruidDataSource(); } @Bean(\"transactionManager\") public PlatformTransactionManager transactionManager(@Qualifier(\"dataSource\") DataSource dataSource) { return new DataSourceTransactionManager(dataSource); } @Bean public MapJobRepositoryFactoryBean mapJobRepositoryFactoryBean(@Qualifier(\"transactionManager\") PlatformTransactionManager transactionManager) throws Exception { MapJobRepositoryFactoryBean factoryBean = new MapJobRepositoryFactoryBean(); factoryBean.setTransactionManager(transactionManager); factoryBean.afterPropertiesSet(); return factoryBean; } @Bean(\"jobRepository\") public JobRepository jobRepository(@Qualifier(\"mapJobRepositoryFactoryBean\") MapJobRepositoryFactoryBean factoryBean) throws Exception{ return factoryBean.getObject(); } @Bean public JobBuilderFactory jobBuilderFactory(@Qualifier(\"jobRepository\") JobRepository jobRepository){ return new JobBuilderFactory(jobRepository); } @Bean public StepBuilderFactory stepBuilderFactory(@Qualifier(\"transactionManager\") PlatformTransactionManager transactionManager, @Qualifier(\"jobRepository\") JobRepository jobRepository){ return new StepBuilderFactory(jobRepository, transactionManager); } @Bean public JobLauncher jobLauncher(@Qualifier(\"jobRepository\") JobRepository jobRepository){ SimpleJobLauncher launcher = new SimpleJobLauncher(); launcher.setJobRepository(jobRepository); return launcher; } @Bean public JobExplorer jobExplorer(@Qualifier(\"mapJobRepositoryFactoryBean\") MapJobRepositoryFactoryBean factoryBean){ return new SimpleJobExplorer(factoryBean.getJobInstanceDao(), factoryBean.getJobExecutionDao(), factoryBean.getStepExecutionDao(), factoryBean.getExecutionContextDao()); } } } 并且，即使加上了@EnableBatchProcessing注解，也可以通过重写batchConfigurer自定义加载上述bean，如下所示：\n/* 因为@EnableBatchProcessing引入了SimpleBatchConfiguration， 而SimpleBatchConfiguration中配置了名为transactionManager的事务管理器， 因此要将上文中定义的transactionManager重命名 */ @Bean(\"myTransactionManager\") public PlatformTransactionManager myTransactionManager(@Qualifier(\"dataSource\") DataSource dataSource) { return new DataSourceTransactionManager(dataSource); } @Bean public BatchConfigurer batchConfigurer(MapJobRepositoryFactoryBean mapJobRepositoryFactory, PlatformTransactionManager myTransactionManager, JobLauncher jobLauncher, JobExplorer jobExplorer) { return new BatchConfigurer() { @Override public JobRepository getJobRepository() throws Exception { return mapJobRepositoryFactory.getObject(); } @Override public PlatformTransactionManager getTransactionManager() throws Exception { return transactionManager; } @Override public JobLauncher getJobLauncher() throws Exception { return jobLauncher; } @Override public JobExplorer getJobExplorer() throws Exception { return jobExplorer; } }; } 上述代码连接了数据库，但实际中并不推荐。因为单用springbatch的注解进行单元测试，不能数据回滚，在网上并没有找到相关的办法，因此推荐连接内存数据库使用。如果批处理任务是读文件写入数据库或者从数据库写出到文件，那么这种单元测试足够了。但是，如果批处理任务涉及多张数据表查询操作，那么构造数据可能会是一个大问题。"},"title":"Springbatch单元测试Notes"},"/lyd-notes/7.%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E8%BF%87%E7%A8%8B/":{"data":{"如何写软件文档#如何写软件文档":"软件工程详细设计文档模板\n系统架构图怎么画？ - 知乎\n常用的软件需求说明书模板","管理绩效#管理、绩效":"管理、绩效大厂的绩效考核，也治不了“造轮子”？\n美团程序员绩效被打C了！！！"},"title":"软件工程过程"},"/lyd-notes/7.%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E8%BF%87%E7%A8%8B/%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91%E8%AF%95%E7%82%B9%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D/":{"data":{"流程#流程":"特点、分工 跨团队、跨职能：分工灰色地带相互交错负责，以项目为目标推进 轻文档、重沟通：需求和设计详细描述，减少不必要的文档输出 自管理、自组织：管理和决策由整个团队讨论的方式进行 流程","特点分工#特点、分工":"","迭代#迭代：":"按周期迭代开发（2周-3周）\n每日站会-\u003e每日任务-\u003e灰度交付-\u003e继续迭代","需求#需求：":"需求讨论-\u003e用户故事-\u003e需求点拆解-\u003e任务拆解"},"title":"敏捷开发试点项目介绍"},"/lyd-notes/8.%E6%95%B0%E6%8D%AE%E5%BA%93/":{"data":{"mysql#Mysql":"面试官问：请介绍一下MySQL数据库的锁机制？ - 腾讯云开发者社区-腾讯云\n深入理解MySQL数据库各种锁（总结）\nSelect for update使用详解_51CTO博客_select for update\n详述 MySQL 中的行级锁、表级锁和页级锁_CG国斌的博客-CSDN博客_mysql 表级锁\n完美解决 ERROR 1064 (42000): You have an error in your SQL……\nwindows连接WSL-ubuntu里安装的MySQL，附安装教程_早睡的叶子的博客-CSDN博客_连接wsl nysq\nLinux中MySQL5.7设置utf8编码格式步骤 - 大热 - 博客园\nubuntu linux下mysql开机自启动_hackerwin7的博客-CSDN博客_ubuntu设置mysql开机自启\nMySQL用户管理：添加用户、授权、删除用户 - 陈树义 - 博客园","oracle#Oracle":"OracleODPI-C Installation — ODPI-C v4.6.0\n【Oracle学习笔记】Oralce随机数与随机取样_51CTO博客_Oracle取随机数\n【史上最详解】Oracle数据库各种”锁” - 看完这篇就够了！ - 腾讯云开发者社区-腾讯云\nORACLE数据库的各种锁 - 文章详情\n从Java和Oracle数据库谈高并发与锁\nFOR UPDATE SKIP LOCKED - Oracle to MySQL Migration - SQLines Tools\n【DB笔试面试659】在Oracle中，SELECT … FOR UPDATE加的是什么锁？_小麦苗的技术博客_51CTO博客\nOracle 数据泵异常JOB状态DEFINING处理_DBhanG的博客-CSDN博客_expdp任务状态显示defining如何取消\nimpdp — tables、remap_table、remap_tablespace_吥輕誩放棄的博客-CSDN博客\nImpdp Fails With ORA-39126 ORA-31604 Iinvalid Transform NAME Parameter “MODIFY” For Object Type PROCACT_INSTANCE in function ADD_TRANSFORM (Doc ID 1596495.1) - 阿西吧li - 博客园\noracle 数据库分页中的order by 后数据重复问题-百度经验\nexpdp导出指定表_Oracle impdp/expdp基础_weixin_39542111的博客-CSDN博客","sql和数据库通用#SQL和数据库通用":"浅谈数据库共享锁与排它锁\n数据库行锁争用问题分析 - 墨天轮\n分库分表利器之Sharding Sphere（深度好文，看过的人都说好）\nSQL中exists和in的用法以及区别 - Hider1214 - 博客园\nSQL中exists和in的用法以及区别 - Hider1214 - 博客园\nOracle与MySQL的SQL语句区别_又菜又爱玩٩( ö̆ ) و的博客-CSDN博客_mysql oracle sql语句\n7种SQL走索引or不走索引的优化，你竟然还不了解？ - 掘金\n事务的4种隔离级别（Isolation Level）分别是什么？_ITPUB博客\n由for update引发的血案 - 腾讯云开发者社区-腾讯云"},"title":"数据库"},"/lyd-notes/8.%E6%95%B0%E6%8D%AE%E5%BA%93/oracle%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/":{"data":{"":"","update不提交也会锁表#update不提交也会锁表":"java代码或者数据库客户端连接数据库后，做update，不做commit，会锁表。等待第一个update在commit或者rollback后，后续的update才能操作。","大量delete后性能下降#大量DELETE后性能下降":"需要对表重新做表分析，\nANALYZE TABLE X.TRAN_SEQ_NO_LOG COMPUTE STATISTICS; ","导入导出dmp#导入导出dmp":"create directory dumpdir as '/data/backup';greate read,write on directory dumpdir to system; --expdp system/oracle directory=dumpdir dumpfile=1.dmp logfile=1.log schemas=zxyf SELECT * FROM DBA_DIRECTORIES dd ; -- sql语句 SELECT * FROM DBA_DIRECTORIES dd ; create directory EXPDT as '/home/oracle/dump';greate read,write on directory dumpdir to system; ## 在oracle服务器上执行 expdp \\'/ as sysdba\\' schemas=zxyf dumpfile=zxyf-2.dmp logfile=zxyf-2.log DIRECTORY=EXPDT; impdp \\'/ as sysdba\\' schemas=zxyf dumpfile=zxyf-2.dmp logfile=zxyf-2.log DIRECTORY=EXPDT; ","查sql日志#查sql日志":"SELECT current_scn FROM v$database; -- 147415125 SELECT * FROM X.CST_RGHT_ORDR_TBL AS OF scn 147415125; SELECT sql_id, SQL_TEXT, module, MODULE_HASH,FIRST_LOAD_TIME FROM v$sqlarea WHERE sql_text LIKE '%cst_rght_ordr_tbl%' AND (sql_text like '%delete%'OR SQL_text LIKE '%update%') AND FIRST_LOAD_TIME BETWEEN '2021-05-28/08:34:27' AND '2021-06-03/15:34:27' ORDER BY FIRST_LOAD_TIME; -- 2021-06-03 14:54:53 SELECT * FROM v$sqlarea WHERE sql_text LIKE '%cst_rght_ordr_tbl%' WHERE fi; SELECT * FROM v$sqlarea WHERE sql_text LIKE '%cst_rght_ordr_tbl%'; SELECT * FROM v$session WHERE sql_id = 'g40hh6trc63xu'; SELECT * FROM v$session; SELECT * FROM sys.\"V_$SESSION\" WHERE sql_id = 'g40hh6trc63xu'; SELECT * FROM DBA_HIST_SQLSTAT WHERE sql_id = 'g40hh6trc63xu'; SELECT * FROM DBA_HIST_SQLTEXT WHERE sql_id = 'g40hh6trc63xu'; SELECT * FROM DBA_HIST_SQL_PLAN WHERE sql_id = 'g40hh6trc63xu'; SELECT * FROM DBA_HIST_ACTIVE_SESS_HISTORY WHERE sql_id = 'g40hh6trc63xu'; -- 2021-06-03 14:54:53 SELECT sysdate FROM dual; SELECT t1.SQL_ID, t1.SQL_TEXT, t1.LAST_ACTIVE_TIME FROM v$sqlarea t1 WHERE t1.SQL_FULLTEXT LIKE '%cst_rght_ordr_tbl%' AND t1.PARSING_SCHEMA_NAME = 'ZXYF'; SELECT * FROM v$session t2 WHERE t2.username='X' AND t2.program = 'plsqldev.exe' ORDER BY t2.LOGON_TIME desc; SELECT UNIQUE t1.SQL_ID, t1.SQL_TEXT, t1.LAST_ACTIVE_TIME,t3.MACHINE, t3.PROGRAM, t2.OSUSER FROM v$sqlarea t1 INNER JOIN DBA_HIST_ACTIVE_SESS_HISTORY t3 ON t1.SQL_ID=t3.SQL_ID INNER JOIN v$session t2 ON t3.MACHINE = t2.MACHINE WHERE t1.SQL_TEXT LIKE '%cst_rght_ordr_tbl%'; SELECT * FROM DBA_HIST_ACTIVE_SESS_HISTORY; SELECT t1.SQL_ID, t1.SQL_TEXT, t1.LAST_ACTIVE_TIME, t1.module FROM v$sqlarea t1 WHERE t1.SQL_TEXT LIKE '%cst_rght_ordr_tbl%' AND t1.SQL_TEXT LIKE '%delete%'; SELECT * FROM v$session t2 WHERE t2.SQL_ID='dbk24dn3dhm74'; SELECT * FROM DBA_HIST_ACTIVE_SESS_HISTORY t3 ORDER BY SQL_EXEC_START desc; SELECT * FROM v$sqlarea t3 WHERE t3.SQL_TEXT LIKE '%dbk24dn3dhm74%'; SELECT * FROM v$sqlarea t1 WHERE t1.PARSING_SCHEMA_NAME='CRMP32' ORDER BY t1.LAST_ACTIVE_TIME ; ","查看和检查sql的运行情况运行效率#查看和检查sql的运行情况、运行效率":"SELECT * FROM table(dbms_xplan.display); explain plan FOR SELECT * FROM ---- ","给用户授权登录#给用户授权登录":"GRANT CREATE SESSION TO NFSE; ","表空间相关#表空间相关":"-- 查询表空间 SELECT * FROM DBA_TABLESPACES dt ; -- 查询文件 SELECT * FROM DBA_DATA_FILES ddf ; -- 创建表空间 CREATE TABLESPACE TBS_NFSE_FUSION DATAFILE '/u01/oracle/app/db/crmdb/nfse_fusion_data.dbf' SIZE 10240m; CREATE TABLESPACE TBS_IDX_NFSE_FUSION DATAFILE '/u01/oracle/app/db/crmdb/nfse_fusion_idx.dbf' SIZE 5120m; -- 给用户指定默认表空间 ALTER USER NFSE DEFAULT tablespace TBS_IDX_NFSE_FUSION; -- 给用户授权表空间 GRANT CREATE SESSION, CREATE TABLE,unlimited tablespace TO nfse; "},"title":"Oracle常见命令大全"},"/lyd-notes/9.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/":{"data":{"elasticsearch#ElasticSearch":"ElasticSearchSearch your data | Elasticsearch Guide [8.5] | Elastic\nSearch API | Elasticsearch Guide [8.5] | Elastic\nElasticSearch原理 - 神一样的存在 - 博客园\n《Elasticsearch中文文档》 | Elasticsearch 技术论坛\nElasticsearch：通过 Spring Boot 创建 REST APIs 来访问 Elasticsearch_Elastic 中国社区官方博客的博客-CSDN博客\nEnterprise：Web Crawler 基础 （一）_Elastic 中国社区官方博客的博客-CSDN博客\nEnterprise：Web Crawler 基础 （二）_Elastic 中国社区官方博客的博客-CSDN博客_elastic web crawler\nEnterprise：Elastic App Search - Web 爬虫器实践_Elastic 中国社区官方博客的博客-CSDN博客 Elastic 网络爬虫\n使用ElasticSearch构建一个完整的搜索引擎 · 叁公子的博客"},"title":"搜索引擎"},"/lyd-notes/java%E5%92%8Cjdk/":{"data":{"amazon3#Amazon3":"Spring Boot 集成 AmazonS3 存储服务教程_OkidoGreen的博客-CSDN博客_spring amazon s3\nJava类com.amazonaws.ClientConfiguration的实例源码 - 编程字典","id生成器#ID生成器":"Hikaricp和Druid对比_晚风暖的博客-CSDN博客_hikaricp和druid哪个好\nGitHub - didi/tinyid: ID Generator id生成器 分布式id生成系统，简单易用、高性能、高可用的id生成系统\n架构师必修：一口气说出 9种 分布式ID生成方式。 - 掘金\n分布式唯一 ID 解析.md","java-时间#Java 时间":"Java中各种时间类型相互转换 - 腾讯云开发者社区-腾讯云","java命名规范#Java命名规范":"一篇文章讲清楚VO，BO，PO，DO，DTO的区别_蜗牛笨跑~的博客-CSDN博客\nSNAPSHOT版和正式版_snpmyn的博客-CSDN博客\n【Java】包的命名规则 - Nemo\u0026 - 博客园\nJava包（package）的命名规范，java中package命名规则_kongjiea的博客-CSDN博客\n详细聊聊Javadoc注释规范_GarfieldEr007的博客-CSDN博客","java日志包#Java日志包":"Log4j2进阶使用(按大小时间备份日志)_木木与呆呆的博客-CSDN博客\nLog4j2进阶使用(更多高级特性)_木木与呆呆的博客-CSDN博客\nslf4j的包使用说明_ericjauy的博客-CSDN博客\nLog4J.xml配置详解_亘易的博客-CSDN博客_log4j.xml","java通用#Java通用":"Java程序员进阶之路x沉默王二\nJava | 使用 StopWatch 优雅打印执行耗时_唐 城的博客-CSDN博客_watch.prettyprint\n并发编程之submit和execute区别（七）\n线程池的execute方法和submit方法有什么区别？ - 腾讯云开发者社区-腾讯云\nExecutorService中submit()和execute()的区别 - yehx - 博客园\n不可不说的Java“锁”事\nJava中的final关键字【详解】_訾博ZiBo的博客-CSDN博客_final关键字\n@Builder @NoArgsConstructor 冲突_你赖东东不错嘛~的博客-CSDN博客\n注解 | 对线面试官\n反射和动态代理 | 对线面试官\njava泛型（泛型接口、泛型类、泛型方法） - JokerShi - 博客园\njava.lang.NoClassDefFoundError: Could not initialize class xxx 原因及解决方法_轻松囧的博客-CSDN博客_could not initialize\nJava8 线程池\njava 泛型详解-绝对是对泛型方法讲解最详细的，没有之一 - little fat - 博客园\n为什么有的程序员不推荐使用Lombok！ - 腾讯云开发者社区-腾讯云","maven#Maven":"彻底解决Library source does not match the bytecode_陌路_一步一步往上爬的博客-CSDN博客_library source does not match\n深入了解maven多版本依赖冲突处理机制_笔墨桑的博客-CSDN博客_maven 多版本依赖\nmaven dependency中scope=compile 和 provided区别_春天的早晨的博客-CSDN博客_compile scope\nmaven的scope值runtime是干嘛用的? - 知乎\n国内已经没有maven镜像站了吗？ - 知乎\nmaven-将依赖的 jar包一起打包到项目 jar 包中\n理解maven命令package、install、deploy的联系与区别_阿童木-atom的博客-CSDN博客_package install deploy\n大厂Java项目如何进行Maven多模块管理 - 掘金","mybatis#Mybatis":"聊聊Mybatis中sql语句不等于的表示_java_脚本之家\nibatis防止sql注入_liuxigiant的博客-CSDN博客\nMyBatis Oracle批量插入数据\nMyBatis插入大量数据效率对比：foreach、SqlSession、sql三种方式批量插入_tuonioooo的博客-CSDN博客_mybatis foreach 效率\nSpringboot + Mybatis-plus事务管理_51CTO博客_spring boot mybatis-plus\nmybatis中type-aliases-package的用法_不可挽回的世界的博客-CSDN博客_type-aliases-package","mybatis-plus分页参数#Mybatis-plus分页参数":"Mybatis-plus分页参数myBatis-plus分页查询参数 - 掘金\nmybatis-plus 分页最大 500条问题解决（自定义查询条数）_mybatisplus分页条数_筏镜的博客-CSDN博客","tomcat#Tomcat":"Smart Tomcat - IntelliJ IDEs Plugin | Marketplace\nSpringBoot2.0启动日志中出现The APR based Apache Tomcat Native library which allows optimal performance in_梦里梦不出梦里梦的梦的博客-CSDN博客_the apr based apache tomcat native library which a"},"title":"Java和JDK"},"/lyd-notes/java%E5%92%8Cjdk/java%E6%8A%80%E6%9C%AF%E6%A0%88%E9%80%89%E6%8B%A9%E5%88%9D%E7%BA%A7%E6%8C%87%E5%8D%97/":{"data":{"excel处理easyexcelapache-poi#Excel处理：easyExcel、Apache poi":"关于阿里easyExcel和poi的差异_easyexcel与poi优劣_博渊同学的博客-CSDN博客\n关于Easyexcel | Easy Excel\nGitHub - alibaba/easyexcel: 快速、简洁、解决大文件内存溢出的java处理Excel工具","json处理jackson#JSON处理：JackSon":"","基础工具类guavahutool#基础工具类：Guava、Hutool":"","安全spring-securitysa-token#安全：Spring Security、Sa-Token":"","微服务网关zuulspring-cloud-gateway#微服务网关：Zuul、Spring Cloud Gateway":"","接口文档swagger-uiknife4j#接口文档：Swagger-UI、Knife4j":"一文读懂Eureka，Feign，Ribbon，Hystrix，Zuul核心组件间的关系\n限流和熔断：Sentinel、Hystrix、Resillience4j参考： 书籍《spring cloud微服务架构 实战派》龙中华-电子工业出版社-P157\nSentinel Hystrix Resilience4j 隔离策略 信号量隔离(并发线程数限流) 线程池隔离/信号量隔离 信号量隔离 熔断降级策略 基于响应时间、异常比率、异常数 基于异常比率 基于异常比率、响应时间 实时统计实现 滑动窗口(LeapArray) 滑动窗口(基于RxJava) 环形缓冲器（Ring Bit Buffer） 动态规则配置 支持多种数据源 支持多种数据源 有限支持 扩展性 多个扩展点 插件的形式 接口的形式 基于注解的支持 支持 支持 支持 限流 基于QPS,支持基于调用关系的限流 有限的支持 rate limit 流量整形 支持预热模式、匀速器模式、预热排队模式 不支持 简单的Rate Limiter模式 系统自适应保护 支持 不支持 不支持 控制台 提供开箱即用的控制台，可配置规则、查看秒级监控、机器发现等 简单的监控查看 不提供控制台，可对接其他监控系统 微服务网关：Zuul、Spring Cloud Gateway参考： 微服务网关对比和选型：Zuul、Spring Cloud GetWay、Kong和Traefik - 掘金\nSpring Cloud Gateway VS Zuul 比较，怎么选择？ 书籍《spring cloud微服务架构 实战派》龙中华-电子工业出版社-P172\n组件 Zuul 1.x Spring Cloud Gateway 底层实现 Servlet 2.5 Spring 5+ API特点 阻塞式API、不支持websockets 非阻塞式API、支持websockets 请求方式 同步请求 异步请求 服务器 tomcat netty（不支持tomcat） WebSockets 它可以在用户的浏览器和服务器之间打开交互式通信会话。在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。\nspring cloud gateway实现了熔断和限流\nZuul 2.x 在底层上有了很大的改变，使用了异步无阻塞式的 API，性能改善明显，不过现在 Spring Cloud 也没集成 Zuul 2.x，也无计划整合Zuul2.x. Netflix旗下产品均进入漫长的停更期，知名产品有eureka、hystrix、zuul、ribbon. 在spring cloud 2021更新后，除了eureka仍在使用，其他产品均不再支持。\neureka从1.x于2022年12月升级到了2.x，但此次更新并非Netflix官方，而是由spring完成. eureka2.x最低springboot版本要求3.x\nspring cloud zuul 原理简介及使用\n基础工具类：Guava、HutoolJSON处理：JackSon安全：Spring Security、Sa-Token10 种保证接口数据安全的方案！\n日志：Logback、Log4j2接口文档：Swagger-UI、Knife4j","日志logbacklog4j2#日志：Logback、Log4j2":"","负载均衡ribbon#负载均衡：ribbon":" 序号 实现类 负载均衡策略 1 RoundRobinRule 按照线性轮询策略，即按照一定的顺序依次选取服务实例 2 RandomRule 随机选取一个服务实例 3 RetryRule 按照 RoundRobinRule（轮询）的策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试（重试时获取服务的策略还是 RoundRobinRule 中定义的策略），如果超过指定时间依然没获取到服务实例则返回 null 。 4 WeightedResponseTimeRule WeightedResponseTimeRule 是 RoundRobinRule 的一个子类，它对 RoundRobinRule 的功能进行了扩展。 根据平均响应时间，来计算所有服务实例的权重，响应时间越短的服务实例权重越高，被选中的概率越大。刚启动时，如果统计信息不足，则使用线性轮询策略，等信息足够时，再切换到 WeightedResponseTimeRule。 | | 5 | BestAvailableRule | 继承自 ClientConfigEnabledRoundRobinRule。先过滤点故障或失效的服务实例，然后再选择并发量最小的服务实例。 | | 6 | AvailabilityFilteringRule | 先过滤掉故障或失效的服务实例，然后再选择并发量较小的服务实例。 | | 7 | ZoneAvoidanceRule | 默认的负载均衡策略，综合判断服务所在区域（zone）的性能和服务（server）的可用性，来选择服务实例。在没有区域的环境下，该策略与轮询（RandomRule）策略类似。 |","限流和熔断sentinelhystrixresillience4j#限流和熔断：Sentinel、Hystrix、Resillience4j":""},"title":"Java技术栈选择初级指南"},"/lyd-notes/linux%E4%BD%BF%E7%94%A8/":{"data":{"fedora#Fedora":"fedora删除多余不用的内核_邓布利多的博客-CSDN博客","linux#Linux":"awk commands within python script\n关于 root 和 wheel：在 Linux 中创建管理员账号（root 权限）的正确方式\nLinux chmod权限详解_IChen.的博客-CSDN博客_chmod 权限\nlinux grep 查询多行的方法 - 北山秋叶 - 博客园\ntail -f 查看日志筛选并实时显示_不止鱼的博客-CSDN博客_tail 筛选\nlinux 环境变量设置（临时 + 永久）_yi412的博客-CSDN博客_linux 环境变量添加\nLinux 查看端口占用情况 - 菜鸟++ - 博客园\nLinux 定时任务crontab_014 - 邹龙彬 - 博客园","wsl-ubuntu#Wsl-Ubuntu":"Wsl-UbuntuWSL2 踩坑分享 – xiabee\n修改 WSL Ubuntu 18.04 默认源为阿里云镜像 - Windows - 大象笔记\nwindow下jetBrains Clion使用wsl安装配置 c++开发神器_2郎神的博客-CSDN博客_clion wsl 开发 每次都要下载 jetbrainsclient\nWSL2 安裝完 docker 後執行 sudo service docker restart`出現 docker: unrecognized service 錯誤 - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天\n使用 WSL 添加或连接数据库\nWSL-Ubuntu18.04 LTS重启方法_久远成传说的博客-CSDN博客_wsl 重启ubuntu\nhttps://www.v2ex.com/t/744955\n安装 WSL\nHow do you install multiple, separate instances of Ubuntu in WSL?\nWSL2 安装教程 CUDA 配置 自测已成功_Zhao-Jichao的博客-CSDN博客_wsl2cuda\n解决WSL2/Linux ll command not found_刘金宝_Arvin的博客-CSDN博客\n适用于 Linux 的 Windows 子系统文档\nUbuntu20.04软件源更换","修改主机名#修改主机名":"sudo hostname \u003c主机名\u003e"},"title":"Linux使用"},"/lyd-notes/linux%E4%BD%BF%E7%94%A8/%E6%9D%83%E9%99%90%E7%9B%B8%E5%85%B3/":{"data":{"添加指定id的用户和组#添加指定id的用户和组":"1、添加指定gid的组\ngroupadd -g 200 nexus 2、添加指定uid的用户，并加入到指定组\nuseradd -u 200 -g nexus nexus 修改用户 加入root组 usermod -G root nexus 修改密码 passwd nexus 修改uid和gid 接下来我们来修改它的UID和GID，修改UID命令：\nusermod -u222 tom 修改GID命令：\ngroupmod -g222 tom（groupmod -gGID 用户组名） 修改完后，我们再来查看有没有修改成功，查看用户ID命令：\nid tom ","添加用户到sudo组#添加用户到sudo组":"修改sudoers文件\nsudo vi /etc/sudoers 添加自己的用户名到root下面，按照root的格式照抄","用户和组#用户和组":"用户和组"},"title":"权限相关"},"/lyd-notes/linux%E4%BD%BF%E7%94%A8/%E7%AB%AF%E5%8F%A3%E7%9B%B8%E5%85%B3/":{"data":{"centos#centos":"firewalled","端口#端口":"iptables","防火墙#防火墙":"防火墙"},"title":"端口相关"},"/lyd-notes/python/":{"data":{"argparse#argparse":"Python-argparse库基本使用方法和add_argument() 参数详解_Foneone的博客-CSDN博客","conda#Conda":"Conda 环境迁移","importlib#importlib":"importlib — import 的实现 — Python 3.11.1 文档","loguru#loguru":"一起读源码：为什么 loguru 的时间 rotation 不能只精确到天 - 腾讯云开发者社区-腾讯云","pip#pip":"python pip安装包导入导出、下载包和离线安装_橘子怕上火的博客-程序员信息网 - 程序员信息网\nPython 修改 pip 源为国内源","python#Python":"PythonHow to read a (static) file from inside a Python package?\npython 错误代码和异常系统设计_DAo_1990的博客-CSDN博客\nPython单例模式(Singleton)的N种实现\nPython之错误码设计 - X-Wolf - 博客园\nPython开发篇——构建虚拟Python开发环境（Conda+Poetry）_DisonTangor的博客-CSDN博客\nExamples of using conda + poetry for cross-platform Python package portability\nPython 风格指南 - 内容目录 — Google 开源项目风格指南","python打包#python打包":"【Python打包】Python embed打包方法 亲测有效","requests#Requests":"Python之Requests模块使用详解\nPython Requests package: Handling xml response","sqlalchemy#sqlalchemy":"Query Oracle databases with Python and SQLAlchemy\nSQLAlchemy简明教程_stone0823的博客-CSDN博客_sqlalchemy\nSQLAlchemy 中的 Session、sessionmaker、scoped_session - 长安223 - 博客园\n深入研究sqlalchemy连接池 - 上官飞鸿 - 博客园","type#type":"Python 标准库 typing 类型注解标注 | Python 教程 - 盖若","参数检查#参数检查":"Pysa：Facebook最新开源Python静态分析工具\npython-函数参数类型检查_Leeeey的博客-CSDN博客\nPython静态类型解析工具简介和实践\nPython：函数参数类型检查","爬虫#爬虫":"GitHub - wistbean/learn_python3_spider: python爬虫教程系列、从0到1学习python爬虫，包括浏览器抓包，手机APP抓包，如 fiddler、mitmproxy，各种爬虫涉及的模块的使用，如：requests、beautifulSoup、selenium、appium、scrapy等，以及IP代理，验证码识别，Mysql，MongoDB数据库的python使用，多线程多进程爬虫的使用，css 爬虫加密逆向破解，JS爬虫逆向，分布式爬虫，爬虫项目实战实例等\n爬虫需要登陆怎么办？这份python登陆代码请收下 - charseki - 博客园","连接hive#连接hive":"python读取hive方案分析_大王go巡山的博客-CSDN博客_hive python\nHow to Access Hive via Python?"},"title":"Python"},"/lyd-notes/springboot%E5%92%8Cspringcloud/":{"data":{"#":"参数校验spring boot 优雅的参数校验 - 掘金\nSpring注解@NoNull、@NotEmpty、@NotBlank注解无效以及嵌套对象的无效解决办法-CSDN博客\nValidated、Valid 、Validator，他们的区别你知道几个-阿里云开发者社区\nSpringBoot自定义请求参数校验\nJava Validated 分组校验的使用 - 掘金\n文件上传下载SpringBoot实现文件下载的几种方式_springboot大文件下载_user2025的博客-CSDN博客\n七、springBoot 简单优雅是实现文件上传和下载 - 腾讯云开发者社区-腾讯云\nSpringBoot实现文件下载的几种方式_springboot大文件下载_user2025的博客-CSDN博客\nAsync异步注解为什么不建议直接使用 Async 注解？-腾讯云开发者社区-腾讯云\nExcel导入导出EasyExcel中无法使用Spring事务的解决办法 - 腾讯云开发者社区-腾讯云\nSpringBoot 实现 MySQL 百万级数据量导出并避免 OOM 的解决方案！_mysql百万数据导出_肥肥技术宅的博客-CSDN博客\nSpringBoot 实现 Excel 导入导出，百万数据量，性能爆表！_公众号-芋道源码的博客-CSDN博客\nEasyExcel 生成文件并上传OSS - 掘金\nEasyExcel导出保留2位小数_@contentstyle_lpch0825的博客-CSDN博客\neasyexcel 添加超链接_easyexcel hyperlink_o_瓜田李下_o的博客-CSDN博客\nspringboot与设计模式【项目实践】依赖注入用得好，设计模式轻松搞 - 掘金\nspring中的观察者模式 - wl_王麟 - 博客园\nSpring 中经典的 9 种设计模式，打死也要记住啊！\n【项目实践】SpringBoot三招组合拳，手把手教你打出优雅的后端接口 - 掘金\nSpring SecuritySpring Security 入门原理及实战 - 逃离沙漠 - 博客园\nSpring Security前后端分离配置以及自定义图片验证码和短信验证码登录功能 - 掘金\n【项目实践】在用安全框架前，我想先让你手撸一个登陆认证 - 掘金\n【项目实践】一文带你搞定页面权限、按钮权限以及数据权限 - 掘金\n【项目实践】一文带你搞定前后端分离下的认证和授权|Spring Security + JWT - 掘金\nXSS 和 CSRF 攻击详解 - 掘金\n前端安全系列（二）：如何防止CSRF攻击？\n一文教你搞定所有前端鉴权与后端鉴权方案，让你不再迷惘 - 掘金\nSpring WebFluxSpring WebFlux 教程\nReactive Spring实战 – WebFlux使用教程\nSpring Boot WebFlux 2.1.7 中文翻译文档 - JMCui - 博客园\nSpring事务传播Spring事务传播属性之REQUIRES_NEW用法 - 百度文库\nSpring中@component的使用_请叫我小艺啊啊啊的博客-CSDN博客_@component\nSpring-bootSpringboot整合MybatisPlus（超详细）完整教程~ - 易水寒的博客 - 博客园\n超详细讲解SpringBoot集成Druid_你的眼睛会下雨的博客-CSDN博客_springboot druid\nspringboot根据参数,调用不同的service接口(策略模式) - 八块腹肌的青书 - 博客园\nSpring-Boot 访问外部接口的几种方案总结_polo2044的博客-CSDN博客_spring调用外部链接\nFailed to configure a DataSource: ‘url’ attribute is not specified and no embedded datasource could be configured. Reason: Failed to determine a suitable driver class\nSpring Boot项目优雅的全局异常处理方式（全网最新） - 掘金\nOauth2授权模式访问之授权码模式(authorization_code)访问_iiaythi的博客-CSDN博客_/oauth/authorize\n@ResponseBody详解_originations的博客-CSDN博客_@responsebody\nSpringBoot 启动后立即自动停止问题分析_爱学习 爱分享的博客-CSDN博客\n@PostConstruct注解详解_Archie_java的博客-CSDN博客_@postconstruct\nspring-boot-configure @ConfigurationProperties 注解使用姿势，这一篇就够了_yusimiao的博客-CSDN博客_configureproperties\nSpringBoot中@ConfigurationProperties配置多层嵌套和复杂Map_qq_三哥啊的博客-CSDN博客_configurationproperties 多层级\nSpringBoot自定义Condition注解 - zeng1994 - 博客园","async异步注解#Async异步注解":"","excel导入导出#Excel导入导出":"","spring-batch#Spring-batch":"Spring Batch 中 Scheduler 定时任务\nSpring Batch 的原理和作用 - 编程宝库\n记一次Spring Batch完整入门实践\n分布式任务调度平台XXL-JOB\n关于@EnableBatchProcessing注解_打回原形__的博客-CSDN博客_@enablebatchprocessing\nSpringBatch源码解析之SpringBatch加载配置（一）_shaoyangdd的博客-CSDN博客_simplebatchconfiguration modularbatchconfiguration","spring-boot#Spring-boot":"","spring-cloud#Spring-Cloud":"Netty入门教程——认识Netty\nspringCloud配置中心（非Git版）_肥仔哥哥1930的博客-CSDN博客_springcloud config不使用git\nspringcloud_config教程（4）–更高级的配置加密与安全_a561066292的博客-CSDN博客\nEureka服务连不上注册中心 - OSCHINA - 中文开源技术交流社区\nSpringCloud 配置${spring.cloud.client.ipAddress}_真是适合睡觉的天气的博客-CSDN博客_client.ip-address\nEureka client instances deregistering and closing Java processes · Issue #2281 · spring-cloud/spring-cloud-netflix\nEureka - Can’t get a response - Can’t contact any eureka nodes\n推荐 7 个牛哄哄 Spring Cloud 实战项目\n服务间调用要通过网关吗_Spring Cloud微服务如何设计异常处理机制？_weixin_39775577的博客-CSDN博客","spring-cloud-alibaba#spring-cloud-alibaba":"Cloud Native App Initializer\nNACOS集群和单机模式启动 - 灰信网（软件开发博客聚合）\nSentinel限流规则-流控模式之链路模式 - Ruthless - 博客园\nNacos Spring Cloud 快速开始","spring-cloud-hystrix#spring-cloud-hystrix":"SpringCloud使用Hystrix的Turbine时，EurekaClient和NacosClient冲突问题_大后生大大大的博客-CSDN博客\ns not in the allowed list of proxy host names. If it should be allowed add it to hystrix.dashboard._G_whang的博客-CSDN博客\n解决Hystrix dashboard Turbine 一直 Loading…… 及其他坑 - CansonHai - 博客园\nSpringCloud在使用Feign框架集成的Hystrix时，controller中注入被【@FeignClient注解】标注的接口，与其fallback实现类注入冲突的问题_edironpr的博客-CSDN博客_feignclient 和 hystrix 注入冲突","spring-security#Spring Security":"","spring-webflux#Spring WebFlux":"","springboot与设计模式#springboot与设计模式":"","spring事务传播#Spring事务传播":"","参数校验#参数校验":"","文件上传下载#文件上传下载":""},"title":"SpringBoot和SpringCloud"},"/lyd-notes/springboot%E5%92%8Cspringcloud/interceptorfilteradviceaop/":{"data":{"aop#AOP":"过滤器 和 拦截器 6个区别，别再傻傻分不清了 过滤器(Filter)和拦截器(Interceptor)的区别-阿里云开发者社区 Filter中抛出异常@RestControllerAdvice注解无法捕获的问题 - 未月廿三 - 博客园\n记一次SpringBoot Filter的坑-过滤器被重复执行问题_springboot过滤器执行了多次_记忆旅途的博客-CSDN博客\n如何在过滤器中修改http请求体和响应体 - 掘金\nJava | Spring 使用 RequestBodyAdvice 来实现请求参数的加解密预处理_双鬼带单的博客-CSDN博客 springboot项目通过ResponseBodyAdvice增加响应头和修改响应内容_springboot设置响应头_程序猿（攻城狮）的博客-CSDN博客\nspringboot 实现拦截器的 3 种方式介绍及异步执行的思考-51CTO.COM\nHow to modify request body before reaching controller in spring boot\nspringboot 过滤器Filter vs 拦截器Interceptor vs 切片Aspect解析_乐乐Java路漫漫的博客-CSDN博客\nAOP在Spring AOP切面中启用新事务_aop切面添加事务_普通网友的博客-CSDN博客"},"title":"Interceptor、Filter、Advice、AOP"},"/lyd-notes/springboot%E5%92%8Cspringcloud/spring-cloud%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/":{"data":{"":" 使用Ribbon实现负载均衡，通过自定义Ribbon配置类来修改负载均衡的规则（默认有9种） 可以用RestTemplate客户端来调用“服务提供者”的接口，也可以用Feign客户端来调用“服务提供者”的接口，二者的注解不同。Feign的一些注解并不是SpringMVC的原生注解。 feign中使用hystrix会使得@autowired注解失效，因此要给FeignClient接口加上注解@Primary，给继承FeignClient的MyHystrix加上@Component 使用HystrixDashboard要加上配置hystrix.dashboard.proxy-stream-allow-list=localhost，才能连接进行访问；在需要监控的feign客户端开启Hystrix和HystrixDashboard进行监控 turbine默认依赖于eureka，要在pom.xml中排除依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-turbine\u003c/artifactId\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-eureka-client\u003c/artifactId\u003e \u003c/exclusion\u003e \u003cexclusion\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-eureka-server\u003c/artifactId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e 网关GateWay在微服务中极其重要，负责路由转发，过滤器等等。路由转发规则有默认的11种路由谓词工厂；路由转发能通过注册到服务中心实现转发；过滤器有20个网关过滤器和9个全局过滤器。路由转发指将请求地址按某种匹配规则（即谓词工厂）转发到服务提供地址；过滤器能对请求内容进行处理，以达到参数校验、服务监控、限流等功能 Gateway的实现是用Netty+WebFlux，因此使用时不要加入Web依赖 "},"title":"Spring Cloud学习笔记"},"/lyd-notes/vue/":{"data":{"js语法#JS语法":"var、let、const的区别\nlet、const、var的区别_前端编程之家的博客-CSDN博客_const let var的区别\nvar与let、const的区别","ngnix配置#ngnix配置":"Vue.js - 使用History模式路由时Nginx的配置方案（附样例）\nnginx中alias、root、try_files的用法_My_Bells的博客-CSDN博客_nginx alias try_files\nvue-router的base和vue.config.js的publicPath区别和联系_口袋の的天空的博客-CSDN博客_router base","npm#Npm":"Npmnpm 切换镜像站点 | 菜鸟教程","vscode-vue插件#VsCode Vue插件":"VSCODE Vue插件_讯达网络的博客-CSDN博客_vscode vue插件","vue#Vue":"Vue3学习与实战 · 全局挂载使用Axios\nvue-ts版生产环境屏蔽日志","前端开发#前端开发":"禁止将http请求强制转换为https请求_风口上的猪2015的博客-CSDN博客\nHtml引入本地Vue和Element UI_原生html引入本地vue_YO_RUI的博客-CSDN博客\nelement-ui表格编辑_element可编辑表格_Eggsy.Li的博客-CSDN博客"},"title":"Vue"},"/lyd-notes/vue/vue-tips/":{"data":{"":"","10绑定样式#10.绑定样式":" class样式 写法:class=“xxx”xxx可以是字符串、对象、数组。 字符串写法适用于:类名不确定，要动态获取。 数组写法适用于:要绑定多个样式，个数不确定，名宁也不确定。 对象写法适用于:要绑定多个样式，个数确定，名字也确定，但不确定用不用。 style样式 对象写法:style=\"{fontsize: xxx}“其中xxx是动态值。 数组写法:style=\"[a,b]“其中a、b是样式对象。样式对象中的key值必须是css中存在的样式关键字。 ","11条件渲染#11.条件渲染":" v-if 写法： v-if=“表达式” v-else-if=“表达式” v-else=“表达式” 适用于:切换频率较低的场景。 特点:不展示的DOM元素直接被移除。注意: v-if可以和v-else-if、v-else一起使用，但要求结构必须连续，不能被“打断” v-show 写法:v-show=“表达式” 适用于:切换频率较高的场景。 特点:不展示的DOM元素未被移除，仅仅是使用样式隐藏掉 备注: 使用v-if的时，元素可能无法获取到，而使用v-show一定可以获取到。 template不会改变页面结构，但只能配合v-if使用 ","12列表渲染#12.列表渲染":"key的基本原理 B站讲解：10:30开始\n面试题:react、vue中的key有什么作用?(key的内部原理)\n虚拟DOM中key的作用: key是虚拟DOM对象的标识，当状态中的数据发生变化时，Vue会根据**[新数据]生成[新的虚拟DOM]，随后Vue进行[新虚拟DOM]与[旧虚拟DOM]** 的差异比较。 对比规则: 旧虚拟DOM中找到了与新虚拟DOM相同的key: 若虚拟DOM中内容没变，直接使用之前的真实DOM 若虚拟DOM中内容变了，则生成新的真实DOM，随后替换掉页面中之前的真实DOM。 旧虚拟DOM中未找到与新虚拟DOM相同的key，则创建新的真实DOM，随后渲染到到页面。 用index作为key可能会引发的问题: 1.若对数据进行逆序添加、逆序删除等破坏顺序操作，会产生没有必要的真实DOM更新 ==\u003e 界面效果没问题，但效率低。 2.如果结构中还包含输入类的DOM，会产生错误DOM更新 ==\u003e 界面有问题 开发中如何选择key? 最好使用每条数据的唯一标识作为key，比如id、手机号、身份证号、学号等唯一值。 如果不存在对数据的逆序添加、逆序删除等破坏顺序操作，仅用于渲染列表用于展示使用index作为key是没有问题的。 Vue监视数据的原理 vue会监视data中所有层次的数据 如何监测对象中的数据? 通过setter实现监视，且要在new Vue时就传入要监测的数据。 对象中后追加的属性，Vue默认不做响应式处理 如需给后添加的属性做响应式，请使用如下API: Vue.set(target，propertyName/index，value) 或vm.$set(target,propertyName/index， value) 如何监测数组中的数据? 通过包裹数组更新元素的方法实现，本质就是做了两件事: 调用原生对应的方法对数组进行更新。 重新解析模板，进而更新页面。 在Vue修改数组中的某个元素一定要用如下方法: 使用这些API:push()、pop()、shift()、unshift()、splice()、sort()、reverse() Vue.set() 或 vm.$set() 特别注意: Vue.set() 和 vm.$set() 不能给vm 或 vm的根数据对象添加属性!! ","13收集表单数据#13.收集表单数据":" 若:\u003cinput type=\"text”/\u003e，则v-model收集的是value值，用户输入的就是value值。 若:\u003cinput type=\"radio\"/\u003e，则v-model收集的是value值，且要给标签配置value值. 若:\u003cinput type=\"checkbox\"/\u003e 没有配置input的value属性，那么收集的就是checked(勾选 or 未勾选，是布尔值) 配置input的value属性: v-model的初始值是非数组，那么收集的就是checked (勾选 or 未勾选，是布尔值) v-model的初始值是数组，那么收集的的就是value组成的数组 备注: v-model的三个修饰符: lazy:失去焦点再收集数据 number:输入字符串转为有效的数字 trim:输入首尾空格过滤 ","14vue内置指令#14.VUE内置指令":" v-bind：单向绑定解析表达式，可简写为 :xxx\nv-model：双向数据绑定\nv-for：遍历数组/对象/字符串\nv-on：绑定事件监听，可简写为@\nv-if：条件渲染（动态控制节点是否存存在）\nv-else：条件渲染（动态控制节点是否存存在）\nv-show：条件渲染（动态控制节点是否存展示）\nv-text指令：\n作用:向其所在的节点中渲染文本内容。 与插值语法的区别:v-text会替换掉节点中的内容，{{xx}}则不会。 v-html指令:\n作用:向指定节点中渲染包含html结构的内容。 与插值语法的区别: v-html会替换掉节点中所有的内容，{{xx}}则不会。 v-html可以识别html结构。 注意: v-html有安全性问题!! !! 在网站上动态渲染任意HTML是非常危险的，容易导致XSS攻击。 一定要在可信的内容上使用v-html，永不要用在用户提交的内容上! v-cloak指令(没有值) :\n本质是一个特殊尿性，Vue实例创建完毕并接管容器后，会删掉v-cloak屁性。 使用css配合v-cloak可以解决网速慢时页面展示出{{xxx}}的问题。 v-once指令:\nv-once所在节点在初次动态渲染后，就视为静态内容了。 以后数据的改变不会引起v-once所在结构的更新，可以用于优化性能。 v-pre指令:\n跳过其所在节点的编译过程。 可利用它跳过:没有使用指令语法、没有使用插值语法的节点，会加快编译。 ","1vue基础知识#1.vue基础知识":" 容器与Vue实例一一对应。真实生产开发中，只会有一个Vue实例。不同的网页模块用组件进行实现，最终这些组件组装成一个网页。 由vue管理的函数，一定不能写成箭头式函数，否则，该函数就不由vue管理了。 vue中data下的才做数据劫持和数据代理，而methods不会。methods配置的函数放在data下也可以正常运行，但会使得vue数据代理的内容过于臃肿，强烈建议不要这样做。 箭头函数没有自己的this，会往作用域的外部找this ","2模板语法#2.模板语法":" 使用指令语法，例如v-bind:var=\"xxx\"或者:var=\"xxx\"，vue会将引号中的内容当做js表达式进行执行。 插值语法用于解析标签体内容:{{}}。 指令语法用于解析标签。通常语法形式为v-xxxx。 函数在指令语法中，无参时可以加小括号或者不加；在插值语法中一定要加小括号，否则会打印函数体本身，而不是执行函数 ","3-数据绑定#3. 数据绑定":" v-model只能应用于表单元素上，即能够输入值的元素（具有value值），或和用户能交互的元素。 ","4-mvvm模型和vue#4. MVVM模型和VUE":" M是指模型Model，对应js中的objects，记录了数据data V是指视图View，对应页面中的dom元素，生成了页面视图 VM是指视图模型ViewModel，即VUE实例对象，完成了dom监听、数据代理、数据劫持、数据绑定等功能 ","5-数据代理#5. 数据代理":"Object.defineProperty方法添加的属性，默认不参与枚举（或遍历），无法被修改，无法被删除\n// 该方法添加的属性默认不参与枚举（或遍历），无法被修改，无法被删除 Object.defineProperty(person,'age',{ enumerable: true, // 控制属性是否参与枚举，默认false // writable: true, // 控制属性是否被修改， 默认false configurable: true, // 控制属性是否能被删除，默认false // getter在读取该属性（这里是age属性）时会被调用，且函数返回值为该属性（age）的值 // getter不能和writable同时使用 get(){ console.log(\"有人读取了age\") return number }, // setter在修改该属性（age）时会被调用，并执行代码逻辑 set(value){ console.log(\"有人修改了age\") number = value } }) 数据代理：通过一个对象代理对另一个对象中属性的操作（读和写）\nvue中的数据代理使得编写代码更简单方便，实现原理即为Object.defineProperty，vm中的数据存储在_data之中。并且，_data又进行了数据劫持，数据劫持的原因是修改了变量后，网页数据能立即随之改变，即响应式操作，因此需添加观察者（Observer）。","6-事件处理#6. 事件处理":"事件修饰符 嵌套事件时，事件捕获阶段是从外到内，事件冒泡阶段是从内到外。事件先进入捕获阶段再进入冒泡阶段，并且默认是在事件冒泡阶段开始处理事件。 事件修饰符passive，不是所有事件都适用，在移动端开发中会经常使用到 事件修饰符支持链式调用 键盘事件 vue中的按键别名 tab键只能配合keydown使用 系统修饰键 (用法特殊) : ctrl、alt、shift、meta 配合keyup使用: 按下修饰键的同时，再按下其他键，随后释放其他键，事件才被触发。 配合keydown使用: 正常触发事件。 系统修饰键+其他按键用法 ","7-计算属性#7. 计算属性":" 定义: 要用的属性不存在，要通过已有属性（vue管理的属性）计算得来。 原理: 底层借助了Object.defineproperty方法提供的getter和setter。 get函数什么时候执行? 初次读取时会执行一次。 当依赖的数据发生改变时会被再次调用。 优势: 与methods实现相比，内部有缓存机制(复用)，效率更高，调试方便 备注: 计算属性最终会出现在vm上，直接读取使用即可。 如果计算属性要被修改，那必须写set函数去响应修改，且set中要引起计算时依赖的数据发生改变 Tips： 计算属性中所用的任何vue中的数据即this.xxx的值发生改变时，计算属性都会重新去计算值 ","8监视属性#8.监视属性":" 当被监视的属性变化时，回调函数handler自动调用，进行相关操作 监视的属性必须存在，才能进行监视!! 监视的两种写法: new Vue时传入watch配置 通过vm.$watch监视 深度监视 Vue中的watch默认不监测对象内部值的改变(一层) 配置deep:true可以监测对象内部值改变(多层) 备注: Vue自身可以监测对象内部值的改变，但vue提供的watch默认不可以! 使用watch时根据数据的具体结构，决定是否采用深度监视。 ","9计算属性和监视属性比较#9.计算属性和监视属性比较":" computed能完成的功能，watch都可以完成。computed的代码实现更为简便 watch能完成的功能，computed不一定能完成，例如: watch可以进行异步操作。 两个重要的小原则:\n所被Vue管理的函数，最好写成普通函数，这样this的指向才是vm或组件实例对象。 所有不被Vue所管理的函数(定时器的回调函数、ajax的回调函数、Promise的回调函数等)，最好写成箭头函数,这样this的指向才是vm或组件实例对象。 "},"title":"Vue Tips"},"/lyd-notes/vue/vue.js%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B-notes/":{"data":{"await关键字#await关键字":"await 是一个 JavaScript 关键字，它只能在 async 函数中使用。 await 用于等待一个异步操作（通常是一个 Promise 对象）完成，并返回其结果。 所以说，await 是用于等待异步请求完成并获取结果的，而不是用于同步请求。 例如：\nasync function getData() { let response = await fetch('https://some-api.com/data'); let data = await response.json(); return data; } 这里的fetch是一个异步的请求，使用await来等待这个请求结束并获取响应数据，这样就可以让程序继续执行下面的代码而不是阻塞程序。 总结，await是异步请求等待。","js中两个等号和三个等号的区别#JS中两个等号和三个等号的区别":"JavaScript 中的比较运算符有两种：等于运算符（==）和严格等于运算符（===）。\n两个等号（==）表示的是“相等”，它会在比较之前进行类型转换。 三个等号（===）表示的是“严格相等”，它不会进行类型转换。 例如：\n1 == '1' // true 1 === '1' // false 在上面这个例子中，第一个表达式返回 true，因为在比较之前，字符串 ‘1’ 被转换成了数字 1。而第二个表达式返回 false，因为它没有进行类型转换。 综上所述，推荐使用三个等号（===）而不是两个等号（==），因为它更严格，可以避免类型转换带来的错误。 总结，三个等号（===）表示严格相等，不会进行类型转换，更严格，更安全。","var-_that--this的意义#var _that = this的意义":"var _that = this的意义在 JavaScript 中，当你在一个函数内部访问 this 指针时，它指向的可能不是你预期的对象。这是因为 this 指针的指向取决于函数调用时的上下文。 在 Vue.js 中，如果你在组件的 methods 中定义了一个函数，那么在函数内部访问 this 指针将指向组件的实例。但是，如果你在函数内部调用了另一个函数，那么此时的 this 指针可能已经指向了另一个对象。 为了解决这个问题，可以在函数的开头声明一个变量 _that，并将 this 赋值给它，这样你就可以在整个函数中使用 _that 来访问组件实例了。 例如：\nmounted() { var _that = this; setTimeout(function() { _that.message = 'Hello'; }, 1000); } 这里 setTimeout 是一个异步函数，在回调函数中 this 指向的不是 vue 组件实例，所以需要使用 _that 来保存 this。 另外在es6中有 let that = this; 或者 使用箭头函数来替代传统的 function 也可以解决 this 指向问题。 总结，var _that = this主要是为了在函数中保存 this 指向。","vue插件和直接导入js的区别#Vue插件和直接导入js的区别":"Vue 插件和直接导入 JavaScript 有以下区别：\nVue 插件是专门为 Vue 构建的，可以更好地集成到 Vue 项目中。而直接导入 JavaScript 可能需要手动进行配置和集成。 Vue 插件可以使用 Vue.use() 方法来安装和使用，并且可以根据需要进行配置。而直接导入 JavaScript 则需要在项目中手动配置和使用。 Vue 插件可以对 Vue 的实例进行扩展，例如添加全局组件、全局指令等。而直接导入 JavaScript 可能仅仅是一些帮助函数或类库。 Vue 插件可以在多个项目中重用，而直接导入的 JavaScript 可能只能在特定的项目中使用。 总结，Vue 插件是专门为 Vue 构建的，可以更好地集成到 Vue 项目中，可以通过 Vue.use() 方法来安装和使用，并且可以根据需要进行配置，可以对 Vue 的实例进行扩展，可以在多个项目中重用。","vue项目的一般结构#Vue项目的一般结构":"Vue.js 项目的一般目录结构如下:\n├── index.html ├── package.json ├── package-lock.json ├── README.md ├── node_modules ├── public │ ├── index.html │ ├── favicon.ico │ └── manifest.json ├── src │ ├── assets │ │ ├── images │ │ └── styles │ ├── components │ ├── views │ ├── router │ ├── store │ ├── App.vue │ └── main.js ├── dist ├── babel.config.js ├── package.json └── README.md 主要目录和文件的作用如下：\npublic目录: 存放静态资源文件，如 index.html、favicon.ico、manifest.json 等。 src目录: 存放项目源代码。 src/assets 目录: 存放静态资源文件，如图片、样式文件等。 src/components 目录: 存放 Vue 组件。 src/views 目录: 存放页面组件。 src/router 目录: 存放 Vue 路由配置。 src/store 目录: 存放 Vuex 的状态管理。 src/App.vue 文件: 根组件。 src/main.js 文件: 程序的入口文件。 dist 目录: 打包后的文件。 babel.config.js 文件: Babel 配置文件。 package.json 文件: 项目配置文件。 README.md 文件: 项目说明文件。 这是一个大致的目录结构，在实际的开发中，根据项目的不同，可能会有所变化。 总结，Vue.js 项目的一般目录结构主要包括 public 目录,src目录,dist目录,babel.config.js,package.json,README.md等文件和目录。","箭头函数和普通函数的区别#箭头函数和普通函数的区别":"箭头函数 优点：\n简洁易读。 没有自己的 this 上下文，能避免 this 指针混乱。 缺点：\n不能使用 arguments 对象。 不能作为构造函数 普通函数 优点：\n可以使用 this 和 arguments 对象。 可以作为构造函数。 缺点：\n可能会出现 this 指针混乱的问题。 如果需要使用 this 和 arguments 对象，或者需要将函数用作构造函数，那么就应该使用普通函数。 如果只是简单的函数调用，并且不需要使用 this 和 arguments 对象，那么可以使用箭头函数。 arguments 对象：在不知道函数传入参数的个数的情况下，可以访问所有的参数。"},"title":"Vue.js开发过程 Notes"},"/lyd-notes/vue/vue3-tips/":{"data":{"props#props#":"更改对象 / 数组类型的 props# 当对象或数组作为 props 被传入时，虽然子组件无法更改 props 绑定，但仍然可以更改对象或数组内部的值。这是因为 JavaScript 的对象和数组是按引用传递，而对 Vue 来说，禁止这样的改动虽然可能，但有很大的性能损耗，比较得不偿失。 这种更改的主要缺陷是它允许了子组件以某种不明显的方式影响父组件的状态，可能会使数据流在将来变得更难以理解。在最佳实践中，你应该尽可能避免这样的更改，除非父子组件在设计上本来就需要紧密耦合。在大多数场景下，子组件应该抛出一个事件来通知父组件做出改变。","动态参数httpsstaging-cnvuejsorgguideessentialstemplate-syntaxhtmldynamic-arguments#动态参数\u003ca href=\"https://staging-cn.vuejs.org/guide/essentials/template-syntax.html#dynamic-arguments\"\u003e#\u003c/a\u003e":"","动态参数值的限制httpsstaging-cnvuejsorgguideessentialstemplate-syntaxhtmldynamic-argument-value-constraints#动态参数值的限制\u003ca href=\"https://staging-cn.vuejs.org/guide/essentials/template-syntax.html#dynamic-argument-value-constraints\"\u003e#\u003c/a\u003e":"","动态参数语法的限制httpsstaging-cnvuejsorgguideessentialstemplate-syntaxhtmldynamic-argument-syntax-constraints#动态参数语法的限制\u003ca href=\"https://staging-cn.vuejs.org/guide/essentials/template-syntax.html#dynamic-argument-syntax-constraints\"\u003e#\u003c/a\u003e":"","组件#组件#":"在component属性中，method属性没有缓存机制，compute属性则有缓存机制。","组件基础httpsstaging-cnvuejsorgguideessentialscomponent-basicshtmlcomponents-basics#组件基础\u003ca href=\"https://staging-cn.vuejs.org/guide/essentials/component-basics.html#components-basics\"\u003e#\u003c/a\u003e":"动态参数#同样在指令参数上也可以使用一个 JavaScript 表达式，需要包含在一对方括号内：\n\u003c!-- 注意，参数表达式有一些约束， 参见下面“动态参数表达式约束”一节的解释 --\u003e \u003ca v-bind:[attributeName]=\"url\"\u003e ... \u003c/a\u003e \u003c!-- 简写 --\u003e \u003ca :[attributeName]=\"url\"\u003e ... \u003c/a\u003e 这里的 attributeName 会作为一个 JavaScript 表达式被动态执行，计算得到的值会被用作最终的参数。举个例子，如果你的组件实例有一个数据 property attributeName，其值为 \"href\"，那么这个绑定就等价于 v-bind:href。\n相似地，你还可以将一个函数绑定到动态的事件名称上：\n\u003ca v-on:[eventName]=\"doSomething\"\u003e ... \u003c/a\u003e \u003c!-- 简写 --\u003e \u003ca @[eventName]=\"doSomething\"\u003e 在此示例中，当 eventName 的值是 \"focus\" 时，v-on:[eventName] 就等价于 v-on:focus。\n动态参数值的限制# 动态参数期望结果为一个字符串，或者是 null。特殊值 null 意为显式移除该绑定。任何其他非字符串的值都将触发一个警告。\n动态参数语法的限制# 动态参数表达式因为某些字符的缘故有一些语法限制，比如空格和引号，在 HTML attribute 名称中都是不合法的。例如下面的示例：\n\u003c!-- 这会触发一个编译器警告 --\u003e \u003ca :['foo' + bar]=\"value\"\u003e ... \u003c/a\u003e 如果你需要传入一个复杂的动态参数，我们推荐使用计算属性替换复杂的表达式，也是 Vue 最基础的概念之一，我们很快就会讲到。\n当使用 DOM 内嵌模板 (直接写在 HTML 文件里的模板) 时，我们需要避免在名称中使用大写字母，因为浏览器会强制将其转换为小写：\n\u003ca :[someAttr]=\"value\"\u003e ... \u003c/a\u003e 上面的例子将会在 DOM 内嵌模板中被转换为 :[someattr]。如果你的组件拥有 “someAttr” property 而非 “someattr”，这段代码将不会工作。\n组件基础#"},"title":"Vue3 Tips"}}